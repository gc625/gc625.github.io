<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gabriel Chan's website on</title><link>https://gc625.github.io/</link><description>Recent content in Gabriel Chan's website on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://gc625.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://gc625.github.io/tags/setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/tags/setup/</guid><description/></item><item><title>3DSSD: Point-based 3D Single Stage Object Detector</title><link>https://gc625.github.io/notes/papers/3DSSD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/3DSSD/</guid><description>arXiv link github #todo
Main Ideas: Introduced Feature-Farthest Point Sampling (F-FPS) Introduced new sampling method called Fusion Sampling in Set Abstraction Layer Feature-Farthest Point Sampling (F-FPS) Objective when downsample: Remove negative points (background points) Preserve only positive points (foreground points, i.</description></item><item><title>An End-to-End Transformer Model for 3D Object Detection</title><link>https://gc625.github.io/notes/papers/3DETR/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/3DETR/</guid><description>link</description></item><item><title>Attentional PointNet for 3D-Object Detection in Point Clouds</title><link>https://gc625.github.io/notes/papers/attentionalpointnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/attentionalpointnet/</guid><description>link</description></item><item><title>Cross Entropy</title><link>https://gc625.github.io/notes/interesting/Cross-Entropy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/interesting/Cross-Entropy/</guid><description>https://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/</description></item><item><title>Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds</title><link>https://gc625.github.io/notes/papers/iassd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/iassd/</guid><description>Main Ideas Turns out F-FPS from 3DSSD is still not good enough in preserving foreground points. Introduced Class-aware Sampling Class-Aware Sampling Superior sampling method compared to F-FPS and D-FPS Using vanilla cross-entropy loss $$L_{cls-aware}=-\sum^{C}_{c=1}(s_i\log(\hat{s_i})+(1-s_i)\log(1-\hat{s_i}))$$ $C:$ Number of catergories $s_i:$ One hot labels $\hat{s_i}:$ predicted logits Centroid Aware Sampling Give higher weight to points near instance centroid $$Mask_i=\sqrt[3]{\frac{\min(f^,b^)}{\max(f^,b^)}\times \frac{\min(l^,r^)}{\max(l^,r^)}\times\frac{\min(u^,d^)}{\max(u^,d^)}}$$ $f^∗, b^∗, l^∗, r^∗, u^∗, d^∗$ represent the distance of a point to the 6 surfaces (front, back, left, right, up and down) This mask is used during training via the ctr-aware loss $L_{ctr-aware}$.</description></item><item><title>OpenPCDet</title><link>https://gc625.github.io/notes/openPCDet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/openPCDet/</guid><description>OpenPCDet Code Architecture file structure for a project should look something like this:
Directories 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 OpenPCDet (or proj name) ├── data │ ├── kitti │ ├── lyft │ └── waymo ├── docker ├── docs ├── pcdet │ ├── datasets │ ├── models │ ├── ops │ └── utils └── tools ├── cfgs ├── eval_utils ├── scripts ├── train_utils └── visual_utils</description></item><item><title>Point Transformer</title><link>https://gc625.github.io/notes/papers/PCT/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/PCT/</guid><description>link</description></item><item><title>Pointformer</title><link>https://gc625.github.io/notes/papers/pointformer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/pointformer/</guid><description>link</description></item><item><title>PointNet++</title><link>https://gc625.github.io/notes/papers/PointNet++/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/PointNet++/</guid><description>Set Abstraction (SA) Layer ![[notes/images/pointnet2.png]]
Sampling Layer Lets call the sampling layer $SL$.
Input: $N\times 3$ N points with $x,y,z$ Output: $M\times 3$ $M$ points with $x,y,z$ where points in $M$ are subset of $N$ Method: Iterative Farthest Point Sampling (FPS) Start by choosing 1 random point Calculate distance for all remaining points to selected points.</description></item><item><title>PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</title><link>https://gc625.github.io/notes/papers/PVRCNN/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/PVRCNN/</guid><description>Main Ideas Two stage detector that uses the point AND voxel features (via sparse convolution) Two novel operations: Voxel-to-keypoint scene encoding: which summarizes all the voxels of the overall scene feature volumes into a small number of feature keypoints point-to-grid RoI feature abstraction: effectively aggregates the scene keypoint features to RoI grids for proposal confidence prediction and location refinement Network Diagram !</description></item><item><title>Research Seminar Ideas</title><link>https://gc625.github.io/notes/HonorsSeminar/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/HonorsSeminar/</guid><description>All Point Cloud Transformer papers 3DETR Pointformer Point transformer Attentional Pointnet Triple Attention Net</description></item><item><title>Supervised Learning</title><link>https://gc625.github.io/notes/back2basics/supervisedlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/back2basics/supervisedlearning/</guid><description>What is supervised learning? Training an algorithm to output $y$ for a given $x$ using sufficient training samples ${(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\ldots,(x^{(n)},y^{(n)})}$ for some input $x^{(i)}$ and correct output $y^{(i)}$ Regression: predicting an a number (infinitely many outputs) Classification: predicting categories (finite outputs) Linear Regression Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\hat{y}$ given an input $x$ for linear regression, $f$ is a straight line.</description></item><item><title>TANet: Robust 3D Object Detection from Point Clouds with Triple Attention</title><link>https://gc625.github.io/notes/papers/TANet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/TANet/</guid><description>link</description></item><item><title>The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling</title><link>https://gc625.github.io/notes/papers/unionofmanifold/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/unionofmanifold/</guid><description>Main Ideas The manifold hypothesis states that high-dimensional data of interest often lives in an unknown lower-dimensional manifold embedded in ambient space title: &amp;ldquo;title&amp;rdquo; enableToc: true</description></item><item><title>Unsupervised Learning</title><link>https://gc625.github.io/notes/back2basics/unsupervisedlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/back2basics/unsupervisedlearning/</guid><description>What is Unsupervised Learning no labelled data, algorithm finds something interesting in unlabeled data. given only inputs ${x_0,\ldots,x_n}$, but no output labels ${y_0,\ldots,y_n}$ Clustering: for example, groups data points together</description></item></channel></rss>