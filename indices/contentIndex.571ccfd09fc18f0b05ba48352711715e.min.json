{"/":{"title":"Gabriel Chan's website","content":"\nHi, this is my website where I keep my notes for research, and studying. currently at [kodifly](https://www.kodifly.com/)\n\n\n# Research Notes\n## SLAM \n- [Lie Algebra](\u003cnotes/SLAM/Lie Groups and Algebra\u003e)\n\n## Point Cloud Networks\n- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](notes/papers/PointNet++)\n### Single Stage Detectors \n- [3DSSD: Point-based 3D Single Stage Object Detector](notes/papers/3DSSD)\n- [(IASSD)Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds](notes/papers/iassd)\n\n### Two Stage Detectors\n- [RVRCNN](notes/papers/PVRCNN)\n\n### Attention on point clouds\n- [3DETR: An End-to-End Transformer Model for 3D Object Detection](notes/papers/3DETR)\n- [Attentional PointNet for 3D-Object Detection in Point Clouds](notes/papers/attentionalpointnet)\n- [Point Transformer](notes/papers/PCT)\n- [TANet: Robust 3D Object Detection from Point Clouds with Triple Attention](notes/papers/TANet)\n- [3D Object Detection with Pointformer](notes/papers/pointformer)\n\n\n# Basic of Machine Learning\n- [Surpervised Learning](notes/back2basics/supervisedlearning)\n- [Unsupervised Learning](notes/back2basics/unsupervisedlearning)\n\n\n# Machine Learning frameworks\n- [OpenPCDet](notes/openPCDet)\n\n","lastmodified":"2023-12-22T01:19:22.693220977Z","tags":[]},"/2023-12-16":{"title":"2023-12-16","content":"","lastmodified":"2023-12-22T01:19:22.693220977Z","tags":[]},"/notes/CJK-+-Latex-Support-%E6%B5%8B%E8%AF%95":{"title":"CJK + Latex Support (测试)","content":"\n## Chinese, Japanese, Korean Support\n几乎在我们意识到之前，我们已经离开了地面。\n\n우리가 그것을 알기도 전에 우리는 땅을 떠났습니다.\n\n私たちがそれを知るほぼ前に、私たちは地面を離れていました。\n\n## Latex\n\nBlock math works with two dollar signs `$$...$$`\n\n$$f(x) = \\int_{-\\infty}^\\infty\n    f\\hat(\\xi),e^{2 \\pi i \\xi x}\n    \\,d\\xi$$\n\t\nInline math also works with single dollar signs `$...$`. For example, Euler's identity but inline: $e^{i\\pi} = -1$\n\nAligned equations work quite well:\n\n$$\n\\begin{aligned}\na \u0026= b + c \\\\ \u0026= e + f \\\\\n\\end{aligned}\n$$\n\nAnd matrices\n\n$$\n\\begin{bmatrix}\n1 \u0026 2 \u0026 3 \\\\\na \u0026 b \u0026 c\n\\end{bmatrix}\n$$\n\n## RTL\nMore information on configuring RTL languages like Arabic in the [config](notes/config.md) page.\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Computer-Organization/Class-Notes":{"title":"Comp org","content":"\n\n# Lecture 3 (Sept 8)\n\n**Program Counter:** (PC) holds address of the next instruction\n\n**Instruction Register:** (IR) holds the instruction currently being executed\n\n\n## Fetch-Decode-Execute Cycle\n1.  Fetch instruction from PC -\u003e IR\n2. update PC\n3. decode instruction from IR, determind if operand needed from memory\n4. If yes, fetch from register\n5. execute instruction\n6. repeat 1-5 \n\n\n\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Computer-Organization/HW1":{"title":"HW1","content":"\n# Q2 \n\n\nConnector Numer | Name of Connector | function \n-- | -- | --\n0 | PS/2 port | legacy port used for mouse and keyboards \n1 | USB type A (Female) | Universal Serial Bus (USB) port for connecting other devices to the computer, e.g. for data/power transfer\n2 | hdmi | a digital display port to connect to a monitor to output video (and audio)\n3 | VGA | analog display port to displaying video only (old)\n4 | DVI | digital and analog display port for video \n5 | SPDIF optical audio port | port for transfering digital audio between computer to another device without needing to convert to analog signal first\n6 | Thunderbolt port | bi-directional port that can be used for audio, data (10 Gigabit/s), power, and video transfer\n7 | SATA port | port (typically internal) for connecting storage devices to a computer \n8 | RAM slots | slots for RAM sticks to be inserted. RAM is used for low latency data storage for compute tasks \n9 | ethernet port | port used to connect ethernet cable for connecting the computer to a network.  \n10 | 24pin power slot  | internal power supply connector used to power components on the motherboard\n11 | PCI-E expansion slots | slots for connecting additional components, typically peripheral cards like a graphics card or networking card\n\n\n# Q3 \n$M  = 3000000$\n- Type A: 20% of instructions, 25ns \n- Type B: 45% of instructions, 40ns \n- Type C: 35% of instructions, 20ns \n\n## A)\nSince $$\\begin{align}I_6=\u0026\\prod^6_{k=1}i_k\\\\=\u0026\\ 4\\cdot2\\cdot3\\cdot4\\cdot3\\\\ =\u0026\\ 288\\end{align}$$\nso one instruction at level 6 is equivalent to 288 at level 1, and the program will have $I_6M = 864000000$ level 1 instructions. \n\n## B)\nAverage instruction time at level 1 is then the weighted sum of all instruction types and their time. $$\\begin{align}t_1=\u0026\\ 0.2(25)+0.45(40)+0.35(20)\\\\ =\u0026\\ 30\\end{align}$$\n## C) \nRecall that $t_n=I_nt_1$, so \n\n$$\\begin{align}\nI_3 =\u0026\\  4*3 = 12\\\\\nt_3 =\u0026\\ 12\\cdot 30 \\\\\n=\u0026\\ 360 \\text{ ns}\n\\end{align}$$\n\n## D) \nsame idea as c) \n$$\n\\begin{align}\nI_6 =\u0026\\ 288 \\\\\nt_6 =\u0026\\ 288*30\\\\\n=\u0026\\ 8640 \\text{ ns}\n\\end{align}\n$$\n\n## E)\nsince $T_{prog}=Mt_N$ with $N=6$ in this case\n$$\\begin{align}T_{prog}=\u0026\\ 3000000*8640\\text{ ns}\\\\\n=\u0026\\ 2.592\\cdot10^{10} \\text{ ns} \\\\ =\u0026\\ 25.92 \\text{ sec}\\end{align}$$\n\n## F) \n\n$$\\begin{align}I_{6,new}=\u0026\\ 4\\cdot2\\cdot3\\cdot4\\cdot2\\\\\n=\u0026\\ 192\\end{align}$$\nSince everything else is the same, ratio $I_{6,new}/I_{6,old}= 192/288$ which is $2/3$. So new program can complete the same task in ~66% of the time \n\n\n# Q4 \nAssume $M$ instructions at level $n$, since each instruction at level $n$ is translated into $S$ instructions at $n-1$ level, we have $MS^{5}$ instructions at level 1. In general, we have $MS^{n-1}$ level 1 instructions . \n\nIf the translation was optimal, we would have $MW^5$ and $MW^{n-1}$  instructions instead (level 6 and in general). Threfore the ratio is \n$$\\left(\\frac{S}{W}\\right)^{5}$$\nand in general \n$$\\left(\\frac{S}{W}\\right)^{n-1}$$\nfor $n$ levels. \n\n\n\n# Q5 \n\nChip/Device | Component Class | Approximate Price\n-- | -- | -- \nAMD Ryzen Threadripper PRO 5995WX | CPU |  $6499\nIntel Xeon W-3175X Skylake X 28 | CPU | $2000\nIntel 10PK OPTANE 800P SERIES | Storage | $150\nNVIDIA GEFORCE RTX 3090 Ti | GPU/PICE card | $1990\nCorsair CX Series CX450M 450 Watt  | Power Supply | $50\nASUS WS C422 SAGE/10G  | (server) motherboard |  $700\nAMD Radeon Pro WX 9100  | GPU | $2000\nIntel Core i7 i7-7700K | CPU | $350\nWD 40TB My Cloud PR4100 Pro Series  | NAS drive | $2000\nIntel Core i9-7980XE X-Series | CPU | $1200\nCisco 32GB DDR4-2666-MHZ RDIMM PC4-21300 DUAL | RAM | $400\nNVIDIA QUADRO RTX 8000  | GPU |  $6000 \nSeagate BarraCuda ST3000DM008 3TB | Hard drive | $80\nKingston 16GB DDR4, 2133MHz DIMM - KVR21R15D4/16 | RAM |  $50\nAMD Phenom II X2 560 | CPU | $20 \nNVIDIA Tesla K80 24GB  | GPU | $100 (used)\nCreative Sound Blaster Z Series ZXR |Sound Card/PCIE card| $80\n\n\n\n\n\n# Q6\n![[notes/images/moores_law.png]]\n\n\n```python\nimport pylab\nimport matplotlib.pyplot as plt\n\na = [pow(10, i) for i in range(10)]\nfig = plt.figure()\nax = fig.add_subplot(111)\nI = 15*3000\n\nx = np.linspace(0,20,6)\ny = I*2**(x/1.5)\ny2 = I*2**(x/2)\n\nline, = ax.plot(x,y,color='blue', lw=2,label=\"18 months\",marker=\"o\")\nline, = ax.plot(x,y2,color='red', lw=2,label=\"2 years (24 months)\",marker=\"o\")\n\nfor i in range(1,6):\n\tax.annotate( \"{:.2E}\".format(y[i]), (x[i], y[i]),(0.85*x[i], 1.25*y[i]))\n\tax.annotate( \"{:.2E}\".format(y2[i]), (x[i], y2[i]),(0.9*x[i], 0.4*y2[i]))\n\nax.legend()\nax.set_yscale('log')\nax.set_xlabel('years')\nax.set_ylabel('number of transistors')\nax.set_title('Number of transistors in chip area=15A')\nplt.savefig(\"moores_law.png\",facecolor=\"white\",dpi=1000)\npylab.show()\n\n```\n\n## Q6B\n\nLet length at year 0 $L_0$ and length at year 24 $L_{24}$.\n\n## doubling every 2 years\n$n_{doubling}=\\frac{24}{2}$\n$$\\begin{align}L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{n_{doubling}}}}\\\\L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{12}}}\\\\ ratio =\u0026\\ \\frac{L_{24}}{L_0}=\\frac{1}{\\sqrt{2^{12}}}\\\\=\u0026\\ 0.015625 \\end{align}$$\n\n## doubling every 18 months\n$n_{doubling}=\\frac{24}{1.5}=16$\n$$\\begin{align}L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{n_{doubling}}}}\\\\L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{16}}}\\\\ ratio =\u0026\\ \\frac{L_{24}}{L_0}=\\frac{1}{\\sqrt{2^{16}}}\\\\=\u0026\\ 0.00390625 \\end{align}$$","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Computer-Organization/HW2":{"title":"HW2","content":"\n\n# Q2\n\n**70%** Type A: register-to-register, 10 clock cycles \n**30%** Type B: read/write, 110 clock cycles to read or write once both the instruction\nand the operand have been fetched.\n\n## A\nType A: total time $= 110 + 10 = 120\\text{ns}$\nType B: total time $= 110 + 110 + 110 = 330\\text{ns}$\n\nAverage  $0.7\\cdot 120 + 0.3\\cdot 330= 183$ clock cycles\n\n## B\n$T =\\frac{1}{200\\text{MHz}} = 5\\text{ns}$ \nso average execution time = $5\\cdot 183 = 915 \\text{ ns}$ \n\n## C\nso $\\frac{1000}{915\\text{ns}} = 1.093 \\text{MIPS}$ \n\n## D \nIf instead $f=1\\text{GHz}$, then $T= 1\\text{ns}$ which impleis instruction time is $183\\text{ ns}$\n$\\frac{1000}{183\\text{ns}} = 5.46 \\text{MIPS}$\n\n## E\nThen $\\frac{20MI}{5.46MIS^{-1}}=3.66 s$\n\n\n# Q3 \n\n## A\nso sequential operation, cycle time is $4+14=18\\text{ ns}$. Since it takes 3 clock cycles, instruction time = $18\\cdot 3 = 54 \\text{ns}$. Therefore $\\frac{1000}{54} = 18.519 \\text{MIPS}$\n$f= 55.555 MHz$\n\n\n## B \nIf its pipelined, then an instruction completes every cycle, which implies frequency = bandwidth  =  55 MIPS\n\n## C \nfor part A:  $\\frac{2 M}{18.519}=0.108 s$\nfor part B:  $\\frac{2 M}{55}=0.0364s$\n\n\n\n# Q4 \n\nSystem | Cycle Time | Clock Frequency | Instruction Completion Time | BW\n-- | -- | -- | -- | --\nA | 4 ns | 250 MHz | 20ns | 250 MIPS\nB | 5 ns | 200 MHz | 25ns | 200 MIPS\nC | 5 ns | 200 MHz | 25ns | 40 MIPS\nD | 1 ns | 1 GHz | 10ns | 1000 MIPS\nE | 6 ns | 166.66 MHz | 36ns | 166.66 MIPS\nF | 1.5 ns | 666 MHz | 15ns | 66 MIPS\nSystem D is the fastest because it has the highest bandwidth (1000MIPS), and therefore it processes the most number of instructions per second. \n\n## Calculations \n\nA: 250 MIPS for pipelined $\\implies f=250MHz$.\nSo $T=1/f = 4ns$. \nTherefore Instruction completion time (latency) is $5T = 20 ns$ \n\nB:  5 stage + cycle time = 5ns $\\implies$ latency = $25ns$. Clock freq is $1/5ns$, BW is same but in MIPS\n\nC: no pipline latency = 25ns  $\\implies$ bandwidth = $1000/25ns =40 MIPS$\ncycle time = 5 ns  $\\implies$ frequency = $1/5ns = 200$ Mhz \n\nD: Pipelined, latency = 10 ns , f = 1GHz \nso T = $1/1GHz = 1ns$ \nso  BW = 1000 MIPS \nand latency = $1\\cdot 10 = 10ns$ \n\nE : 6 pipeline, latency = $36 ns$ implies $T=6ns$ and $f= 1/6ns = 166.66 MHz$ BW is same but in MIPs becuase pipeline\n\nF: No pipeline, latency = 15ns $\\implies BW = 66MIPS$   \n$f= 666.6MHz \\implies T=1.5ns$\n\n\n\n\n# Q5\n\n. | 00101101 | 10010101\n-- | -- | -- \nPure Binary | 1+4+8+32 = 45  | 1+4+16+128=149\nSign-magnitude | MSB=0 so 45 | MSB=1 so -(1+4+16)= -21\nTwo's Complement |MSB=0 so 45 | MSB=1 so -128+21=-107\nExcess 128 | 45+128=163, 163 in binary = 10100011 | not possible with $k=8$\nBCD | 0010 = 2, 1101 = 13, so not valid | 1001 = 9, 0101 = 5 so 95\nhexadecimal |0010 = 2 1101 = D | also 95 since neither digit \u003e9 \n\n\n# Q6\n\nOriginal Number | Binary Number in Register | Value as interpreted with the new representation\n-- | -- | -- \n10 pure bin k=4 | 1010 | 10-8 = 2 \n-11 sign-magnitude k=5 | 11011 | -16+(11) = -5\n48 pure bin k=6| 110000| -32+(16) = -16\n-27 2s k=6| 100101| 1+4+32 = 37\n-76 2s k=8  | 10110100 | B4\n38 2d bcd| 00111000 | MSD=0 so just 8+16+32 = 56\n6DH 2D hex | 01101101 | 109-128 = -18 \n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Computer-Organization/HW4":{"title":"HW4","content":"\n\n# Q2 \n\ncfg\t| mean access time (T_c) |\tCache access time (c) |Main memory access time (m)\t| Hit ratio (h,%) \n--|-|-| -|-\nA|\t26 |\t10 |80 | 80\nB|21|15|60|90\nC|23|8|60|75\nB, since it has the shortest mean access time.\n\n\n$H_1= 0.9$\n$H_2=0.8$\n$C_1=0.5C_2$\n$C_2=0.1M$\n$M$ = main memory access time\n\n$T_{avg} =  C_1 + (1-H_1)(C_2 + (1-H_2)M)$\n$T_{avg} = 0.5C_2 + 0.1(C_2+0.2M)$\n$T_{avg}=0.6C_2+0.02M$\n$T_{avg}=0.06M+0.02M=0.08M$\n\n$\\frac{1}{0.08}=12.5\\implies$ two lvl system is 12.5 times faster \n\n# Q3\nsetup|effective disk size (TB)|per tb cost|failure tolerance\n-| -| - | - \nRAID 0\t|96|$18.75|\t0, basically one big hard drive, whole thing dies if one fails\nRAID 1|\t8|$225.00|11, 11 drives are just copies, so they can all die\nRAID 1+0|16|$112.50|5, we have 2 sets of 6 drives (RAID1), so up to 5 can die in a RAID1 cfg\nRAID 100 |32| $56.25 | 2, 4 sets of 3 drives in RAID1, so 2 can die \nRAID 5|\t88\t|$20.45|1, just one parity disk\nRAID 5+0 |80|$22.50|1, also just one parity disk\nRAID 6\t|80|$22.50|\t2, two parity disks\nRAID 6+0 |64|$28.13\t|2, also two parity disks\n\n\n# Q4 \n| $b_3$ | $b_2$ | $b_1$ | $b_0$ | f |\n|-------|-------|-------|-------|---|\n|   0   |   0   |   0   |   0   | 0 |\n|   0   |   0   |   0   |   1   | 1 |\n|   0   |   0   |   1   |   0   | 0 |\n|   0   |   0   |   1   |   1   | 1 |\n|   0   |   1   |   0   |   0   | 0 |\n|   0   |   1   |   0   |   1   | 1 |\n|   0   |   1   |   1   |   0   | 0 |\n|   0   |   1   |   1   |   1   | 1 |\n|   1   |   0   |   0   |   0   | 0 |\n|   1   |   0   |   0   |   1   | 0 |\n|   1   |   0   |   1   |   0   | 0 |\n|   1   |   0   |   1   |   1   | 0 |\n|   1   |   1   |   0   |   0   | 0 |\n|   1   |   1   |   0   |   1   | 0 |\n|   1   |   1   |   1   |   0   | 0 |\n|   1   |   1   |   1   |   1   | 0 |\n## b)\n$f=b_3'b_2'b_1'b_0+b_3'b_2'b_1b_0+b_3'b_2b_1'b_0+b_3'b_2b_1b_0$\n![[logic gate.jpeg]]\n## c) \n$f=b_3'b_2'b_1'b_0+b_3'b_2'b_1b_0+b_3'b_2b_1'b_0+b_3'b_2b_1b_0$\n$=b_0b'_3(b_2'(b_1+b_1')+b_2(b_1+b_1'))$\n$=b_0b_3'$\n![[Pasted image 20221024222420.png]]\n## d)\n$g= (b_2'b_3')(b_3'b_0)$\n$= b_0b_2'b_3'$\n![[Pasted image 20221024222630.png]]\n\n\n\n# Q5 \n\ni) \nf \n| x | y | z | y'z | z' | yz' | y'z+z'+yz' | x(y'z+z'+yz') | z'+x' | y'(x'+z') | x(y'z + z' + yz') + y'(z' + x') |\n|:-:|:-:|:-:|-----|----|-----|------------|---------------|-------|-----------|---------------------------------|\n| 0 | 0 | 0 |   0 |  1 |   0 |          1 |             0 |     1 |         1 |                               1 |\n| 0 | 0 | 1 |   1 |  0 |   0 |          1 |             0 |     1 |         1 |                               1 |\n| 0 | 1 | 0 |   0 |  1 |   1 |          1 |             0 |     1 |         0 |                               0 |\n| 0 | 1 | 1 |   0 |  0 |   0 |          0 |             0 |     1 |         0 |                               0 |\n| 1 | 0 | 0 |   0 |  1 |   0 |          1 |             1 |     1 |         1 |                               1 |\n| 1 | 0 | 1 |   1 |  0 |   0 |          1 |             1 |     0 |         1 |                               1 |\n| 1 | 1 | 0 |   0 |  1 |   1 |          1 |             1 |     1 |         0 |                               1 |\n| 1 | 1 | 1 |   0 |  0 |   0 |          0 |             0 |     0 |         0 |                               0 |\n\ng \n![[Screenshot 2022-10-24 at 11.08.48 PM.png]]\n\n# ii)\n\n$$\\begin{align}f =\u0026\\ xy'z + xz'+xyz' + y'z' + y'x'\\\\ \n=\u0026\\ y'(xz+x')+z'(x+xy+y') \\\\ \n=\u0026\\ y'((x+x')(x'+z))+z'(x+y')\\\\\n=\u0026\\ y'(x'+z)+z'(x+y')\\\\\n=\u0026\\ y'x'+y'z+z'x+y'z'\\\\\n=\u0026\\ y'(z'+z)+y'x'+z'x \\\\\n=\u0026\\ y'+y'x'+z'x\\\\\n=\u0026\\ y'+z'x\\end{align}$$\n$$\\begin{align}g=\u0026\\ x'y'z' + z(x'w'y' + x'y + wy') + wy'z'\\\\\n=\u0026\\ x'y'z'+z(x'(w'y'+y)+wy')+wy'z'\\\\\n=\u0026\\ x'y'z'+z(x'(w'+y)+wy')+wy'z'\\\\\n=\u0026\\ x'y'z'+zx'(w'+y)+wy'(z+z')\\\\\n=\u0026\\ x'y'z'+zx'w'+zx'w+wy'\\end{align}$$\n\n## iii)\n\n| x | y | z | y'+z'x |\n|:-:|:-:|:-:|:------:|\n| 0 | 0 | 0 |    1   |\n| 0 | 0 | 1 |    1   |\n| 0 | 1 | 0 |    0   |\n| 0 | 1 | 1 |    0   |\n| 1 | 0 | 0 |    1   |\n| 1 | 0 | 1 |    1   |\n| 1 | 1 | 0 |    1   |\n| 1 | 1 | 1 |    0   |\nwhich matches\n| w | x | y | z | x'y'z' | zx'w' | zx'w | wy' | g |\n|---|---|---|---|--------|-------|------|-----|---|\n| 0 | 0 | 0 | 0 | 1      | 0     | 0    | 0   | 1 |\n| 0 | 0 | 0 | 1 | 0      | 1     | 0    | 0   | 1 |\n| 0 | 0 | 1 | 0 | 0      | 0     | 0    | 0   | 0 |\n| 0 | 0 | 1 | 1 | 0      | 1     | 0    | 0   | 1 |\n| 0 | 1 | 0 | 0 | 0      | 0     | 0    | 0   | 0 |\n| 0 | 1 | 0 | 1 | 0      | 0     | 0    | 0   | 0 |\n| 0 | 1 | 1 | 0 | 0      | 0     | 0    | 0   | 0 |\n| 0 | 1 | 1 | 1 | 0      | 0     | 0    | 0   | 0 |\n| 1 | 0 | 0 | 0 | 1      | 0     | 0    | 1   | 1 |\n| 1 | 0 | 0 | 1 | 0      | 0     | 1    | 1   | 1 |\n| 1 | 0 | 1 | 0 | 0      | 0     | 0    | 0   | 0 |\n| 1 | 0 | 1 | 1 | 0      | 0     | 1    | 0   | 1 |\n| 1 | 1 | 0 | 0 | 0      | 0     | 0    | 1   | 1 |\n| 1 | 1 | 0 | 1 | 0      | 0     | 0    | 1   | 1 |\n| 1 | 1 | 1 | 0 | 0      | 0     | 0    | 0   | 0 |\n| 1 | 1 | 1 | 1 | 0      | 0     | 0    | 0   | 0 |\n\n\nwhich matches.","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Grad-School/people-to-email":{"title":"people to email","content":"\n# Stanford \n- Davis Rempe \\[ [Leonidas Guibas](https://geometry.stanford.edu/member/guibas/)\\]: https://davrempe.github.io//\n\n\n# Princeton\n- https://www.cs.princeton.edu/~jiadeng/\n\n\n# UPenn \n- grasp lab: https://www.grasp.upenn.edu/\n\n# MIT \n- daniela rus?\n- http://groups.csail.mit.edu/vision/welcome/\n\n# UW \n[ROBOT LEARNING LAB @UW](https://robotlearning.cs.washington.edu/)\n\n\n# Berkeley \n- https://rll.berkeley.edu/research.html\n\n\n# UIUC\nhttp://shenlong.web.illinois.edu/\n\n# Brown\nhttps://chensun.me/\nhttp://cs.brown.edu/people/ssrinath/research.html\n\n\n\n\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/HonorsSeminar":{"title":"Research Seminar Ideas","content":"\n\n# All Point Cloud Transformer papers\n\n- [3DETR](notes/papers/3DETR)\n- [Pointformer](notes/papers/pointformer)\n- [Point cloud transformer](notes/papers/pointformer)\n- [Attentional Pointnet](notes/papers/attentionalpointnet)\n- [Triple Attention Net](notes/papers/TANet)\n\n\nNLP book: https://slds-lmu.github.io/seminar_nlp_ss20/\n\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/LC/Patterns":{"title":"Patterns","content":"\n\n# Binary Trees\n## Definition\n```Python\nclass node:\n\tdef __init__(val,left=None,right=None)\n\t\tself.val = val\n\t\tself.left = left\n\t\tself.right = right\n```\n\n## DFS\n\n```Python\n\ndef dfs(node):\n\tif node is None:\n\t\treturn #base case\n\n\t# LOGIC \n\n\tif root.left is not None:\n\t\tdfs(root.left)\n\tif root.right is not None:\n\t\tdfs(root.right)\n```\n\n\n\n\n## BFS\n\n```Python\nfrom collections import deque\n# PSUEDOCODE\n\n# 1. corner case\nif node is empty:\n\treturn # sth\n\n\nqueue =  deque([node])\n# if need to keep track of depth or sth\nqueue =  deque([(node,depth or column)])\n\n\nwhile len(queue) \u003e 0:\n\n\tnode = queue.popleft()\n\n\tif node is not None:\n\t\t# logic to node\n\n\t\tqueue.append(node.left)\n\t\tqueue.append(node.right)\n\t\nreturn #answer\n\n```\n\n\n\n# \n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/ML-and-Data-Mining/class-notes":{"title":"Class notes","content":"\n\n# What is ML?\n- study of algorithms and statistical models to perform a **task** without **explicit instructions**\n\n# What is Data Mining?\n- discovering patterns in large datasets \n- mix of cs and stats, goal of extracting info and structure of data\n\n## ML vs Data Mining?\n- ML learning tools are evaluated based on ability to reproduce known knowledge\n- Data mining is focused on discovering previously unknown knowledge\n\n## Data Mining vs Data analysis?\n- Data analysis: testing models on dataset \n- Data Mining: finding hidden patterns in large num of data\n\t- dont necessarily need to know what youre looking for\n\n\n\n# Problems in ML \n- Consider your data: Continuous or discrete?\n- Classification or Regression?\n- Abundant or Rare (sparse)?\n- ","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/Math-Seminar":{"title":"Math Seminar","content":"\n--\n\n$\\newcommand{\\R}{\\mathbb{R}}$\n$\\newcommand{\\calH}{\\mathcal{H}}$\n$\\newcommand{\\la}{\\langle}$\n$\\newcommand{\\ra}{\\rangle}$\n--\n# Hilbert Spaces\n- A [Vector Space](\u003cnotes/definitions#Vector Spaces\u003e) equipped with an [inner product](\u003cnotes/definitions#Inner Product\u003e) which defines a [distance function](\u003cnotes/definitions#Metric\u003e) for which it is a [complete metric space](\u003cnotes/definitions#Complete Metric Space\u003e)\n- In a Hilbert Space, we use the norm $\\|x\\|=\\sqrt{\\langle x, x\\rangle}$\n- and the metric is defined as: $$d(x,y)=\\|x-y\\|=\\sqrt{\\langle x-y,x-y\\rangle}$$\n\n# Repoducing Kernel Hilbert Spaces\nFor a set $X$, let $\\mathbb{R}^X$ denote the set of functions $X \\mapsto \\mathbb{R}$ . We endown $\\mathbb{R}^X$ with the foowing operations: $$(f_1+f_2)(x)=f_1(x)+f_2(x)\\quad,\\quad(a\\cdot f)(x)=af(x)$$\nfor it to become a vector space. \n\n## Evaluation functionals\nIn this case, linear functionals, as defined as members of the [dual space](\u003cdefinitions#Dual Spaces\u003e)  of $\\mathbb{R}^X$, are linear functions of the form $$\\varphi:\\mathbb{R}^X\\to\\mathbb{R}$$A specfial linear functional $e_x$ called the **evaluation functional**, sends a function $f$ to its value at a point $x$:$$e_x(f)=f(x)$$\nWhen this special functional is bounded, the set $\\mathbb{R}^X$ takes on a lot of structure.\n\n## RKHS Definition\nLet $X$ be a nonempty set. $\\mathcal{H}$ is a Reproducing Kernel Hilbert Space on $X$ if:\n\n1. $\\calH$  is a vector subspace of $\\R^X$\n2. $\\calH$ is equipped with inner product $\\la \\cdot,\\cdot\\ra$ \n\t- so $\\calH$ is a Hilbert Space\n3. For all $x\\in X$, the linear evaluation functional $e_x:\\calH\\to \\R$ is bounded.\n\n\n# Riesz Representation Theorem \nIf $\\phi$ is a bounded [linear functional](\u003cdefinitions#Linear Functional\u003e) on a hilbert space $\\calH$, then there is a unique $g\\in\\calH$ such that $$\\phi(f)=\\la g,f\\ra$$for all $f\\in\\calH$\n\n## Collary 1\nLet $\\calH$ be a RKHS on $X$. For every $x\\in X$, there exists a unique $k_x\\in \\calH$ such that $$\\la k_x,f\\ra=f(x)$$for all $f\\in\\calH$\n\n### Note\n- $k_x$ and $f$ are functions that sends $X\\to\\R$.  so $\\la k_x,f\\ra$ evaluates to $\\R$. \n- this collary is just saying that the Riesz Representation Theorem applies to RKHS\n\n\n# Reproducing Kernel\nThe function $K: X\\times X \\to\\R$ defined by $$K(x,y)=k_y(x)$$\nis called the reproducing kernel of $\\calH$.\n\n# (Theorem) Equivalence Between Kernels and RKHS\nEvery RKHS has a unique reproducing kernel, and every reproducing kernel induces a unique RKHS.\n\n- RKHS induces Reproducing Kernel\n\t- follows by Riesz Representation Theorem \n- Reproducing Kernel induces RKHS \n\t- follows by Cauchy-Schwartz: If $K$ is a reproducing jernel on Hilbert space $\\calH$, then $$e_x(f)=\\la k_x,f\\ra\\leq\\|k_x\\|\\|f\\|=\\sqrt{K(x,x)}\\cdot \\|f\\|$$\n\t- so $e_x$ is bounded, ans $\\calH$ is an RKHS\n\n\n\n# Examples of RKHS \n\n## Linear Functions in $\\R^d$\n- Let $\\calH= \\R^d$ with canonical basis vectors $e_1,\\ldots,e_d$, and the standard inner product $$\\la x,w\\ra=\\sum^n_{i=1}x_iw_i$$\n- $X$ is the discrete set $\\{1,\\ldots,d\\}$ \n- $e_i\\in\\calH$ are the kernel functions, since $$\\la e_i,x\\ra= x(i)=x_i$$\n- ","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/SLAM/Lie-Groups-and-Algebra":{"title":"Lie Algebra and Lie Groups","content":"\n*Special Orthogonal Group*: the group of rotational matrices \n$$\n\\mathrm{SO}(3) = \\{ \\mathbf{R} \\in \\mathbb{R}^{3 \\times 3} | \\mathbf{RR}^T = \\mathbf{I}, \\det(\\mathbf{R})=1 \\}.\n$$\nSpecial Euclidean Group: the group of transformation matrices\n$$\n\\mathrm{SE}(3) = \\left\\{ \\mathbf{T} = \n\\begin{bmatrix}\n\\mathbf{R} \u0026 \\mathbf{t} \\\\\n{{\\mathbf{0}^T}} \u0026 1\n\\end{bmatrix}\n\\in \\mathbb{R}^{4 \\times 4} | \\mathbf{R} \\in \\mathrm{SO}(3), \\mathbf{t} \\in \\mathbb{R}^3\\right\\}\n$$\n\nThese are groups because they satisfy all the [required properties](\u003cnotes/definitions#Groups\u003e), but furthermore, because the elements of the set are continuous (smooth), they actually also form what we call a Lie Group\n\n\n# Lie Algebra\nLet $\\mathbf{R}(t)$ be the rotation of a camera that changes continuously over time, we know that it satisfies:\n$$\n\\begin{equation}\n\\mathbf{R}(t)\\mathbf{R}(t)^T = \\mathbf{I}\n\\end{equation}\n$$\nTake derivative yields (by chain rule):\n$$\n\\dot{\\mathbf{R}}(t)\\mathbf{R}(t)^T+\\mathbf{R}(t)\\dot{\\mathbf{R}}(t)^T=0\n$$\nwhich equals\n\n\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/SLAM/Visual-Slam-book":{"title":"Visual Slam","content":"\ntest\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/back2basics/Decision-and-Classification-Trees":{"title":"Decision and Classification Trees","content":"\n- Decision Tree makes a statement, then makes a statement based on the truth of the statement\n- ## Classification Tree\n- classifies things into categories\n\t- If True, go left \u003c- convention \n\t- top tree is root node\n\t- leaves have nodes pointing towards them, none leaving.\n\t- else its a branch\n- **Impure:** Leaves that contain true and false outcomes \n- Gini Impurity for leaf: $$=1-(\\text{P(Yes)})^2-(\\text{P(No)})^2$$\n- Total **Gini Impurity** = weighted average of Gini Impurities for leaves\n\t- weight of leaf = number of ppl in leaf / all ppl in other leafs\n- predictor with lowest gini impurity is chosen as root node\n## Regression Tree \n- predicts numeric values \n- split data into two groups by finding the threshold that gives the smallest sum of squared residuals\n- typically we set a minimum number of observations to allow for a split\n\t- common threshold is 20\n\t- i.e. leaves can have most 20 observations \n- Calculate sum of squared residuals for all predictors, candidate with lowest value is root node\n\n## Pruning Regression Tree\n- Cost Complexity Pruning \n- Calculate sum of squared residuals for all possible subtrees \n\t- calculate SSR for all leaves and sum \n- Tree score = $SSR +\\alpha T$\n\t- T = number of leaves\n- To find $\\alpha$, we increase it until pruning leaves increases tree score \n\n\n## AdaBoost\n\nAmount of Say = $\\frac{1}{2}\\log{\\left(\\frac{1-\\text{Total Error}}{\\text{Total Error}}\\right)}$\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/back2basics/supervisedlearning":{"title":"Supervised Learning","content":"\n\n# What is supervised learning?\n\n- Training an algorithm to output $y$ for a given $x$ using sufficient training samples $\\{(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\\ldots,(x^{(n)},y^{(n)})\\}$ for some input $x^{(i)}$ and **correct** output $y^{(i)}$ \n- **Regression:** predicting an a number (infinitely many outputs)\n- **Classification:** predicting categories (finite outputs)\n\n\n# Linear Regression \n- Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\\hat{y}$ given an input $x$ \n- for linear regression, $f$ is a straight line. With parameters $w,b$ we can then represent $f$ as: $$f_{w,b}(x)=wx+b$$\n## Cost Function\n- Since our objective to find $w,b$ such taht $\\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$. \n- Squared error cost function:  $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$\n- where $m=$ number of training examples. So $\\frac{1}{m}$ is to average it so it doesnt blow up, factor of $2$ is for computational convience later. \n- Now we can also rewrite it as: $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}f_{w,b}(x^{(i)})-y^{(i)})^2$$\n- This can be solved analytically for simple cost functions, but for complicated $J$, we can use gradiant descent to minimize $J$ instead: \n\n## Gradient Descent\n- initialize $w,b$, calcuated $J$ \n- adjust $w,b$ to decrease $J$ \n- repeat until hopefully $J$ settles near minimum \n\n- Step 1: ($=$ here is assignment, not equals)\n$$w=w -\\alpha \\frac{d}{dw}J(w,b)$$ where $\\alpha$ is the learning rate, a **hyperparameter** that controls the \"fast\" we change $w$ \n- Step 2: do the same for $b$ $$b=b -\\alpha \\frac{d}{db}J(w,b)$$\n- **Note:** $w$ and $b$ must be updated at the same time. \n### Learning rate\n- If $\\alpha$ is too small, then it will take many steps to reach minimum \n- If $\\alpha$ is too large, then it might never reach the minimum \n\n# Gradient Descent for linear regression\nCalculating derivatives   for $w$, $$\\frac{d}{dw}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$=\\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)}-y^{(i)})x^{(i)}$$\n\nand derivative for $b$, \n$$\\frac{d}{db}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$ =\\frac{d}{db}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})$$\n\nPsuedocode for gradient descent:\n```python\nwhile J not converged:\n\tw = w - a * dJdW\n\tb = b- a * dJdb\n```\nwhere `dJdW` = $\\frac{d}{dw}J(w,b)$ and `dJdb` = $\\frac{d}{db}J(w,b)$\n\n\n\n# Multiple features\nwhat if you have multiple features (variables)? \n\n- $x_j = j^{th}$ feature\n- $n$ = number of features\n- $\\vec{x}^{(i)}$= features of $i^{th}$ training example\n- $x_{j}^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n\nWe can then express the linear regression model as:\n\n$$f_{w,b}(x)=w_1x_1+w_2x_2+\\cdots+w_nx_n+b$$\ndefine $\\vec{w} = [w_1,\\ldots,w_n]$ and $\\vec{x}=[x_1,\\ldots x_n]^T$, $T$ here represents transpose. Then \n$$f_{\\vec{w},b}=\\vec{w}\\cdot \\vec{x}+b$$Where $(\\cdot)$ represents the dot product. \n\n\n\n# Feature Scaling\nWhen the range of values your features can take up differ greatly, i.e. \n- $x_1$ = square footage of house $\\in [500,5000]$ \n- $x_2$ = number of bedrooms $\\in [1,5]$\n\nthis may cause gradient descent to run slowly. ![[notes/images/feature_scaling.png]]\n\nSome examples of feature scaling\n## max scaling\n- divide each data point for a feature by the max value for that feature.\n\n## mean normalization\n- eg: if $300 \\leq x_1 \\leq 2000$ , we can scale it like such $$x_{1new} = \\frac{x_1-\\mu_1}{2000-300}$$\n- where $\\mu_1$ = mean\n\n\n## Z-score normalization \n- find standard deviation $\\sigma$ , mean $\\mu$ then $$x_1=\\frac{x_1-\\mu_1}{\\sigma_1}$$\n\n\n\n\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/back2basics/unsupervisedlearning":{"title":"Unsupervised Learning","content":"\n\n# What is Unsupervised Learning\n\n- no labelled data, algorithm finds something interesting in unlabeled data.\n\t- given only inputs $\\{x_0,\\ldots,x_n\\}$, but no output labels $\\{y_0,\\ldots,y_n\\}$\n- **Clustering:**  for example, groups data points together\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/callouts":{"title":"Callouts","content":"\n## Callout support\n\nQuartz supports the same Admonition-callout syntax as Obsidian.\n\nThis includes\n- 12 Distinct callout types (each with several aliases)\n- Collapsable callouts\n\nSee [documentation on supported types and syntax here](https://help.obsidian.md/Editing+and+formatting/Callouts).\n\n## Showcase\n\n\u003e [!EXAMPLE] Examples\n\u003e\n\u003e Aliases: example\n\n\u003e [!note] Notes\n\u003e\n\u003e Aliases: note\n\n\u003e [!abstract] Summaries \n\u003e\n\u003e Aliases: abstract, summary, tldr\n\n\u003e [!info] Info \n\u003e\n\u003e Aliases: info, todo\n\n\u003e [!tip] Hint \n\u003e\n\u003e Aliases: tip, hint, important\n\n\u003e [!success] Success \n\u003e\n\u003e Aliases: success, check, done\n\n\u003e [!question] Question \n\u003e\n\u003e Aliases: question, help, faq\n\n\u003e [!warning] Warning \n\u003e\n\u003e Aliases: warning, caution, attention\n\n\u003e [!failure] Failure \n\u003e\n\u003e Aliases: failure, fail, missing\n\n\u003e [!danger] Error\n\u003e\n\u003e Aliases: danger, error\n\n\u003e [!bug] Bug\n\u003e\n\u003e Aliases: bug\n\n\u003e [!quote] Quote\n\u003e\n\u003e Aliases: quote, cite\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/definitions":{"title":"Definitions","content":"\n\n\n# Dual Spaces\nGiven vector space $V$ over field $F$, its dual space $V^*$ is the set of all [linear transformation](\u003cdefinitions/Linear Transformations\u003e) $$\\varphi: V\\to F$$Note that $\\varphi$ is called a linear functional (or linear form, one-form) \n\nIn other words, $V^*$ is just the set of all functions that take in a vector $v\\in V$, and spits out an element in $F$ \n\n\n# Linear Functional\nA linear functional, linear form, or one-form on a vector space $V$ over $\\mathbb{R}$ is a linear transformation $$\\varphi:V\\to\\mathbb{R}$$\nit is bounded if there exists an $M\u003e0$ such that $$\\|\\varphi(x)\\|\\leq M\\|x\\|$$ for all $x\\in V$\n\n# Linear Transformations\nGiven two vector spaces $V$ and $W$ over field $F$. A mapping $$f:V\\to W$$ is called linear (or $F$-linear, homomorphism of $F$-vector spaces) if \n1.  $f(v_1+v_2)=f(v_1)+f(v_2)$\n2.  $f(\\lambda v_1)=\\lambda f(v_1)$\n\n## Variations\n- **Isomorphism**: Bijective linear transformations\n- **Endomorphism**: Linear map from vector space to itself\n- **Automorphism**: Bijective endomorphism\n\n\n\n# Vector Spaces\n- A set $V$ that is closed under finite vector addition and scalar multiplication\n\n# Inner Product\nA mapping on a vector space $V$ over a field $F$: $$\\langle\\cdot,\\cdot\\rangle: V\\times V \\to F$$\nwith \n1. conjugate symmetry: $\\langle x,y\\rangle = \\overline{\\langle x,y\\rangle}$\n2. linearity in the first argument: $\\langle ax+by,z\\rangle = a\\langle x,z\\rangle+b\\langle y,z\\rangle$\n3. Positive-definiteness: $\\langle x,x\\rangle \\geq 0$\n\n# Metric\nor distance function, is a function $$d: X\\times X \\to \\mathbb{R}$$\nsuch that for all $x,y,z\\in X$, the metric has the following properties\n1. Identity of indiscernibles: $d(x,y)=0 \\iff x=y$ \n2. Symmetry: $d(x,y)=d(y,x)$\n3. Triangle Inequality:$d(x,z)\\leq d(x,y)+d(y,z)$\n\n# Complete Metric Space\nA metric space $M$ where every [Cauchy sequence](\u003cdefinitions#Cauchy Sequence\u003e) of points in $M$ converges to a limit in $M$.\n\n\n# Cauchy Sequence\nIn a metric space $(X,d)$, a sequence $$x_1,x_2,x_3,\\ldots$$where for all real number $\\varepsilon\u003e0$ there is a positive integer $N$ such that for all $m,n\u003eN$:\n$$d(x_m,x_n)\u003c\\varepsilon$$\n\n\n\n# Groups\nA group is an algebraic structure of one set plus one operator. We denote the set as $A$ and the operation as $\\cdot$, then the group can be denoted as $G=(A,\\cdot)$. We say $G$ is a \\textit{group} if the operation satisfies the following conditions:\n\n- Closure:   $\\forall a_1, a_2 \\in A, \\ a_1 \\cdot a_2 \\in A$\n- Combination: $\\forall a_1, a_2, a_3 \\in A, \\ (a_1 \\cdot a_2) \\cdot a_3 = a_1 \\cdot ( a_2 \\cdot a_3)$\n- Unit element: $\\exists a_0 \\in A, \\ \\mathrm{s.t.} \\ \\forall a \\in A, \\ a_0 \\cdot a = a \\cdot a_0 = a$.\n- Inverse element: $\\forall a \\in A, \\ \\exists a^{-1} \\in A, \\ st \\ a \\cdot a^{-1} = a_0$.\n\n\n# Lie Group\nA group with continuous elements ","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/definitions/groups":{"title":"groups","content":"","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":[]},"/notes/docker":{"title":"Hosting with Docker","content":"\nIf you want to host Quartz on a machine without using a webpage hosting service, it may be easier to [install Docker Compose](https://docs.docker.com/compose/install/) and follow the instructions below than to [install Quartz's dependencies manually](notes/preview%20changes.md).\n## Hosting Quartz Locally\nYou can serve Quartz locally at `http://localhost:1313` with the following script, replacing `/path/to/quartz` with the \nactual path to your Quartz folder.\n\ndocker-compose.yml\n```\nservices:\n  quartz-hugo:\n    image: ghcr.io/jackyzha0/quartz:hugo\n    container_name: quartz-hugo\n    volumes:\n      - /path/to/quartz:/quartz\n    ports:\n      - 1313:1313\n\n    # optional\n    environment:\n      - HUGO_BIND=0.0.0.0\n      - HUGO_BASEURL=http://localhost\n      - HUGO_PORT=1313\n      - HUGO_APPENDPORT=true\n      - HUGO_LIVERELOADPORT=-1\n```\n\nThen run with: `docker-compose up -d` in the same directory as your `docker-compose.yml` file.\n\nWhile the container is running, you can update the `quartz` fork with: `docker exec -it quartz-hugo make update`.\n\n## Exposing Your Container to the Internet\n\n### To Your Public IP Address with Port Forwarding (insecure)\n\nAssuming you are already familiar with [port forwarding](https://en.wikipedia.org/wiki/Port_forwarding) and [setting it up with your router model](https://portforward.com):\n\n1. You should set the environment variable `HUGO_BASEURL=http://your-public-ip` and then start your container.\n2. Set up port forwarding on your router from port `p` to `your-local-ip:1313`.\n3. You should now be able to access Quartz from outside your local network at `http://your-public-ip:p`.\n\nHowever, your HTTP connection will be unencrypted and **this method is not secure**.\n\n### To a Domain using Cloudflare Proxy\n\n1. Port forward 443 (HTTPS) from your machine.\n2. Buy a custom domain (say, `your-domain.com`) from [Cloudflare](https://www.cloudflare.com/products/registrar/). Point a DNS A record from `your-domain.com` to your public IP address and enable the proxy.\n3. Set the environment variables `HUGO_BASEURL=https://your-domain.com`, `HUGO_PORT=443`, and `HUGO_APPENDPORT=false`. Change `1313:1313` to `443:443` for the `ports` in `docker-compose.yml`.\n4. Spin up your Quartz container and enjoy it at `https://your-domain.com`!\n\n### To a Domain using a Reverse Proxy\n\nIf you want to serve more than just Quartz to the internet on this machine (or don't want to use the Cloudflare registrar and proxy), you should follow the steps in the section above (as appropriate) and also set up a reverse proxy, like [Traefik](https://doc.traefik.io/traefik). Be sure to configure your TLS certificates too!\n","lastmodified":"2023-12-22T01:19:22.725220855Z","tags":["setup"]},"/notes/interesting/Cross-Entropy":{"title":"Cross Entropy","content":"\nhttps://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/interesting/ML-EXPLAINED":{"title":"title","content":"\nhttps://mlu-explain.github.io/logistic-regression/\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/interesting/ML-jobs":{"title":"title","content":"\n\n\nhttps://huyenchip.com/ml-interviews-book/contents/1.1.1-working-in-research-vs.-workingin-production.html\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/interesting/understanding-JS":{"title":"Understanding Jane Street","content":"\nhttps://www.thediff.co/p/jane-street?triedSigningIn=true\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/openPCDet":{"title":"OpenPCDet","content":"# OpenPCDet \n\n## Code Architecture \nfile structure for a project should look something like this:\n\n### Directories\n```bash\nOpenPCDet (or proj name)\n├── data \n│   ├── kitti\n│   ├── lyft\n│   └── waymo\n├── docker\n├── docs\n├── pcdet\n│   ├── datasets\n│   ├── models\n│   ├── ops\n│   └── utils\n└── tools\n    ├── cfgs\n    ├── eval_utils\n    ├── scripts\n    ├── train_utils\n    └── visual_utils\n```\n\n\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/3DETR":{"title":"An End-to-End Transformer Model for 3D Object Detection","content":"\n2020, [link](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)\n\n\n# Intro\n- Many detection models work directly on point clouds\n\t- turning unordered set of inputs (point cloud) into unordered set of outputs (bbx)\n\t- i.e VoteNet with encoder ([PointNet++](notes/papers/PointNet++)) and decoder architecture\n\t\t- effective but required years of careful development of hand-encoding inductive biases, radii, and designing special 3D operators and loss functions.\n- recently, set-to-set encoder-decoder models also emerged in 2D object detection as a competitive method. (i.e. DETR)\n- **Central question:** since transformers are permutation invariant (good for set-to-set problems), can we create a 3D object detector with it w/p hand-designed inductive biases?\n\n\n# Related Work \n\n## Grid-based 3D Architectures\n- convert irregular point clouds into 3D/2D grids (voxels) then apply convnets \n\n\n## Point Cloud Architectures \n- use pointnet++ operations (downsampling + MLPs + maxPool) on point clouds directly\n- construct graphs (i.e: DGCNN, PointWeb)\n- continous point convolutions (pointConv, KPConv)\n\n\n# 3DETR: Encoder-decoder Transformer\n- Input: $N$ points where each point is associated with $x,y,z$ coordinates \n- downsample input to points with $N'$ features via pointnet ops.\n- pass $N'$ features into decoder to output bbx, \n\n## Encoder\n- Input: points $(N,3+C)$\n- SA: $(N,3+C)$ $\\to$  $(N',d)$ where $d=256$ \n\t- via $MLP(64,128,256)$  \n- Attn: $(N',d)\\to(N',d)$ multiple times\n\n\n## Decoder\n- Frame detection as a set prediction problem.\n\t- i.e: predict set of boxes w/o order \n- Parallel decoder takes in $N'$ point features and set of $B$ query embeddings $\\{\\mathbf{q}^e_1,\\ldots,\\mathbf{q}^e_B\\}$ to produce $B$ feautres for bbx. \n- $\\mathbf{q}^e$ represent locations in 3D space around which our final 3D bounding boxes are predicted. \n- positional embeddings is used \n\n\n## Non-parametric query embeddings \n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/3DSSD":{"title":"3DSSD: Point-based 3D Single Stage Object Detector","content":"\n[arXiv link](https://arxiv.org/pdf/2002.10187.pdf) \ngithub #todo \n\n\n## Main Ideas:\n-  Introduced **Feature-Farthest Point Sampling (F-FPS)**\n- Introduced new sampling method called **Fusion Sampling** in [Set Abstraction Layer](notes/papers/PointNet++)\n\n\n### Feature-Farthest Point Sampling (F-FPS)\n- Objective when downsample: \n\t1. Remove **negative points** (background points) \n\t2. Preserve only **positive points** (foreground points, i.e: points within any instance := ground truth box)\n- Therefore leverage semantic features of points as well when applying FPS \n- Given two points $A$ and $B$, the criterion used to compare them in FPS is:$$C(A,B)=\\lambda L_d(A,B)+L_f(A,B)$$\n\t\twhere \n\t\t- $L_d(A,B)$ is $L^2$ euclidean distance (xyz) \n\t\t- $L_f(A,B)$ is $L^2$ feature distance (distance between the two feature vectors)\n\t\t- $\\lambda$ is chosen parameter, paper seems to choose $\\lambda=1$\n\t\t- *reminder*: $L^2(A,B)= \\sqrt{(B_1-A_1)^2+(B_1-A_1)^2+\\cdots+(B_n-A_n)^2}$  if $A$ and $B$ are $n$ dimensional vectors\n- Result should be a subset of points that are less redundant and more diverse, as points are not only physically distant when sampling, but also in feature space.  \n\n### Fusion Sampling\n- Downsampling to $N_m$ points with **F-FPS** results in:\n\t\t- lots of positive points -\u003e good for regression \n\t\t- few negative points (due to limiting ) -\u003e bad for classification\n\t\t- **Why?** Negative points don't have enough neighbours #expand \n- Input: $N_i\\times C_i :=$ $N$ points each with feature vector of length $C$\n- want to output $N_{i+1}$ points, where $N_{i+1}$ points are subset of the $N_i$ points\n\t1. F-FPS$: N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t2. D-FPS:$N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t3. [grouping operation](notes/papers/PointNet++)\n\t4. MLP\n\t5. MaxPool\n\n![[notes/images/3dssdbackbone.png]]\n\n\n\n\n# Network walkthrough\nusing the following network config: ![[notes/images/3dssdcfg.png]]\n\n\n## Network Input:\n- Radar pts of dim 4: $[x,y,z,RCS]$\n- **input size** is determined by `sample_points` in `DATA_PROCESSOR`: e.g.: 512 \n- then the feature dimension is 1 (RCS)\n### Assumptions\n- For each layer, the points remaining $\u003e$ `npoints`. If not, there'll be some padding/repeated points during downsampling.  \n### syntax\n$B$: is batch size\n## 3DSSD Backbone \n### SA_Layer 1 (D-FPS)\n**Input:**\n- xyz: (B,512,3) \u003c- `npoints=512` \n- feature:  (B,1,512) \u003c- 1 feature for 512 pts\n\n**Operations**\n1. D-FPS to sample 512 points\n   \n3. Grouping: create `new_feature_list`\n\t1. ball query with r=0.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t\n\t3. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n\t5. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e) \n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e32+32+64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=128`, `out_channel=64`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\n**Output:**\n- new_xyz: (B,512,3)\n- new_feature: (B,64,512) \u003c- 64 features for 512 pts, etc... \n\n### SA_Layer 2 (FS)\n**Input**\n- xyz: (B,512,3) \u003c-`npoint=512`\n- feature: (B,64,512) \u003c- 64 features from layer 1 \n  \n**Operations**\n1. Sample 512 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e) \n\t- Note: unlike sampling via \\[F-FPS,D-FPS\\] (see next layer), it seems like FS may select the same point more than once. \n2. Grouping\n\t1. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t3. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, then MLP=\\[64,96,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e128+128+128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\t2. Conv1d with `in_channel=384`, `out_channel=128`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,1024,3)\n- new_feature: (B,128,1024)\n\n### SA_Layer 3 (F-FPS, D-FPS) \n- Not sure why we use F-FPS and D-FPS instead of just FS, I think this is to make sure the set of points sampled $F\\text{-}FPS \\cap D\\text{-}FPS =\\emptyset$ ![[notes/images/samplingrange.png]]\n\t- so in this layer, F-FPS samples `[0:512]` pts, then D-FPS samples from `[512:1024]`\n\t- but then wouldn't this cause some sampling bias? if there are good foreground pts in `[512:1024]` then F-FPS cant sample them\n\n**Input**\n- xyz: (B,1024,3)\n- feature: (B,128,1024)\n\n**Operations**\n1. Sample 256 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) \n\n2. Grouping \n\t1. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,128,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\n\t3. ball query with r=3.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,196,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,256,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e256+256+256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=768`, `out_channel=256`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t\t  \n**Output**\n- new_xyz: (B,512,3)\n- new_feature: (B,256,512)\n\n### SA_Layer 4 (F-FPS, D-FPS)\n **THIS IS THE FIRST PART OF CANDIDATE GENERATION**\n**Input**\n- xyz: (B,512,3)\n- feature: (B,256,512)\n\n**Operations**\n  \n**Output:**\n- new_xyz: (B,128,3)\n- new_feature: (B,256,128)\n\n### Vote_Layer (n/a)\n  **THIS IS THE SECOND PART OF CANDIDATE GENERATION** \n \n**Input:**\n- xyz: (B,128,3)\n- feature: (B,256,128)\n  \n**Output:**\n- new_xyz: (B,128,3)\n- new_feature: (B,128,128)\n- ctr_offset: (B,128,3)\n\n### SA_Layer5 (D-FPS)\n**Input**\n- xyz: (B,512,3) \u003c- output of SA_Layer 3\n- feature: (B,256,512) \u003c- output of SA_Layer 3\n- ctr_xyz: (B,127.3) \u003c- output from vote layer \n\n**Operationa**\n1. Sample 128 points via D-FPS (total pts=\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e) \n\n2. Grouping \n\t1. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e16\u003c/span\u003e, then  MLP=\\[256,256,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e16\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e), append to `new_feature_list\n\t3. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[256,512,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e), append to `new_feature_list\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e512+1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e)\n\t2. Conv1d with `in_channel=1536`, `out_channel=512`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,128,3)\n- new_feature: (1,512,128)\n\n# Detection head\n- For box prediction and classification, we take the center features from the last layer of the backbone (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e) reshaped into: (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e) and feed it into two MLPs.\n\n## Box prediction \n- MLP = \\[\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\\]\n1. FC: in_channel=\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e, out_channel=\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e\n2. BN then ReLU\n3. FC: in_channel=\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e, out_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\n4. BN then ReLU \n5. FC: in_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e, out_channel=30 \n\n**Output:** \n- (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,30)\n\n**note**\n- the 30 is:\n\t- (x,y,z,dx,dy,dz), + 2* (12 angle bins ) \n\t\t- $\\times 2$ because angle bin and confidence\n\n## Box classification \n- MLP = \\[\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\\]\n1. FC: in_channel=\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e, out_channel=\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e\n2. BN then ReLU\n3. FC: in_channel=\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e, out_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\n4. BN then ReLU \n5. FC: in_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e, out_channel=3 \u003c- number of classes \n\n**Output**\n- (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,3)\n\n\n## Target assignment\n- After the two sets of MLP above, we need to assign predicted boxes to gt boxes. this is done via `assign_targets(self, input_dict):`\n\n1. Take GT boxes and enlarge them by `GT_EXTRA_WIDTH: [0.2, 0.2, 0.2]`\n2. call   `assign_stack_targets()`  with params:\n\t- `points = centers`, centers are predicted centers with shape (B\\*128,4)\n\t- `gt_boxes = gt_boxes`,   GT boxes with shape (B,G,8) \u003c- G = number of boxes per scene, 8: xy,z,dx,dy,dz,angle,class\n\t- `extend_gt_boxes=extend_gt_boxes`: enlarged GT boxes with shape (B,G,8)\n\t- `set_ignore_flag = True`: not sure what this is used atm...\n\t- `use_ball_constraint = False`: not sure \n\t- `ret_part_labels = False`:  not sure\n\t- `ret_box_labels = True `:  but not sure why\n\t1. For each scene in the batch, run `roiaware_pool3d_utils.points_in_boxes_gpu()` to find the predicted centroids that lie inside a gt box. \n\t\t- return shape is (128): values are either idx of the gt box it lies in, or -1 if its background \n\t\t- create flag: `box_fg_flag = (box_idxs_of_pts \u003e= 0)`\n\t2. if `set_ignore_flag = True`, then do the same for the extended gt boxes. \n\t\t- `fg_flag = box_fg_flag\n\t\t- `ignore_flag = fg_flag ^ (extend_box_idxs_of_pts \u003e= 0)`\n\t\t\t- note that `^` is [xor gate](https://www.maximintegrated.com/content/dam/images/glossary/xor-gate-symbol.jpg) in other flag, we only want to ignore flag points that are **ONLY** in the extended gt box, or **ONLY** in the gt box \n\t\t- then `point_cls_labels_single[ignore_flag] = -1`, \n\t\t- `gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]` \n\t\t\t- `box_idxs_of_pts[fg_flag]`: is a 1D tensor with indices of the gt box each pt lies in. e.g: \\[0,4,2,4,0,1,...\\]\n\t\t\t- then `gt_box_of_fg_points` is a 2D tensor of shape (M,8) where M is the number of pts that are inside a gt box, with the associated gt box. \n\t\t- `point_cls_labels_single[fg_flag] = 1 if self.num_class == 1 else gt_box_of_fg_points[:, -1].long()`\n\t\t\t- \\[-1\\] is the class of the gt, so this gets the label and puts it into a 1D tensor.\n\t3. if `ret_box_labels and gt_box_of_fg_points.shape[0] \u003e 0:`, i.e. if there are points that lie inside gt boxes,\n\t\t- call `fg_point_box_labels =  self.box_coder.encode_torch()` with params:\n\t\t\t-  `gt_boxes=gt_box_of_fg_points[:, :-1]`: so just all boxes, with x,y,z,dx,dy,dz,angle\n\t\t\t- `points=points_single[fg_flag]`, all predicted centers that lie inside gt box\n\t\t\t- `gt_classes=gt_box_of_fg_points[:, -1].long()` class of gt boxes. \n\t\t\t\t- but not used if `self.use_mean = False` , this is set via `'use_mean_size': False` under  `BOX_CODER_CONFIG`\n\t\t- function basically assigned a label based on residuals for each pt based on the gt box it lies in.\n\t\t\t- i.e. the label for each point is different between point (x,y,z) and box center +\n\t\t\t- log() of dx dy dz of gt box\n\t\t\t- bin of angle + residual\n\t\t\t- total = length of 8 \n\t4. `point_box_labels_single[fg_flag] = fg_point_box_labels` \n\t\t-  assign the points their new labels \n\t5. `point_box_labels[bs_mask] = point_box_labels_single`\n\t\t- assign it to the \"outer\" list (where all labels for each sample in the batch will be) \n\t6. after doing this for all samples in batch:  concat all `gt_box_of_fg_points` into a tensor `gt_boxes_of_fg_points` \n\t7.  Return the following dict. \n```\ntargets_dict = {\n'point_cls_labels': point_cls_labels, # (B*128)\n'point_box_labels': point_box_labels, # (B*128,8)\n'point_part_labels': point_part_labels, # None\n'box_idxs_of_pts': box_idxs_of_pts, # (128)\n'gt_box_of_fg_points': gt_boxes_of_fg_points,#(M,8), M is the number of pts that \n\t\t\t\t\t     # are inside a gt box, so this is list\n\t\t\t\t\t\t # since it changes for every batch\n}\n```\n\n \n\n\t\t\n\t\t\t ","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/PCT":{"title":"Point Transformer","content":"\n[link](https://arxiv.org/pdf/2012.09164.pdf)","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/PVRCNN":{"title":"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection","content":"\n\n\n\n\n# Main Ideas\n- Two stage detector that uses the point **AND** voxel features (via sparse convolution) \n- Two novel operations: \n\t- **Voxel-to-keypoint scene encoding:** \n\t\t- which summarizes all the voxels of the overall scene feature volumes into a small number of feature keypoints\n\t- **point-to-grid RoI feature abstraction:** \n\t\t- effectively aggregates the scene keypoint features to RoI grids for proposal confidence prediction and location refinement\n\n# Network Diagram\n![[notes/images/pvrcnn.png]]\n# Feature Encoding \n- Input points **P** are split into voxels $L\\times W \\times H$  in $(x,y,z)$ respectively. \n- A $3\\times 3 \\times 3$  (3D) kernel is used in the sparse convulation.\n\t- result: **P** into feature volumes with $1\\times,2\\times,4\\times,8\\times$ downsampled sizes. \n\t\t- $2\\times$ means the voxel size (or $L/W/H$?) is $2\\times$ larger, so there are few voxels.   \n- One we have the the $8\\times$ downsampled feature volumes $\\left(\\frac{L}{8}\\times \\frac{W}{8}\\times \\frac{H}{8}\\right)$ \n\t- we stack the voxels along the $Z$ axis to get a $\\frac{L}{8}\\times \\frac{W}{8}$ bird-view feature maps.\n\t- \u003cspan style=\"color:red\"\u003ewhat is stacking here \u003c/span\u003e\n- Each class has $2\\times \\frac{L}{8} \\times \\frac{W}{8}$ 3D anchor boxes, and two anchors of $0^\\circ$ and $90^\\circ$ orientations. each anchor box is evaluated for each pixel of the BEV feature map.\n\n\n# Voxel Set Abstraction Module\n- from **P**, we first use [FPS](notes/papers/PointNet++#Sampling Layer ) to sample $n$ keypoints $\\mathcal{K}=\\{p_1,\\ldots,p_n\\}$ \n- Denote: $$\\mathcal{F}^{(l_k)}=\\{f_1^{(l_k)},\\ldots,f_{N_k}^{(l_k)}\\}: \\text{set of voxel-wise feature vectors at in }k\\text{th level}$$ $$\\mathcal{V}^{(l_k)}=\\{v_1^{(l_k)},\\ldots,v_{N_k}^{(l_k)}\\}: \\text{set o}$$ \n- Where $N_k$ is the number of non-empty voxels in the $k$th level\n\nFor each keypoint $p_i$ we find neighboarding non-empty voxels at the $k$th level within radius $r_k$. The resulting set of voxel-wise features vectors is:$$S_{i}^{(l_k)}=\n      \\left \\{\n        [f_j^{(lk)};\\underbrace{v{j}^{(l_k)}-\n        p_i}_{\\text{relative coords}}]^T\n      \\; \\middle | \\;\n      \\begin{array}{cc}\n        \\lVert v_{j}^{(l_k)}-p_i \\rVert^2 \u003c r_{k}, \\\\[1ex]\n        \\forall v_{j}^{(l_k)}\\in \\mathcal{V}^{(l_k)}, \\\\[1ex]\n        \\forall f{j}^{(l_k)}\\in\\mathcal{F}^{(l_k)}\n      \\end{array}\n      \\right\\}$$\n  where:\n  - $v_{j}^{(l_k)}-p_i$ : relative coordinates/location of $f_j^{(l_k)}$\n  - $f_j^{(l_k)}$: semantic voxel feature\n\n- The voxel-wise features within neighboring voxel set $S_{i}^{(l_k)}$ is then passed into a PointNet block:$$f_{i}^{(pv_k)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(S_{i}^{(l_k)}\\right)\\right)\\right\\}\\tag{2}$$\n- where:\n- $M(\\cdot)$: randomly sampling at most $T_k$ voxels from $S_{i}^{(l_k)}$. (for saving computation)\n- $G(\\cdot)$: MLP \n- $\\max(\\cdot)$: max-pooling along channel dimension.\n\t- The result of the block are the features of key point $p_i$ \n\n\nThen for each keypoint $p_i$ , we concatinate its features from different levels\n$$f_i^{(pv)}=\\left[f_{i}^{(pv_1)},f_{i}^{(pv_2)},f_{i}^{(pv_3)},f_{i}^{(pv_4)}\\right], \\text{for } i=1,\\ldots,n$$\nwhere:\n- $f_i^{(pv)}$: is 3D voxel CNN-based feature + pointnet features as explained above\n\n\n# Extended VSA Module\n- With the 8x downsampled 2D BEV feature map, and original point cloud **P**, put **P** through eq 2. We call these features $f_i^{(raw)}$ \n- Also project each $p_i$ onto the BEV map, and use bilinear interpolation to get $f_i^{(bev)}$ from BEV feature map\n\t- \u003cspan style=\"color:red\"\u003eHow exactly?\u003c/span\u003e\n - Then finally concatinate all these features together:$$f_i^{(p)}=\\left[f_{i}^{(pv)},f_i^{(raw)},f_{i}^{(bev)}\\right], \\text{ for }i=1,\\ldots,n$$\n\n\n# Predicted Keypoint Weighting\n![[notes/images/PKW.png]]\n- Because [FPS](notes/papers/PointNet++#Sampling Layer) might chose background points, we want to put more weight into the foreground points + their features. \n- Take the keypoint features, and pass it through a three layer mlp: $\\mathcal{A}(f_i^{(p)})$  with sigmoid as the final layer as illustrated above. \n\t- In training this is supervised via checking if the keypoint is in a GT box or not. i.e.: $p_i$ has label $1$ if $p_i$ in GT, else $0$. Focal loss is used here. \n- weights are then multiplied by the features: $$\\tilde{f_i}^{(p)}=\\mathcal{A}(f_i^{(p)})\\cdot f_{i}^{(p)}$$\n\n# RoI-grid Pooling via Set Abstraction\n- After all the previous steps, we have the set of keypoint features $$\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f_{i}}^{(p)},\\ldots,\\tilde{f_{n}}^{(p)}\\right\\}$$ \n- For each 3D proposal **(RoI)** we need to aggregate the keypoint features\n- We uniformly sample $6\\times6\\times6$ **grid points** within each 3D proposal **(RoI)** denoted as $\\mathcal{G}=\\{g_1,\\ldots,g_{216}\\}$\n\t-  (Grid points need not be points from raw pointcloud **P**) \n\t![[notes/images/roi_pooling.png]]\n- For each *grid point*, we gather all *key point* features within radius $\\tilde{r}$ $$\\tilde{\\Psi}=\\left\\{\\left[\\tilde{f}_{j}^{(p)};p_j-g_i\\right]^T  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert p_{j}-g_i\\rVert^2 \u003c \\tilde{r}, \\\\[1ex]\n    \\forall p_{j}\\in \\mathcal{K}, \\\\[1ex]\n    \\forall \\tilde{f}_{j}^{(p)}\\in\\tilde{\\mathcal{F}}\n  \\end{array}\\right\\}$$\n- $p_j-g_i$: local coordinates of features $\\tilde{f_j}^{(p)}$ relative to grid point $g_i$ \n- pointnet is used again to aggregate features: $$\\tilde{f}_i^{(g)}=\\max\\left\\{G\\left(\\mathcal\\{M\\}\\left(\\tilde\\{\\Psi\\}\\right)\\right)\\right\\}$$\n- Two seperate MLP heads (256 dims) are then used for box refinement and confidence. \n\t- **Box Refinement:** for each of the RoI, it predicts the residuals compared to GT. $$L_{iou}=-y_k\\log(\\tilde{y}_k)-(1-y_k)\\log(1-\\tilde{y}_k)$$\n\t\t- where $\\tilde{y}_k$ is predicted score by network \n\t- **Confidence:** For the $k$th RoI, its confidence for a target $y_k$ is normalized to be between $[0,1]$ $$y_k=\\min(1,\\max(0,2\\text{IoU}_k-0.5))$$\n\t\t- Where $\\text{IoU}_k$ is the IoU of the $k$th RoI w.r.t to tis GT box.\n\t- Both are optimized via smooth-L1  \n\n\n# Training Loss\n- 3 Losses are used:\n\t- Region proposal loss: $L_{rpn}$ $$L_{rpn}=\\underbrace{L_{cls}}_{\\text{focal loss}}+\\beta \\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^a}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^a)$$\n\t\t- predicted residual: $\\widehat{\\Delta r^a}$ \n\t\t- regression target: $\\Delta r^a$\n\t- Keypoint segmentation loss: $L_{seg}$ \n\t\t- loss calculation is explained in the [Predicted Keypoint Weighting](notes/papers/PVRCNN#Predicted Keypoint Weighting)\n\t- Proposal refinement loss: $L_{rcnn}$ $$L_{rcnn}=L_{iou}+\\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^p}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^p)$$\n\t\t- predicted box residual: $\\widehat{\\Delta r^p}$\n\t\t- proposal regression target: $\\Delta r^p$\n- Overall loss is sum of these three with equal loss weights. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/PointNet++":{"title":"PointNet++","content":"\n\n## Set Abstraction (SA) Layer\n![[notes/images/pointnet2.png]]\n\n### Sampling Layer \nLets call the sampling layer $SL$.\n- Input: $N\\times 3$ \n\t- N points with $x,y,z$\n- Output: $M\\times 3$ \n\t- $M$ points with $x,y,z$ where points in $M$ are subset of $N$\n- Method: **Iterative Farthest Point Sampling** (FPS)\n\t1. Start by choosing 1 random point\n\t2. Calculate distance for all remaining points to selected points.\n\t\t1. so each point will have an array keeping track of the distances \n\t\t3. take the minimum of the arrray, i.e. for each remaining point, set its distance to the closest selected point.\n\t\t4. Finally, select the point with the greatest distance\n\t3. Repeat step 2 $M-1$ more times\n\t- **1D Example:** Consider an array of points $P=[1,6,7,8,20]$, \n\t\t1. suppose the list of selected points $S=[7,20]$ , $P$ is now $[1,6,8]$\n\t\t2. we calculate distances:\n\t\t   $$\\begin{align*}dist_1=[6,19]\\\\\n\t\t   dist_6=[1,14]\\\\\n\t\t   dist_8=[1,12]\\\\\n\t\t   \\end{align*}$$\n\t\t3. perform $min (dist_i)$$$\\begin{align*}dist_1=6\\\\\n\t\t   dist_6=1\\\\\n\t\t   dist_8=1\\\\\n\t\t   \\end{align*}$$\n\t\t4. now select the point with highest distance. i.e. $$\\underset{i}{\\operatorname{argmax}}(dist_i)=1$$\n\t\t5. So the next chosen point is 1. $S=[1,7,20]$ and $P=[6,8]$\n\n\n\n### Grouping Layer\n- Sampling Layer $SL$ yields $N'\\times 3$ points.\n\t- $N'$ centroids and $3:= x,y,z$ coordinates \n\t- $SL:(N\\times 3) \\to (N'\\times 3)$\n- Input: \n\t- $(N\\times 3+ C)$: points before sampling and their feature vectors of len $C$\n\t-  $(N'\\times d)$: predicted centroids \n- Output: \n\t- $(N'\\times K \\times (3+C))$: centroid + $K$ points within radius of $r$ along their features\n- Method: \n\t- Ball query\n\t\t\t- what if points within radius is $\u003cK$? \n\t\t\t- what if no points (only centroid point)?\n\t\t\t- with if more than $K$ points?  \n\n#### Multi-scale grouping (MSG)\n- Due to nonuniformity of point clouds, we needs some more tricks to make feature learning more robust. \n- One simple way is to applying grouping + pointnet multiple times then concat their features \n- **Dilated Group:** implementation setting where you set a min radius.\n\t- eg: $r=[0.2,0.4,0.8]$, then \n\t- for $r=0.2$, sample within radius $[0,0.2]$\n\t- for $r=0.4$, sample within radius $[0.2,0.4]$\n\t- for $r=0.8$, sample within radius $[0.4,0.8]$\n![[notes/images/msg.png|300]]\n\n\n\n#### Multi-resolution grouping (MRG)\n- MSG works, but its computationally expensive, so this is an alternative method \n- Consider a layer for input of points, we first apply set abstraction layer (sampling + grouping + pointnet), to yield a vector of features $L_1$. We then  apply pointnet of the raw pointcloud (before set abstraction) to yield feature vector $L_2$. Then we concat the results $(L_1,L_2)$ \n\n![[notes/images/mrg.png|300]]\n### PointNet Layer\nessentially just a FC layer on each \"ball\" of points\n- Input $(N'\\times K \\times (3+C))$.\n\t- $N'$ local regions (balls of points)\n\t- $K$ is num of balls in the ball\n\t- $3+C = x,y,z+C \\text{ features}$\n- Output: $(N'\\times (3+C'))$ \n- Method: \n\t- for each point $x_j$ in a ball, transform it into a local coordinate frame relative to its centroid $\\hat{x}$ $$x^{new}_j = x_j-\\hat{x}$$\n\t- Then we apply series of FC layers\n\n# Feature Propogation for Set Segmentation \n\n\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/PointPainting":{"title":"PointPainting: Sequential Fusion for 3D Object Detection","content":"\n\n\n## Main Ideas:\n- preform 2D semantic segmentation on image, project class of labeld pixeled back onto lidar pts \n\n\n\n\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/TANet":{"title":"TANet: Robust 3D Object Detection from Point Clouds with Triple Attention","content":"\n[link](https://arxiv.org/pdf/1912.05163.pdf)\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/attentionalpointnet":{"title":"Attentional PointNet for 3D-Object Detection in Point Clouds","content":"\n[link](https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf)\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/iassd":{"title":"Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds","content":"\n# Main Ideas \n- Turns out [F-FPS](notes/papers/3DSSD) from 3DSSD is still not good enough in preserving foreground points. \n\t- Introduced Class-aware Sampling\n\n\n\n# Class-Aware Sampling\n- Superior sampling method compared to [F-FPS](notes/papers/3DSSD)  and [D-FPS](notes/papers/PointNet++)\n-  Using vanilla cross-entropy loss $$L_{cls-aware}=-\\sum^{C}_{c=1}(s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n- $C:$ Number of catergories \n- $s_i:$ One hot labels \n- $\\hat{s_i}:$ predicted logits  \n\n\n# Centroid Aware Sampling\n- Give higher weight to points near instance centroid $$Mask_i=\\sqrt[3]{\\frac{\\min(f^*,b^*)}{\\max(f^*,b^*)}\\times \\frac{\\min(l^*,r^*)}{\\max(l^*,r^*)}\\times\\frac{\\min(u^*,d^*)}{\\max(u^*,d^*)}}$$\n- $f^∗, b^∗, l^∗, r^∗, u^∗, d^∗$ represent the distance of a point to the 6 surfaces (front, back, left, right, up and down)\n- This mask is used during **training** via the ctr-aware loss $L_{ctr-aware}$. At inference we simply keep top $k$ points with highest scores. $$L_{ctr-aware}=-\\sum^{C}_{c=1}(Mask_i\\cdot s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n\n# Contextual Centroid Prediction \n\n- bruh $$L_{cent}=\\frac{1}{|\\mathcal{F}_+|}\\frac{1}{|\\mathcal{S}_+|}\\sum_i\\sum_j (|\\Delta\\hat{c_{ij}}-\\Delta c_{ij}|+|\\hat{c_{ij}}-\\overline{c_i}|)\\cdot \\mathbb{1}_S(p_{ij})$$\n- where $$\\overline{c_i}=\\frac{1}{|S_+|}\\sum_j\\hat{c_{ij}}\\quad,\\quad\\mathbb{1}_{S}(p_{ij}):\\mathcal{P}\\to\\{0,1\\}$$\n- $|\\mathcal{F}_+|$ =  the total number of GT boxes used to predict centroids\n- $|\\mathcal{S}_+|$ = number of points used to predict the instance center\n- $\\Delta\\hat{c_{ij}}$ = offset predicted to instance center\n- $\\Delta{c_{ij}}$ = GT offset from a point $p_{ij}$ to center point \n- $\\mathbb{1}_S$ = indicator fn to determine whether point is used \n\t- implementation: expand GT box by some amount, then include all points inside the expanded box.\n\nExamine the $|\\hat{c_{ij}}-\\overline{c_i}|$ term a bit more:\n- Each GT box has one of these.\n- For each relevant point (i.e. points inside the expanded GT box)\n\t- take their average, that becomes $\\overline{c_i}$ \n- then while looping over each predicted centroid, the term is calculated. \n- overall effect seems to promote all assignment points inside a GT box to predict the same centroid location\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/pointformer":{"title":"Pointformer","content":"\n[link](https://arxiv.org/pdf/2012.11409.pdf)","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/papers/unionofmanifold":{"title":"The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling","content":"\n\n# Main Ideas\n\n- The manifold hypothesis states that high-dimensional data of interest often lives in an unknown lower-dimensional manifold embedded in ambient space\n ","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/physics/experimental":{"title":"Chapter 12 notes","content":"\n\n## Driven Damped Pendulum (DDP)\n\n\n$mL^2 \\ddot{\\phi} = \\underbrace{-b L^2 \\dot{\\phi}}_{\\text{torque from resistive force}} - \\underbrace{mgL\\sin \\phi}_{\\text{torque of weight}} + \\underbrace{LF(t)}_{driving force}$\nAssume driving force $F(t)$ is sinusoiodal. $F(T)= F_0\\cos (\\omega t)$\n\n### Properties\n- For a linear damped oscillator with sinusoidal driving force:\n\t1.  $\\exists$ unique attactor which motion approaches despite initial conditions.\n\t2. motion of attactor itself is sinusoidal frequency (equal to drive freq)\n\n## Near Linear Oscillations of DDP\n- As drive strength is increased and nonlinearity becomes more important, pendulum will pick up various harmonics of the drive frequency $\\omega$.\n- DDP in linear and near linear regimes: \n\t- motions approaches unique attactor as initial treansients die out \n\t- then oscillates with same peroid as driver. \n\n\n\n## Difference in initial conditions\n\n- for linear oscillators, its fine if initial conditions vary by some $\\varepsilon$ since the error rapidly approaches zero. (insensitive to intial conditions)\n- for DDP if we are close enough its fine.\n\n\n\n## State Space Orbits \n\n\n\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/notes/search":{"title":"Search","content":"\nQuartz supports two modes of searching through content.\n\n## Full-text\nFull-text search is the default in Quartz. It produces results that *exactly* match the search query. This is easier to setup but usually produces lower quality matches.\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nenableSemanticSearch: false\n```\n\n## Natural Language\nNatural language search is powered by [Operand](https://beta.operand.ai/). It understands language like a person does and finds results that best match user intent. In this sense, it is closer to how Google Search works.\n\nNatural language search tends to produce higher quality results than full-text search.\n\nHere's how to set it up.\n\n1. Login or Register for a new Operand account. Click the verification link sent to your email, and you'll be redirected to the dashboard. (Note) You do not need to enter a credit card to create an account, or get started with the Operand API. The first $10 of usage each month is free. To learn more, see pricing. If you go over your free quota, we'll (politely) reach out and ask you to configure billing.\n2. Create your first index. On the dashboard, under \"Indexes\", enter the name and description of your index, and click \"Create Index\". Note down the ID of the index (obtained by clicking on the index name in the list of indexes), as you'll need it in the next step. IDs are unique to each index, and look something like `uqv1duxxbdxu`.\n3. Click into the index you've created. Under \"Index Something\", select \"SITEMAP\" from the dropdown and click \"Add Source\".\n4. For the \"Sitemap.xml URL\", put your deployed site's base URL followed by `sitemap.xml`. For example, for `quartz.jzhao.xyz`, put `https://quartz.jzhao.xyz/sitemap.xml`. Leave the URL Regex empty. \n5. Get your API key. On the dashboard, under \"API Keys\", you can manage your API keys. If you don't already have an API key, click \"Create API Key\". You'll need this for the next step.\n6. Open `data/config.yaml`. Set `enableSemanticSearch` to `true`, `operandApiKey` to your copied key, and `operandIndexId` to the ID of the index we created from earlier..\n\n```yaml {title=\"data/config.yaml\"}\n# the default option\nsearch:\n  enableSemanticSearch: true\n  operandApiKey: \"jp9k5hudse2a828z98kxd6z3payi8u90rnjf\"\n  operandIndexId: \"s0kf3bd6tldw\"\n```\n7. Push your changes to the site and wait for it to deploy.\n8. Check the Operand dashboard and wait for your site to index. Enjoy natural language search powered by Operand!\n","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]},"/tags/setup":{"title":"setup","content":"","lastmodified":"2023-12-22T01:19:22.745220778Z","tags":[]}}