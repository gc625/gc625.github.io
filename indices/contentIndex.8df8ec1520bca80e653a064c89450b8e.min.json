{"/":{"title":"Gabriel Chan's website","content":"\nHi, this is my website where I keep my notes for research, classes and personal projects. checkout my [github](https://github.com/gc625) too!\n\n\n# Research Notes\n## Point Cloud Networks\n- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](notes/papers/PointNet++)\n### Single Stage Detectors \n- [3DSSD: Point-based 3D Single Stage Object Detector](notes/papers/3DSSD)\n- [(IASSD)Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds](notes/papers/iassd)\n\n### Two Stage Detectors\n- [RVRCNN](notes/papers/PVRCNN)\n\n### Attention on point clouds\n- [3DETR: An End-to-End Transformer Model for 3D Object Detection](notes/papers/3DETR)\n- [Attentional PointNet for 3D-Object Detection in Point Clouds](notes/papers/attentionalpointnet)\n- [Point Transformer](notes/papers/PCT)\n- [TANet: Robust 3D Object Detection from Point Clouds with Triple Attention](notes/papers/TANet)\n- [3D Object Detection with Pointformer](notes/papers/pointformer)\n\n\n# Basic of Machine Learning\n- [Surpervised Learning](notes/back2basics/supervisedlearning)\n- [Unsupervised Learning](notes/back2basics/unsupervisedlearning)\n\n\n# Machine Learning frameworks\n- [OpenPCDet](notes/openPCDet)\n\n","lastmodified":"2022-09-10T03:14:34.135618082Z","tags":null},"/notes/Computer-Organization/Class-Notes":{"title":"Comp org","content":"\n\n# Lecture 3 (Sept 8)\n\n**Program Counter:** (PC) holds address of the next instruction\n\n**Instruction Register:** (IR) holds the instruction currently being executed\n\n\n## Fetch-Decode-Execute Cycle\n1.  Fetch instruction from PC -\u003e IR\n2. update PC\n3. decode instruction from IR, determind if operand needed from memory\n4. If yes, fetch from register\n5. execute instruction\n6. repeat 1-5 \n\n\n\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/Computer-Organization/HW1":{"title":"HW1","content":"\n# Q2 \n\n\nConnector Numer | Name of Connector | function \n-- | -- | --\n0 | PS/2 port | legacy port used for mouse and keyboards \n1 | USB type A (Female) | Universal Serial Bus (USB) port for connecting other devices to the computer, e.g. for data/power transfer\n2 | hdmi | a digital display port to connect to a monitor to output video (and audio)\n3 | VGA | analog display port to displaying video only (old)\n4 | DVI | digital and analog display port for video \n5 | SPDIF optical audio port | port for transfering digital audio between computer to another device without needing to convert to analog signal first\n6 | Thunderbolt port | bi-directional port that can be used for audio, data (10 Gigabit/s), power, and video transfer\n7 | SATA port | port (typically internal) for connecting storage devices to a computer \n8 | RAM slots | slots for RAM sticks to be inserted. RAM is used for low latency data storage for compute tasks \n9 | ethernet port | port used to connect ethernet cable for connecting the computer to a network.  \n10 | 24pin power slot  | internal power supply connector used to power components on the motherboard\n11 | PCI-E expansion slots | slots for connecting additional components, typically peripheral cards like a graphics card or networking card\n\n\n# Q3 \n$M  = 3000000$\n- Type A: 20% of instructions, 25ns \n- Type B: 45% of instructions, 40ns \n- Type C: 35% of instructions, 20ns \n\n## A)\nSince $$\\begin{align}I_6=\u0026\\prod^6_{k=1}i_k\\\\=\u0026\\ 4\\cdot2\\cdot3\\cdot4\\cdot3\\\\ =\u0026\\ 288\\end{align}$$\nso one instruction at level 6 is equivalent to 288 at level 1, and the program will have $I_6M = 864000000$ level 1 instructions. \n\n## B)\nAverage instruction time at level 1 is then the weighted sum of all instruction types and their time. $$\\begin{align}t_1=\u0026\\ 0.2(25)+0.45(40)+0.35(20)\\\\ =\u0026\\ 30\\end{align}$$\n## C) \nRecall that $t_n=I_nt_1$, so \n\n$$\\begin{align}\nI_3 =\u0026\\  4*3 = 12\\\\\nt_3 =\u0026\\ 12\\cdot 30 \\\\\n=\u0026\\ 360 \\text{ ns}\n\\end{align}$$\n\n## D) \nsame idea as c) \n$$\n\\begin{align}\nI_6 =\u0026\\ 288 \\\\\nt_6 =\u0026\\ 288*30\\\\\n=\u0026\\ 8640 \\text{ ns}\n\\end{align}\n$$\n\n## E)\nsince $T_{prog}=Mt_N$ with $N=6$ in this case\n$$\\begin{align}T_{prog}=\u0026\\ 3000000*8640\\text{ ns}\\\\\n=\u0026\\ 2.592\\cdot10^{10} \\text{ ns} \\\\ =\u0026\\ 25.92 \\text{ sec}\\end{align}$$\n\n## F) \n\n$$\\begin{align}I_{6,new}=\u0026\\ 4\\cdot2\\cdot3\\cdot4\\cdot2\\\\\n=\u0026\\ 192\\end{align}$$\nSince everything else is the same, ratio $I_{6,new}/I_{6,old}= 192/288$ which is $2/3$. So new program can complete the same task in ~66% of the time \n\n\n# Q4 \nAssume $M$ instructions at level $n$, since each instruction at level $n$ is translated into $S$ instructions at $n-1$ level, we have $MS^{5}$ instructions at level 1. In general, we have $MS^{n-1}$ level 1 instructions . \n\nIf the translation was optimal, we would have $MW^5$ and $MW^{n-1}$  instructions instead (level 6 and in general). Threfore the ratio is \n$$\\left(\\frac{S}{W}\\right)^{5}$$\nand in general \n$$\\left(\\frac{S}{W}\\right)^{n-1}$$\nfor $n$ levels. \n\n\n\n# Q5 \n\nChip/Device | Component Class | Approximate Price\n-- | -- | -- \nAMD Ryzen Threadripper PRO 5995WX | CPU |  $6499\nIntel Xeon W-3175X Skylake X 28 | CPU | $2000\nIntel 10PK OPTANE 800P SERIES | Storage | $150\nNVIDIA GEFORCE RTX 3090 Ti | GPU/PICE card | $1990\nCorsair CX Series CX450M 450 Watt  | Power Supply | $50\nASUS WS C422 SAGE/10G  | (server) motherboard |  $700\nAMD Radeon Pro WX 9100  | GPU | $2000\nIntel Core i7 i7-7700K | CPU | $350\nWD 40TB My Cloud PR4100 Pro Series  | NAS drive | $2000\nIntel Core i9-7980XE X-Series | CPU | $1200\nCisco 32GB DDR4-2666-MHZ RDIMM PC4-21300 DUAL | RAM | $400\nNVIDIA QUADRO RTX 8000  | GPU |  $6000 \nSeagate BarraCuda ST3000DM008 3TB | Hard drive | $80\nKingston 16GB DDR4, 2133MHz DIMM - KVR21R15D4/16 | RAM |  $50\nAMD Phenom II X2 560 | CPU | $20 \nNVIDIA Tesla K80 24GB  | GPU | $100 (used)\nCreative Sound Blaster Z Series ZXR |Sound Card/PCIE card| $80\n\n\n\n\n\n# Q6\n![[notes/images/moores_law.png]]\n\n\n```python\nimport pylab\nimport matplotlib.pyplot as plt\n\na = [pow(10, i) for i in range(10)]\nfig = plt.figure()\nax = fig.add_subplot(111)\nI = 15*3000\n\nx = np.linspace(0,20,6)\ny = I*2**(x/1.5)\ny2 = I*2**(x/2)\n\nline, = ax.plot(x,y,color='blue', lw=2,label=\"18 months\",marker=\"o\")\nline, = ax.plot(x,y2,color='red', lw=2,label=\"2 years (24 months)\",marker=\"o\")\n\nfor i in range(1,6):\n\tax.annotate( \"{:.2E}\".format(y[i]), (x[i], y[i]),(0.85*x[i], 1.25*y[i]))\n\tax.annotate( \"{:.2E}\".format(y2[i]), (x[i], y2[i]),(0.9*x[i], 0.4*y2[i]))\n\nax.legend()\nax.set_yscale('log')\nax.set_xlabel('years')\nax.set_ylabel('number of transistors')\nax.set_title('Number of transistors in chip area=15A')\nplt.savefig(\"moores_law.png\",facecolor=\"white\",dpi=1000)\npylab.show()\n\n```\n\n## Q6B\n\nLet length at year 0 $L_0$ and length at year 24 $L_{24}$.\n\n## doubling every 2 years\n$n_{doubling}=\\frac{24}{2}$\n$$\\begin{align}L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{n_{doubling}}}}\\\\L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{12}}}\\\\ ratio =\u0026\\ \\frac{L_{24}}{L_0}=\\frac{1}{\\sqrt{2^{12}}}\\\\=\u0026\\ 0.015625 \\end{align}$$\n\n## doubling every 18 months\n$n_{doubling}=\\frac{24}{1.5}=16$\n$$\\begin{align}L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{n_{doubling}}}}\\\\L_{24}=\u0026\\ \\frac{L_0}{\\sqrt{2^{16}}}\\\\ ratio =\u0026\\ \\frac{L_{24}}{L_0}=\\frac{1}{\\sqrt{2^{16}}}\\\\=\u0026\\ 0.00390625 \\end{align}$$","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/Grad-School/people-to-email":{"title":"people to email","content":"\n# Stanford \n- Davis Rempe \\[ [Leonidas Guibas](https://geometry.stanford.edu/member/guibas/)\\]: https://davrempe.github.io//\n\n\n# Princeton\n- https://www.cs.princeton.edu/~jiadeng/\n\n\n# UPenn \n- grasp lab: https://www.grasp.upenn.edu/\n\n# MIT \n- daniela rus?\n- http://groups.csail.mit.edu/vision/welcome/\n\n# UW \n[ROBOT LEARNING LAB @UW](https://robotlearning.cs.washington.edu/)\n\n\n# Berkeley \n- https://rll.berkeley.edu/research.html\n\n\n# UIUC\nhttp://shenlong.web.illinois.edu/\n\n# Brown\nhttps://chensun.me/\nhttp://cs.brown.edu/people/ssrinath/research.html\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/HonorsSeminar":{"title":"Research Seminar Ideas","content":"\n\n# All Point Cloud Transformer papers\n\n- [3DETR](notes/papers/3DETR)\n- [Pointformer](notes/papers/pointformer)\n- [Point cloud transformer](notes/papers/pointformer)\n- [Attentional Pointnet](notes/papers/attentionalpointnet)\n- [Triple Attention Net](notes/papers/TANet)\n\n\nNLP book: https://slds-lmu.github.io/seminar_nlp_ss20/\n\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/ML-and-Data-Mining/class-notes":{"title":"Class notes","content":"\n\n# What is ML?\n- study of algorithms and statistical models to perform a **task** without **explicit instructions**\n\n# What is Data Mining?\n- discovering patterns in large datasets \n- mix of cs and stats, goal of extracting info and structure of data\n\n## ML vs Data Mining?\n- ML learning tools are evaluated based on ability to reproduce known knowledge\n- Data mining is focused on discovering previously unknown knowledge\n\n## Data Mining vs Data analysis?\n- Data analysis: testing models on dataset \n- Data Mining: finding hidden patterns in large num of data\n\t- dont necessarily need to know what youre looking for\n\n\n\n# Problems in ML \n- Consider your data: Continuous or discrete?\n- Classification or Regression?\n- Abundant or Rare (sparse)?\n- ","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/Math-Seminar":{"title":"Math Seminar","content":"\n--\n\n$\\newcommand{\\R}{\\mathbb{R}}$\n$\\newcommand{\\calH}{\\mathcal{H}}$\n$\\newcommand{\\la}{\\langle}$\n$\\newcommand{\\ra}{\\rangle}$\n--\n# Hilbert Spaces\n- A [Vector Space](\u003cnotes/definitions#Vector Spaces\u003e) equipped with an [inner product](\u003cnotes/definitions#Inner Product\u003e) which defines a [distance function](\u003cnotes/definitions#Metric\u003e) for which it is a [complete metric space](\u003cnotes/definitions#Complete Metric Space\u003e)\n- In a Hilbert Space, we use the norm $\\|x\\|=\\sqrt{\\langle x, x\\rangle}$\n- and the metric is defined as: $$d(x,y)=\\|x-y\\|=\\sqrt{\\langle x-y,x-y\\rangle}$$\n\n# Repoducing Kernel Hilbert Spaces\nFor a set $X$, let $\\mathbb{R}^X$ denote the set of functions $X \\mapsto \\mathbb{R}$ . We endown $\\mathbb{R}^X$ with the foowing operations: $$(f_1+f_2)(x)=f_1(x)+f_2(x)\\quad,\\quad(a\\cdot f)(x)=af(x)$$\nfor it to become a vector space. \n\n## Evaluation functionals\nIn this case, linear functionals, as defined as members of the [dual space](\u003cdefinitions#Dual Spaces\u003e)  of $\\mathbb{R}^X$, are linear functions of the form $$\\varphi:\\mathbb{R}^X\\to\\mathbb{R}$$A specfial linear functional $e_x$ called the **evaluation functional**, sends a function $f$ to its value at a point $x$:$$e_x(f)=f(x)$$\nWhen this special functional is bounded, the set $\\mathbb{R}^X$ takes on a lot of structure.\n\n## RKHS Definition\nLet $X$ be a nonempty set. $\\mathcal{H}$ is a Reproducing Kernel Hilbert Space on $X$ if:\n\n1. $\\calH$  is a vector subspace of $\\R^X$\n2. $\\calH$ is equipped with inner product $\\la \\cdot,\\cdot\\ra$ \n\t- so $\\calH$ is a Hilbert Space\n3. For all $x\\in X$, the linear evaluation functional $e_x:\\calH\\to \\R$ is bounded.\n\n\n# Riesz Representation Theorem \nIf $\\phi$ is a bounded [linear functional](\u003cdefinitions#Linear Functional\u003e) on a hilbert space $\\calH$, then there is a unique $g\\in\\calH$ such that $$\\phi(f)=\\la g,f\\ra$$for all $f\\in\\calH$\n\n## Collary 1\nLet $\\calH$ be a RKHS on $X$. For every $x\\in X$, there exists a unique $k_x\\in \\calH$ such that $$\\la k_x,f\\ra=f(x)$$for all $f\\in\\calH$\n\n### Note\n- $k_x$ and $f$ are functions that sends $X\\to\\R$.  so $\\la k_x,f\\ra$ evaluates to $\\R$. \n- this collary is just saying that the Riesz Representation Theorem applies to RKHS\n\n\n# Reproducing Kernel\nThe function $K: X\\times X \\to\\R$ defined by $$K(x,y)=k_y(x)$$\nis called the reproducing kernel of $\\calH$.\n\n# (Theorem) Equivalence Between Kernels and RKHS\nEvery RKHS has a unique reproducing kernel, and every reproducing kernel induces a unique RKHS.\n\n- RKHS induces Reproducing Kernel\n\t- follows by Riesz Representation Theorem \n- Reproducing Kernel induces RKHS \n\t- follows by Cauchy-Schwartz: If $K$ is a reproducing jernel on Hilbert space $\\calH$, then $$e_x(f)=\\la k_x,f\\ra\\leq\\|k_x\\|\\|f\\|=\\sqrt{K(x,x)}\\cdot \\|f\\|$$\n\t- so $e_x$ is bounded, ans $\\calH$ is an RKHS\n\n\n\n# Examples of RKHS \n\n## Linear Functions in $\\R^d$\n- Let $\\calH= \\R^d$ with canonical basis vectors $e_1,\\ldots,e_d$, and the standard inner product $$\\la x,w\\ra=\\sum^n_{i=1}x_iw_i$$\n- $X$ is the discrete set $\\{1,\\ldots,d\\}$ \n- $e_i\\in\\calH$ are the kernel functions, since $$\\la e_i,x\\ra= x(i)=x_i$$\n- ","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/back2basics/Decision-and-Classification-Trees":{"title":"Decision and Classification Trees","content":"\n- Decision Tree makes a statement, then makes a statement based on the truth of the statement\n- ## Classification Tree\n- classifies things into categories\n\t- If True, go left \u003c- convention \n\t- top tree is root node\n\t- leaves have nodes pointing towards them, none leaving.\n\t- else its a branch\n- **Impure:** Leaves that contain true and false outcomes \n- Gini Impurity for leaf: $$=1-(\\text{P(Yes)})^2-(\\text{P(No)})^2$$\n- Total **Gini Impurity** = weighted average of Gini Impurities for leaves\n\t- weight of leaf = number of ppl in leaf / all ppl in other leafs\n- predictor with lowest gini impurity is chosen as root node\n## Regression Tree \n- predicts numeric values \n- split data into two groups by finding the threshold that gives the smallest sum of squared residuals\n- typically we set a minimum number of observations to allow for a split\n\t- common threshold is 20\n\t- i.e. leaves can have most 20 observations \n- Calculate sum of squared residuals for all predictors, candidate with lowest value is root node\n\n## Pruning Regression Tree\n- Cost Complexity Pruning \n- Calculate sum of squared residuals for all possible subtrees \n\t- calculate SSR for all leaves and sum \n- Tree score = $SSR +\\alpha T$\n\t- T = number of leaves\n- To find $\\alpha$, we increase it until pruning leaves increases tree score \n\n\n## AdaBoost\n\nAmount of Say = $\\frac{1}{2}\\log{\\left(\\frac{1-\\text{Total Error}}{\\text{Total Error}}\\right)}$\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/back2basics/supervisedlearning":{"title":"Supervised Learning","content":"\n\n# What is supervised learning?\n\n- Training an algorithm to output $y$ for a given $x$ using sufficient training samples $\\{(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\\ldots,(x^{(n)},y^{(n)})\\}$ for some input $x^{(i)}$ and **correct** output $y^{(i)}$ \n- **Regression:** predicting an a number (infinitely many outputs)\n- **Classification:** predicting categories (finite outputs)\n\n\n# Linear Regression \n- Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\\hat{y}$ given an input $x$ \n- for linear regression, $f$ is a straight line. With parameters $w,b$ we can then represent $f$ as: $$f_{w,b}(x)=wx+b$$\n## Cost Function\n- Since our objective to find $w,b$ such taht $\\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$. \n- Squared error cost function:  $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$\n- where $m=$ number of training examples. So $\\frac{1}{m}$ is to average it so it doesnt blow up, factor of $2$ is for computational convience later. \n- Now we can also rewrite it as: $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}f_{w,b}(x^{(i)})-y^{(i)})^2$$\n- This can be solved analytically for simple cost functions, but for complicated $J$, we can use gradiant descent to minimize $J$ instead: \n\n## Gradient Descent\n- initialize $w,b$, calcuated $J$ \n- adjust $w,b$ to decrease $J$ \n- repeat until hopefully $J$ settles near minimum \n\n- Step 1: ($=$ here is assignment, not equals)\n$$w=w -\\alpha \\frac{d}{dw}J(w,b)$$ where $\\alpha$ is the learning rate, a **hyperparameter** that controls the \"fast\" we change $w$ \n- Step 2: do the same for $b$ $$b=b -\\alpha \\frac{d}{db}J(w,b)$$\n- **Note:** $w$ and $b$ must be updated at the same time. \n### Learning rate\n- If $\\alpha$ is too small, then it will take many steps to reach minimum \n- If $\\alpha$ is too large, then it might never reach the minimum \n\n# Gradient Descent for linear regression\nCalculating derivatives   for $w$, $$\\frac{d}{dw}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$=\\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)}-y^{(i)})x^{(i)}$$\n\nand derivative for $b$, \n$$\\frac{d}{db}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$ =\\frac{d}{db}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})$$\n\nPsuedocode for gradient descent:\n```python\nwhile J not converged:\n\tw = w - a * dJdW\n\tb = b- a * dJdb\n```\nwhere `dJdW` = $\\frac{d}{dw}J(w,b)$ and `dJdb` = $\\frac{d}{db}J(w,b)$\n\n\n\n# Multiple features\nwhat if you have multiple features (variables)? \n\n- $x_j = j^{th}$ feature\n- $n$ = number of features\n- $\\vec{x}^{(i)}$= features of $i^{th}$ training example\n- $x_{j}^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n\nWe can then express the linear regression model as:\n\n$$f_{w,b}(x)=w_1x_1+w_2x_2+\\cdots+w_nx_n+b$$\ndefine $\\vec{w} = [w_1,\\ldots,w_n]$ and $\\vec{x}=[x_1,\\ldots x_n]^T$, $T$ here represents transpose. Then \n$$f_{\\vec{w},b}=\\vec{w}\\cdot \\vec{x}+b$$Where $(\\cdot)$ represents the dot product. \n\n\n\n# Feature Scaling\nWhen the range of values your features can take up differ greatly, i.e. \n- $x_1$ = square footage of house $\\in [500,5000]$ \n- $x_2$ = number of bedrooms $\\in [1,5]$\n\nthis may cause gradient descent to run slowly. ![[notes/images/feature_scaling.png]]\n\nSome examples of feature scaling\n## max scaling\n- divide each data point for a feature by the max value for that feature.\n\n## mean normalization\n- eg: if $300 \\leq x_1 \\leq 2000$ , we can scale it like such $$x_{1new} = \\frac{x_1-\\mu_1}{2000-300}$$\n- where $\\mu_1$ = mean\n\n\n## Z-score normalization \n- find standard deviation $\\sigma$ , mean $\\mu$ then $$x_1=\\frac{x_1-\\mu_1}{\\sigma_1}$$\n\n\n\n\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/back2basics/unsupervisedlearning":{"title":"Unsupervised Learning","content":"\n\n# What is Unsupervised Learning\n\n- no labelled data, algorithm finds something interesting in unlabeled data.\n\t- given only inputs $\\{x_0,\\ldots,x_n\\}$, but no output labels $\\{y_0,\\ldots,y_n\\}$\n- **Clustering:**  for example, groups data points together\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/definitions":{"title":"Definitions","content":"\n\n\n# Dual Spaces\nGiven vector space $V$ over field $F$, its dual space $V^*$ is the set of all [linear transformation](\u003cdefinitions/Linear Transformations\u003e) $$\\varphi: V\\to F$$Note that $\\varphi$ is called a linear functional (or linear form, one-form) \n\nIn other words, $V^*$ is just the set of all functions that take in a vector $v\\in V$, and spits out an element in $F$ \n\n\n# Linear Functional\nA linear functional, linear form, or one-form on a vector space $V$ over $\\mathbb{R}$ is a linear transformation $$\\varphi:V\\to\\mathbb{R}$$\nit is bounded if there exists an $M\u003e0$ such that $$\\|\\varphi(x)\\|\\leq M\\|x\\|$$ for all $x\\in V$\n\n# Linear Transformations\nGiven two vector spaces $V$ and $W$ over field $F$. A mapping $$f:V\\to W$$ is called linear (or $F$-linear, homomorphism of $F$-vector spaces) if \n1.  $f(v_1+v_2)=f(v_1)+f(v_2)$\n2.  $f(\\lambda v_1)=\\lambda f(v_1)$\n\n## Variations\n- **Isomorphism**: Bijective linear transformations\n- **Endomorphism**: Linear map from vector space to itself\n- **Automorphism**: Bijective endomorphism\n\n\n\n# Vector Spaces\n- A set $V$ that is closed under finite vector addition and scalar multiplication\n\n# Inner Product\nA mapping on a vector space $V$ over a field $F$: $$\\langle\\cdot,\\cdot\\rangle: V\\times V \\to F$$\nwith \n1. conjugate symmetry: $\\langle x,y\\rangle = \\overline{\\langle x,y\\rangle}$\n2. linearity in the first argument: $\\langle ax+by,z\\rangle = a\\langle x,z\\rangle+b\\langle y,z\\rangle$\n3. Positive-definiteness: $\\langle x,x\\rangle \\geq 0$\n\n# Metric\nor distance function, is a function $$d: X\\times X \\to \\mathbb{R}$$\nsuch that for all $x,y,z\\in X$, the metric has the following properties\n1. Identity of indiscernibles: $d(x,y)=0 \\iff x=y$ \n2. Symmetry: $d(x,y)=d(y,x)$\n3. Triangle Inequality:$d(x,z)\\leq d(x,y)+d(y,z)$\n\n# Complete Metric Space\nA metric space $M$ where every [Cauchy sequence](\u003cdefinitions#Cauchy Sequence\u003e) of points in $M$ converges to a limit in $M$.\n\n\n# Cauchy Sequence\nIn a metric space $(X,d)$, a sequence $$x_1,x_2,x_3,\\ldots$$where for all real number $\\varepsilon\u003e0$ there is a positive integer $N$ such that for all $m,n\u003eN$:\n$$d(x_m,x_n)\u003c\\varepsilon$$\n\n\n\n","lastmodified":"2022-09-10T03:14:34.187618368Z","tags":null},"/notes/interesting/Cross-Entropy":{"title":"Cross Entropy","content":"\nhttps://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/interesting/ML-EXPLAINED":{"title":"title","content":"\nhttps://mlu-explain.github.io/logistic-regression/\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/interesting/ML-jobs":{"title":"title","content":"\n\n\nhttps://huyenchip.com/ml-interviews-book/contents/1.1.1-working-in-research-vs.-workingin-production.html\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/interesting/understanding-JS":{"title":"Understanding Jane Street","content":"\nhttps://www.thediff.co/p/jane-street?triedSigningIn=true\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/openPCDet":{"title":"OpenPCDet","content":"# OpenPCDet \n\n## Code Architecture \nfile structure for a project should look something like this:\n\n### Directories\n```bash\nOpenPCDet (or proj name)\n├── data \n│   ├── kitti\n│   ├── lyft\n│   └── waymo\n├── docker\n├── docs\n├── pcdet\n│   ├── datasets\n│   ├── models\n│   ├── ops\n│   └── utils\n└── tools\n    ├── cfgs\n    ├── eval_utils\n    ├── scripts\n    ├── train_utils\n    └── visual_utils\n```\n\n\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/3DETR":{"title":"An End-to-End Transformer Model for 3D Object Detection","content":"\n2020, [link](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)\n\n\n# Intro\n- Many detection models work directly on point clouds\n\t- turning unordered set of inputs (point cloud) into unordered set of outputs (bbx)\n\t- i.e VoteNet with encoder ([PointNet++](notes/papers/PointNet++)) and decoder architecture\n\t\t- effective but required years of careful development of hand-encoding inductive biases, radii, and designing special 3D operators and loss functions.\n- recently, set-to-set encoder-decoder models also emerged in 2D object detection as a competitive method. (i.e. DETR)\n- **Central question:** since transformers are permutation invariant (good for set-to-set problems), can we create a 3D object detector with it w/p hand-designed inductive biases?\n\n\n# Related Work \n\n## Grid-based 3D Architectures\n- convert irregular point clouds into 3D/2D grids (voxels) then apply convnets \n\n\n## Point Cloud Architectures \n- use pointnet++ operations (downsampling + MLPs + maxPool) on point clouds directly\n- construct graphs (i.e: DGCNN, PointWeb)\n- continous point convolutions (pointConv, KPConv)\n\n\n# 3DETR: Encoder-decoder Transformer\n- Input: $N$ points where each point is associated with $x,y,z$ coordinates \n- downsample input to points with $N'$ features via pointnet ops.\n- pass $N'$ features into decoder to output bbx, \n\n## Encoder\n- Input: points $(N,3+C)$\n- SA: $(N,3+C)$ $\\to$  $(N',d)$ where $d=256$ \n\t- via $MLP(64,128,256)$  \n- Attn: $(N',d)\\to(N',d)$ multiple times\n\n\n## Decoder\n- Frame detection as a set prediction problem.\n\t- i.e: predict set of boxes w/o order \n- Parallel decoder takes in $N'$ point features and set of $B$ query embeddings $\\{\\mathbf{q}^e_1,\\ldots,\\mathbf{q}^e_B\\}$ to produce $B$ feautres for bbx. \n- $\\mathbf{q}^e$ represent locations in 3D space around which our final 3D bounding boxes are predicted. \n- positional embeddings is used \n\n\n## Non-parametric query embeddings \n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/3DSSD":{"title":"3DSSD: Point-based 3D Single Stage Object Detector","content":"\n[arXiv link](https://arxiv.org/pdf/2002.10187.pdf) \ngithub #todo \n\n\n## Main Ideas:\n-  Introduced **Feature-Farthest Point Sampling (F-FPS)**\n- Introduced new sampling method called **Fusion Sampling** in [Set Abstraction Layer](notes/papers/PointNet++)\n\n\n### Feature-Farthest Point Sampling (F-FPS)\n- Objective when downsample: \n\t1. Remove **negative points** (background points) \n\t2. Preserve only **positive points** (foreground points, i.e: points within any instance := ground truth box)\n- Therefore leverage semantic features of points as well when applying FPS \n- Given two points $A$ and $B$, the criterion used to compare them in FPS is:$$C(A,B)=\\lambda L_d(A,B)+L_f(A,B)$$\n\t\twhere \n\t\t- $L_d(A,B)$ is $L^2$ euclidean distance (xyz) \n\t\t- $L_f(A,B)$ is $L^2$ feature distance (distance between the two feature vectors)\n\t\t- $\\lambda$ is chosen parameter, paper seems to choose $\\lambda=1$\n\t\t- *reminder*: $L^2(A,B)= \\sqrt{(B_1-A_1)^2+(B_1-A_1)^2+\\cdots+(B_n-A_n)^2}$  if $A$ and $B$ are $n$ dimensional vectors\n- Result should be a subset of points that are less redundant and more diverse, as points are not only physically distant when sampling, but also in feature space.  \n\n### Fusion Sampling\n- Downsampling to $N_m$ points with **F-FPS** results in:\n\t\t- lots of positive points -\u003e good for regression \n\t\t- few negative points (due to limiting ) -\u003e bad for classification\n\t\t- **Why?** Negative points don't have enough neighbours #expand \n- Input: $N_i\\times C_i :=$ $N$ points each with feature vector of length $C$\n- want to output $N_{i+1}$ points, where $N_{i+1}$ points are subset of the $N_i$ points\n\t1. F-FPS$: N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t2. D-FPS:$N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t3. [grouping operation](notes/papers/PointNet++)\n\t4. MLP\n\t5. MaxPool\n\n![[notes/images/3dssdbackbone.png]]\n\n\n\n\n# Network walkthrough\nusing the following network config: ![[notes/images/3dssdcfg.png]]\n\n\n## Network Input:\n- Radar pts of dim 4: $[x,y,z,RCS]$\n- **input size** is determined by `sample_points` in `DATA_PROCESSOR`: e.g.: 512 \n- then the feature dimension is 1 (RCS)\n### Assumptions\n- For each layer, the points remaining $\u003e$ `npoints`. If not, there'll be some padding/repeated points during downsampling.  \n### syntax\n$B$: is batch size\n## 3DSSD Backbone \n### SA_Layer 1 (D-FPS)\n**Input:**\n- xyz: (B,512,3) \u003c- `npoints=512` \n- feature:  (B,1,512) \u003c- 1 feature for 512 pts\n\n**Operations**\n1. D-FPS to sample 512 points\n   \n3. Grouping: create `new_feature_list`\n\t1. ball query with r=0.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t\n\t3. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n\t5. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e) \n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e32+32+64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=128`, `out_channel=64`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\n**Output:**\n- new_xyz: (B,512,3)\n- new_feature: (B,64,512) \u003c- 64 features for 512 pts, etc... \n\n### SA_Layer 2 (FS)\n**Input**\n- xyz: (B,512,3) \u003c-`npoint=512`\n- feature: (B,64,512) \u003c- 64 features from layer 1 \n  \n**Operations**\n1. Sample 512 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e) \n\t- Note: unlike sampling via \\[F-FPS,D-FPS\\] (see next layer), it seems like FS may select the same point more than once. \n2. Grouping\n\t1. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t3. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, then MLP=\\[64,96,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e128+128+128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\t2. Conv1d with `in_channel=384`, `out_channel=128`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,1024,3)\n- new_feature: (B,128,1024)\n\n### SA_Layer 3 (F-FPS, D-FPS) \n- Not sure why we use F-FPS and D-FPS instead of just FS, I think this is to make sure the set of points sampled $F\\text{-}FPS \\cap D\\text{-}FPS =\\emptyset$ ![[notes/images/samplingrange.png]]\n\t- so in this layer, F-FPS samples `[0:512]` pts, then D-FPS samples from `[512:1024]`\n\t- but then wouldn't this cause some sampling bias? if there are good foreground pts in `[512:1024]` then F-FPS cant sample them\n\n**Input**\n- xyz: (B,1024,3)\n- feature: (B,128,1024)\n\n**Operations**\n1. Sample 256 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) \n\n2. Grouping \n\t1. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,128,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\n\t3. ball query with r=3.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,196,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[128,256,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e256+256+256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=768`, `out_channel=256`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t\t  \n**Output**\n- new_xyz: (B,512,3)\n- new_feature: (B,256,512)\n\n### SA_Layer 4 (F-FPS, D-FPS)\n **THIS IS THE FIRST PART OF CANDIDATE GENERATION**\n**Input**\n- xyz: (B,512,3)\n- feature: (B,256,512)\n\n**Operations**\n  \n**Output:**\n- new_xyz: (B,128,3)\n- new_feature: (B,256,128)\n\n### Vote_Layer (n/a)\n  **THIS IS THE SECOND PART OF CANDIDATE GENERATION** \n \n**Input:**\n- xyz: (B,128,3)\n- feature: (B,256,128)\n  \n**Output:**\n- new_xyz: (B,128,3)\n- new_feature: (B,128,128)\n- ctr_offset: (B,128,3)\n\n### SA_Layer5 (D-FPS)\n**Input**\n- xyz: (B,512,3) \u003c- output of SA_Layer 3\n- feature: (B,256,512) \u003c- output of SA_Layer 3\n- ctr_xyz: (B,127.3) \u003c- output from vote layer \n\n**Operationa**\n1. Sample 128 points via D-FPS (total pts=\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e) \n\n2. Grouping \n\t1. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e16\u003c/span\u003e, then  MLP=\\[256,256,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e16\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e), append to `new_feature_list\n\t3. ball query with r=4.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[256,512,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e), append to `new_feature_list\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e512+1024\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e)\n\t2. Conv1d with `in_channel=1536`, `out_channel=512`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,128,3)\n- new_feature: (1,512,128)\n\n# Detection head\n- For box prediction and classification, we take the center features from the last layer of the backbone (B,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e) reshaped into: (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e) and feed it into two MLPs.\n\n## Box prediction \n- MLP = \\[\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\\]\n1. FC: in_channel=\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e, out_channel=\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e\n2. BN then ReLU\n3. FC: in_channel=\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e, out_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\n4. BN then ReLU \n5. FC: in_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e, out_channel=30 \n\n**Output:** \n- (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,30)\n\n**note**\n- the 30 is:\n\t- (x,y,z,dx,dy,dz), + 2* (12 angle bins ) \n\t\t- $\\times 2$ because angle bin and confidence\n\n## Box classification \n- MLP = \\[\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e,\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\\]\n1. FC: in_channel=\u003cspan style=\"color: red\"\u003e512\u003c/span\u003e, out_channel=\u003cspan style=\"color: RoyalBlue\"\u003e256\u003c/span\u003e\n2. BN then ReLU\n3. FC: in_channel=\u003cspan style=\"color: red\"\u003e256\u003c/span\u003e, out_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e\n4. BN then ReLU \n5. FC: in_channel=\u003cspan style=\"color: LightBlue\"\u003e256\u003c/span\u003e, out_channel=3 \u003c- number of classes \n\n**Output**\n- (B*\u003cspan style=\"color: green\"\u003e128\u003c/span\u003e,3)\n\n\n## Target assignment\n- After the two sets of MLP above, we need to assign predicted boxes to gt boxes. this is done via `assign_targets(self, input_dict):`\n\n1. Take GT boxes and enlarge them by `GT_EXTRA_WIDTH: [0.2, 0.2, 0.2]`\n2. call   `assign_stack_targets()`  with params:\n\t- `points = centers`, centers are predicted centers with shape (B\\*128,4)\n\t- `gt_boxes = gt_boxes`,   GT boxes with shape (B,G,8) \u003c- G = number of boxes per scene, 8: xy,z,dx,dy,dz,angle,class\n\t- `extend_gt_boxes=extend_gt_boxes`: enlarged GT boxes with shape (B,G,8)\n\t- `set_ignore_flag = True`: not sure what this is used atm...\n\t- `use_ball_constraint = False`: not sure \n\t- `ret_part_labels = False`:  not sure\n\t- `ret_box_labels = True `:  but not sure why\n\t1. For each scene in the batch, run `roiaware_pool3d_utils.points_in_boxes_gpu()` to find the predicted centroids that lie inside a gt box. \n\t\t- return shape is (128): values are either idx of the gt box it lies in, or -1 if its background \n\t\t- create flag: `box_fg_flag = (box_idxs_of_pts \u003e= 0)`\n\t2. if `set_ignore_flag = True`, then do the same for the extended gt boxes. \n\t\t- `fg_flag = box_fg_flag\n\t\t- `ignore_flag = fg_flag ^ (extend_box_idxs_of_pts \u003e= 0)`\n\t\t\t- note that `^` is [xor gate](https://www.maximintegrated.com/content/dam/images/glossary/xor-gate-symbol.jpg) in other flag, we only want to ignore flag points that are **ONLY** in the extended gt box, or **ONLY** in the gt box \n\t\t- then `point_cls_labels_single[ignore_flag] = -1`, \n\t\t- `gt_box_of_fg_points = gt_boxes[k][box_idxs_of_pts[fg_flag]]` \n\t\t\t- `box_idxs_of_pts[fg_flag]`: is a 1D tensor with indices of the gt box each pt lies in. e.g: \\[0,4,2,4,0,1,...\\]\n\t\t\t- then `gt_box_of_fg_points` is a 2D tensor of shape (M,8) where M is the number of pts that are inside a gt box, with the associated gt box. \n\t\t- `point_cls_labels_single[fg_flag] = 1 if self.num_class == 1 else gt_box_of_fg_points[:, -1].long()`\n\t\t\t- \\[-1\\] is the class of the gt, so this gets the label and puts it into a 1D tensor.\n\t3. if `ret_box_labels and gt_box_of_fg_points.shape[0] \u003e 0:`, i.e. if there are points that lie inside gt boxes,\n\t\t- call `fg_point_box_labels =  self.box_coder.encode_torch()` with params:\n\t\t\t-  `gt_boxes=gt_box_of_fg_points[:, :-1]`: so just all boxes, with x,y,z,dx,dy,dz,angle\n\t\t\t- `points=points_single[fg_flag]`, all predicted centers that lie inside gt box\n\t\t\t- `gt_classes=gt_box_of_fg_points[:, -1].long()` class of gt boxes. \n\t\t\t\t- but not used if `self.use_mean = False` , this is set via `'use_mean_size': False` under  `BOX_CODER_CONFIG`\n\t\t- function basically assigned a label based on residuals for each pt based on the gt box it lies in.\n\t\t\t- i.e. the label for each point is different between point (x,y,z) and box center +\n\t\t\t- log() of dx dy dz of gt box\n\t\t\t- bin of angle + residual\n\t\t\t- total = length of 8 \n\t4. `point_box_labels_single[fg_flag] = fg_point_box_labels` \n\t\t-  assign the points their new labels \n\t5. `point_box_labels[bs_mask] = point_box_labels_single`\n\t\t- assign it to the \"outer\" list (where all labels for each sample in the batch will be) \n\t6. after doing this for all samples in batch:  concat all `gt_box_of_fg_points` into a tensor `gt_boxes_of_fg_points` \n\t7.  Return the following dict. \n```\ntargets_dict = {\n'point_cls_labels': point_cls_labels, # (B*128)\n'point_box_labels': point_box_labels, # (B*128,8)\n'point_part_labels': point_part_labels, # None\n'box_idxs_of_pts': box_idxs_of_pts, # (128)\n'gt_box_of_fg_points': gt_boxes_of_fg_points,#(M,8), M is the number of pts that \n\t\t\t\t\t     # are inside a gt box, so this is list\n\t\t\t\t\t\t # since it changes for every batch\n}\n```\n\n \n\n\t\t\n\t\t\t ","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/PCT":{"title":"Point Transformer","content":"\n[link](https://arxiv.org/pdf/2012.09164.pdf)","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/PVRCNN":{"title":"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection","content":"\n\n\n\n\n# Main Ideas\n- Two stage detector that uses the point **AND** voxel features (via sparse convolution) \n- Two novel operations: \n\t- **Voxel-to-keypoint scene encoding:** \n\t\t- which summarizes all the voxels of the overall scene feature volumes into a small number of feature keypoints\n\t- **point-to-grid RoI feature abstraction:** \n\t\t- effectively aggregates the scene keypoint features to RoI grids for proposal confidence prediction and location refinement\n\n# Network Diagram\n![[notes/images/pvrcnn.png]]\n# Feature Encoding \n- Input points **P** are split into voxels $L\\times W \\times H$  in $(x,y,z)$ respectively. \n- A $3\\times 3 \\times 3$  (3D) kernel is used in the sparse convulation.\n\t- result: **P** into feature volumes with $1\\times,2\\times,4\\times,8\\times$ downsampled sizes. \n\t\t- $2\\times$ means the voxel size (or $L/W/H$?) is $2\\times$ larger, so there are few voxels.   \n- One we have the the $8\\times$ downsampled feature volumes $\\left(\\frac{L}{8}\\times \\frac{W}{8}\\times \\frac{H}{8}\\right)$ \n\t- we stack the voxels along the $Z$ axis to get a $\\frac{L}{8}\\times \\frac{W}{8}$ bird-view feature maps.\n\t- \u003cspan style=\"color:red\"\u003ewhat is stacking here \u003c/span\u003e\n- Each class has $2\\times \\frac{L}{8} \\times \\frac{W}{8}$ 3D anchor boxes, and two anchors of $0^\\circ$ and $90^\\circ$ orientations. each anchor box is evaluated for each pixel of the BEV feature map.\n\n\n# Voxel Set Abstraction Module\n- from **P**, we first use [FPS](notes/papers/PointNet++#Sampling Layer ) to sample $n$ keypoints $\\mathcal{K}=\\{p_1,\\ldots,p_n\\}$ \n- Denote: $$\\mathcal{F}^{(l_k)}=\\{f_1^{(l_k)},\\ldots,f_{N_k}^{(l_k)}\\}: \\text{set of voxel-wise feature vectors at in }k\\text{th level}$$ $$\\mathcal{V}^{(l_k)}=\\{v_1^{(l_k)},\\ldots,v_{N_k}^{(l_k)}\\}: \\text{set o}$$ \n- Where $N_k$ is the number of non-empty voxels in the $k$th level\n\nFor each keypoint $p_i$ we find neighboarding non-empty voxels at the $k$th level within radius $r_k$. The resulting set of voxel-wise features vectors is:$$S_{i}^{(l_k)}=\n      \\left \\{\n        [f_j^{(lk)};\\underbrace{v{j}^{(l_k)}-\n        p_i}_{\\text{relative coords}}]^T\n      \\; \\middle | \\;\n      \\begin{array}{cc}\n        \\lVert v_{j}^{(l_k)}-p_i \\rVert^2 \u003c r_{k}, \\\\[1ex]\n        \\forall v_{j}^{(l_k)}\\in \\mathcal{V}^{(l_k)}, \\\\[1ex]\n        \\forall f{j}^{(l_k)}\\in\\mathcal{F}^{(l_k)}\n      \\end{array}\n      \\right\\}$$\n  where:\n  - $v_{j}^{(l_k)}-p_i$ : relative coordinates/location of $f_j^{(l_k)}$\n  - $f_j^{(l_k)}$: semantic voxel feature\n\n- The voxel-wise features within neighboring voxel set $S_{i}^{(l_k)}$ is then passed into a PointNet block:$$f_{i}^{(pv_k)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(S_{i}^{(l_k)}\\right)\\right)\\right\\}\\tag{2}$$\n- where:\n- $M(\\cdot)$: randomly sampling at most $T_k$ voxels from $S_{i}^{(l_k)}$. (for saving computation)\n- $G(\\cdot)$: MLP \n- $\\max(\\cdot)$: max-pooling along channel dimension.\n\t- The result of the block are the features of key point $p_i$ \n\n\nThen for each keypoint $p_i$ , we concatinate its features from different levels\n$$f_i^{(pv)}=\\left[f_{i}^{(pv_1)},f_{i}^{(pv_2)},f_{i}^{(pv_3)},f_{i}^{(pv_4)}\\right], \\text{for } i=1,\\ldots,n$$\nwhere:\n- $f_i^{(pv)}$: is 3D voxel CNN-based feature + pointnet features as explained above\n\n\n# Extended VSA Module\n- With the 8x downsampled 2D BEV feature map, and original point cloud **P**, put **P** through eq 2. We call these features $f_i^{(raw)}$ \n- Also project each $p_i$ onto the BEV map, and use bilinear interpolation to get $f_i^{(bev)}$ from BEV feature map\n\t- \u003cspan style=\"color:red\"\u003eHow exactly?\u003c/span\u003e\n - Then finally concatinate all these features together:$$f_i^{(p)}=\\left[f_{i}^{(pv)},f_i^{(raw)},f_{i}^{(bev)}\\right], \\text{ for }i=1,\\ldots,n$$\n\n\n# Predicted Keypoint Weighting\n![[notes/images/PKW.png]]\n- Because [FPS](notes/papers/PointNet++#Sampling Layer) might chose background points, we want to put more weight into the foreground points + their features. \n- Take the keypoint features, and pass it through a three layer mlp: $\\mathcal{A}(f_i^{(p)})$  with sigmoid as the final layer as illustrated above. \n\t- In training this is supervised via checking if the keypoint is in a GT box or not. i.e.: $p_i$ has label $1$ if $p_i$ in GT, else $0$. Focal loss is used here. \n- weights are then multiplied by the features: $$\\tilde{f_i}^{(p)}=\\mathcal{A}(f_i^{(p)})\\cdot f_{i}^{(p)}$$\n\n# RoI-grid Pooling via Set Abstraction\n- After all the previous steps, we have the set of keypoint features $$\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f_{i}}^{(p)},\\ldots,\\tilde{f_{n}}^{(p)}\\right\\}$$ \n- For each 3D proposal **(RoI)** we need to aggregate the keypoint features\n- We uniformly sample $6\\times6\\times6$ **grid points** within each 3D proposal **(RoI)** denoted as $\\mathcal{G}=\\{g_1,\\ldots,g_{216}\\}$\n\t-  (Grid points need not be points from raw pointcloud **P**) \n\t![[notes/images/roi_pooling.png]]\n- For each *grid point*, we gather all *key point* features within radius $\\tilde{r}$ $$\\tilde{\\Psi}=\\left\\{\\left[\\tilde{f}_{j}^{(p)};p_j-g_i\\right]^T  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert p_{j}-g_i\\rVert^2 \u003c \\tilde{r}, \\\\[1ex]\n    \\forall p_{j}\\in \\mathcal{K}, \\\\[1ex]\n    \\forall \\tilde{f}_{j}^{(p)}\\in\\tilde{\\mathcal{F}}\n  \\end{array}\\right\\}$$\n- $p_j-g_i$: local coordinates of features $\\tilde{f_j}^{(p)}$ relative to grid point $g_i$ \n- pointnet is used again to aggregate features: $$\\tilde{f}_i^{(g)}=\\max\\left\\{G\\left(\\mathcal\\{M\\}\\left(\\tilde\\{\\Psi\\}\\right)\\right)\\right\\}$$\n- Two seperate MLP heads (256 dims) are then used for box refinement and confidence. \n\t- **Box Refinement:** for each of the RoI, it predicts the residuals compared to GT. $$L_{iou}=-y_k\\log(\\tilde{y}_k)-(1-y_k)\\log(1-\\tilde{y}_k)$$\n\t\t- where $\\tilde{y}_k$ is predicted score by network \n\t- **Confidence:** For the $k$th RoI, its confidence for a target $y_k$ is normalized to be between $[0,1]$ $$y_k=\\min(1,\\max(0,2\\text{IoU}_k-0.5))$$\n\t\t- Where $\\text{IoU}_k$ is the IoU of the $k$th RoI w.r.t to tis GT box.\n\t- Both are optimized via smooth-L1  \n\n\n# Training Loss\n- 3 Losses are used:\n\t- Region proposal loss: $L_{rpn}$ $$L_{rpn}=\\underbrace{L_{cls}}_{\\text{focal loss}}+\\beta \\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^a}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^a)$$\n\t\t- predicted residual: $\\widehat{\\Delta r^a}$ \n\t\t- regression target: $\\Delta r^a$\n\t- Keypoint segmentation loss: $L_{seg}$ \n\t\t- loss calculation is explained in the [Predicted Keypoint Weighting](notes/papers/PVRCNN#Predicted Keypoint Weighting)\n\t- Proposal refinement loss: $L_{rcnn}$ $$L_{rcnn}=L_{iou}+\\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^p}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^p)$$\n\t\t- predicted box residual: $\\widehat{\\Delta r^p}$\n\t\t- proposal regression target: $\\Delta r^p$\n- Overall loss is sum of these three with equal loss weights. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/PointNet++":{"title":"PointNet++","content":"\n\n## Set Abstraction (SA) Layer\n![[notes/images/pointnet2.png]]\n\n### Sampling Layer \nLets call the sampling layer $SL$.\n- Input: $N\\times 3$ \n\t- N points with $x,y,z$\n- Output: $M\\times 3$ \n\t- $M$ points with $x,y,z$ where points in $M$ are subset of $N$\n- Method: **Iterative Farthest Point Sampling** (FPS)\n\t1. Start by choosing 1 random point\n\t2. Calculate distance for all remaining points to selected points.\n\t\t1. so each point will have an array keeping track of the distances \n\t\t3. take the minimum of the arrray, i.e. for each remaining point, set its distance to the closest selected point.\n\t\t4. Finally, select the point with the greatest distance\n\t3. Repeat step 2 $M-1$ more times\n\t- **1D Example:** Consider an array of points $P=[1,6,7,8,20]$, \n\t\t1. suppose the list of selected points $S=[7,20]$ , $P$ is now $[1,6,8]$\n\t\t2. we calculate distances:\n\t\t   $$\\begin{align*}dist_1=[6,19]\\\\\n\t\t   dist_6=[1,14]\\\\\n\t\t   dist_8=[1,12]\\\\\n\t\t   \\end{align*}$$\n\t\t3. perform $min (dist_i)$$$\\begin{align*}dist_1=6\\\\\n\t\t   dist_6=1\\\\\n\t\t   dist_8=1\\\\\n\t\t   \\end{align*}$$\n\t\t4. now select the point with highest distance. i.e. $$\\underset{i}{\\operatorname{argmax}}(dist_i)=1$$\n\t\t5. So the next chosen point is 1. $S=[1,7,20]$ and $P=[6,8]$\n\n\n\n### Grouping Layer\n- Sampling Layer $SL$ yields $N'\\times 3$ points.\n\t- $N'$ centroids and $3:= x,y,z$ coordinates \n\t- $SL:(N\\times 3) \\to (N'\\times 3)$\n- Input: \n\t- $(N\\times 3+ C)$: points before sampling and their feature vectors of len $C$\n\t-  $(N'\\times d)$: predicted centroids \n- Output: \n\t- $(N'\\times K \\times (3+C))$: centroid + $K$ points within radius of $r$ along their features\n- Method: \n\t- Ball query\n\t\t\t- what if points within radius is $\u003cK$? \n\t\t\t- what if no points (only centroid point)?\n\t\t\t- with if more than $K$ points?  \n\n#### Multi-scale grouping (MSG)\n- Due to nonuniformity of point clouds, we needs some more tricks to make feature learning more robust. \n- One simple way is to applying grouping + pointnet multiple times then concat their features \n- **Dilated Group:** implementation setting where you set a min radius.\n\t- eg: $r=[0.2,0.4,0.8]$, then \n\t- for $r=0.2$, sample within radius $[0,0.2]$\n\t- for $r=0.4$, sample within radius $[0.2,0.4]$\n\t- for $r=0.8$, sample within radius $[0.4,0.8]$\n![[notes/images/msg.png|300]]\n\n\n\n#### Multi-resolution grouping (MRG)\n- MSG works, but its computationally expensive, so this is an alternative method \n- Consider a layer for input of points, we first apply set abstraction layer (sampling + grouping + pointnet), to yield a vector of features $L_1$. We then  apply pointnet of the raw pointcloud (before set abstraction) to yield feature vector $L_2$. Then we concat the results $(L_1,L_2)$ \n\n![[notes/images/mrg.png|300]]\n### PointNet Layer\nessentially just a FC layer on each \"ball\" of points\n- Input $(N'\\times K \\times (3+C))$.\n\t- $N'$ local regions (balls of points)\n\t- $K$ is num of balls in the ball\n\t- $3+C = x,y,z+C \\text{ features}$\n- Output: $(N'\\times (3+C'))$ \n- Method: \n\t- for each point $x_j$ in a ball, transform it into a local coordinate frame relative to its centroid $\\hat{x}$ $$x^{new}_j = x_j-\\hat{x}$$\n\t- Then we apply series of FC layers\n\n# Feature Propogation for Set Segmentation \n\n\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/TANet":{"title":"TANet: Robust 3D Object Detection from Point Clouds with Triple Attention","content":"\n[link](https://arxiv.org/pdf/1912.05163.pdf)\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/attentionalpointnet":{"title":"Attentional PointNet for 3D-Object Detection in Point Clouds","content":"\n[link](https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf)\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/iassd":{"title":"Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds","content":"\n# Main Ideas \n- Turns out [F-FPS](notes/papers/3DSSD) from 3DSSD is still not good enough in preserving foreground points. \n\t- Introduced Class-aware Sampling\n\n\n\n# Class-Aware Sampling\n- Superior sampling method compared to [F-FPS](notes/papers/3DSSD)  and [D-FPS](notes/papers/PointNet++)\n-  Using vanilla cross-entropy loss $$L_{cls-aware}=-\\sum^{C}_{c=1}(s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n- $C:$ Number of catergories \n- $s_i:$ One hot labels \n- $\\hat{s_i}:$ predicted logits  \n\n\n# Centroid Aware Sampling\n- Give higher weight to points near instance centroid $$Mask_i=\\sqrt[3]{\\frac{\\min(f^*,b^*)}{\\max(f^*,b^*)}\\times \\frac{\\min(l^*,r^*)}{\\max(l^*,r^*)}\\times\\frac{\\min(u^*,d^*)}{\\max(u^*,d^*)}}$$\n- $f^∗, b^∗, l^∗, r^∗, u^∗, d^∗$ represent the distance of a point to the 6 surfaces (front, back, left, right, up and down)\n- This mask is used during **training** via the ctr-aware loss $L_{ctr-aware}$. At inference we simply keep top $k$ points with highest scores. $$L_{ctr-aware}=-\\sum^{C}_{c=1}(Mask_i\\cdot s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n\n# Contextual Centroid Prediction \n\n- bruh $$L_{cent}=\\frac{1}{|\\mathcal{F}_+|}\\frac{1}{|\\mathcal{S}_+|}\\sum_i\\sum_j (|\\Delta\\hat{c_{ij}}-\\Delta c_{ij}|+|\\hat{c_{ij}}-\\overline{c_i}|)\\cdot \\mathbb{1}_S(p_{ij})$$\n- where $$\\overline{c_i}=\\frac{1}{|S_+|}\\sum_j\\hat{c_{ij}}\\quad,\\quad\\mathbb{1}_{S}(p_{ij}):\\mathcal{P}\\to\\{0,1\\}$$\n- $|\\mathcal{F}_+|$ =  the total number of GT boxes used to predict centroids\n- $|\\mathcal{S}_+|$ = number of points used to predict the instance center\n- $\\Delta\\hat{c_{ij}}$ = offset predicted to instance center\n- $\\Delta{c_{ij}}$ = GT offset from a point $p_{ij}$ to center point \n- $\\mathbb{1}_S$ = indicator fn to determine whether point is used \n\t- implementation: expand GT box by some amount, then include all points inside the expanded box.\n\nExamine the $|\\hat{c_{ij}}-\\overline{c_i}|$ term a bit more:\n- Each GT box has one of these.\n- For each relevant point (i.e. points inside the expanded GT box)\n\t- take their average, that becomes $\\overline{c_i}$ \n- then while looping over each predicted centroid, the term is calculated. \n- overall effect seems to promote all assignment points inside a GT box to predict the same centroid location\n\n","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/pointformer":{"title":"Pointformer","content":"\n[link](https://arxiv.org/pdf/2012.11409.pdf)","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/notes/papers/unionofmanifold":{"title":"The Union of Manifolds Hypothesis and its Implications for Deep Generative Modelling","content":"\n\n# Main Ideas\n\n- The manifold hypothesis states that high-dimensional data of interest often lives in an unknown lower-dimensional manifold embedded in ambient space\n ","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null},"/tags/setup":{"title":"","content":"","lastmodified":"2022-09-10T03:14:34.215618523Z","tags":null}}