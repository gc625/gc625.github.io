{"/":{"title":"Gabriel Chan's website","content":"\nHi, this is my website where I keep my notes for research, classes and personal projects. checkout my [github](https://github.com/gc625) too!\n\n\n\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD\n# Research Notes\n## Point Cloud Networks\n- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](notes/papers/PointNet++)\n### Single Stage Detectors \n- [3DSSD: Point-based 3D Single Stage Object Detector](notes/papers/3DSSD)\n- [(IASSD)Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds](notes/papers/iassd)\n\n### Two Stage Detectors\n- [RVRCNN](notes/papers/PVRCNN)\n  \n### Attention on point clouds\n- [3DETR: An End-to-End Transformer Model for 3D Object Detection](notes/papers/3DETR)\n- [Attentional PointNet for 3D-Object Detection in Point Clouds](notes/papers/attentionalpointnet)\n- [Point Transformer](notes/papers/PCT)\n- [TANet: Robust 3D Object Detection from Point Clouds with Triple Attention](notes/papers/TANet)\n- [3D Object Detection with Pointformer](notes/papers/pointformer)\n\n\n# Basic of Machine Learning\n- [Surpervised Learning](notes/back2basics/supervisedlearning)\n- [Unsupervised Learning](notes/back2basics/unsupervisedlearning)\n\n\n# Machine Learning frameworks\n- [OpenPCDet](notes/openPCDet)\n=======\n## Get Started\n\u003e üìö Step 1: [Setup your own digital garden using Quartz](notes/setup.md)\n\u003e\u003e\u003e\u003e\u003e\u003e\u003e upstream/hugo\n\n\n\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD\n\n\n=======\nReturning user? Figure out how to [[notes/updating|update]] your existing Quartz garden.\n\n### Content Lists\nIf you prefer browsing the contents of this site through a list instead of a graph, you see a list of all [setup-related notes](/tags/setup).\n\n### Troubleshooting\n- üöß [Troubleshooting and FAQ](notes/troubleshooting.md)\n- üêõ [Submit an Issue](https://github.com/jackyzha0/quartz/issues)\n- üëÄ [Discord Community](https://discord.gg/cRFFHYye7t)\n\u003e\u003e\u003e\u003e\u003e\u003e\u003e upstream/hugo\n","lastmodified":"2022-07-03T05:01:05.713255991Z","tags":null},"/notes/HonorsSeminar":{"title":"Research Seminar Ideas","content":"\n\n# All Point Cloud Transformer papers\n\n- [3DETR](notes/papers/3DETR)\n- [Pointformer](notes/papers/pointformer)\n- [Point transformer](notes/papers/pointformer)\n- [Attentional Pointnet](notes/papers/attentionalpointnet)\n- [Triple Attention Net](notes/papers/TANet)\n\n\n\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/back2basics/supervisedlearning":{"title":"Supervised Learning","content":"\n\n# What is supervised learning?\n\n- Training an algorithm to output $y$ for a given $x$ using sufficient training samples $\\{(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\\ldots,(x^{(n)},y^{(n)})\\}$ for some input $x^{(i)}$ and **correct** output $y^{(i)}$ \n- **Regression:** predicting an a number (infinitely many outputs)\n- **Classification:** predicting categories (finite outputs)\n\n\n# Linear Regression \n- Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\\hat{y}$ given an input $x$ \n- for linear regression, $f$ is a straight line. With parameters $w,b$ we can then represent $f$ as: $$f_{w,b}(x)=wx+b$$\n## Cost Function\n- Since our objective to find $w,b$ such taht $\\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$. \n- Squared error cost function:  $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$\n- where $m=$ number of training examples. So $\\frac{1}{m}$ is to average it so it doesnt blow up, factor of $2$ is for computational convience later. \n- Now we can also rewrite it as: $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}f_{w,b}(x^{(i)})-y^{(i)})^2$$\n- This can be solved analytically for simple cost functions, but for complicated $J$, we can use gradiant descent to minimize $J$ instead: \n\n## Gradient Descent\n- initialize $w,b$, calcuated $J$ \n- adjust $w,b$ to decrease $J$ \n- repeat until hopefully $J$ settles near minimum \n\n- Step 1: ($=$ here is assignment, not equals)\n$$w=w -\\alpha \\frac{d}{dw}J(w,b)$$ where $\\alpha$ is the learning rate, a **hyperparameter** that controls the \"fast\" we change $w$ \n- Step 2: do the same for $b$ $$b=b -\\alpha \\frac{d}{db}J(w,b)$$\n- **Note:** $w$ and $b$ must be updated at the same time. \n### Learning rate\n- If $\\alpha$ is too small, then it will take many steps to reach minimum \n- If $\\alpha$ is too large, then it might never reach the minimum \n\n# Gradient Descent for linear regression\nCalculating derivatives   for $w$, $$\\frac{d}{dw}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$=\\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)}-y^{(i)})x^{(i)}$$\n\nand derivative for $b$, \n$$\\frac{d}{db}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$ =\\frac{d}{db}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})$$\n\nPsuedocode for gradient descent:\n```python\nwhile J not converged:\n\tw = w - a * dJdW\n\tb = b- a * dJdb\n```\nwhere `dJdW` = $\\frac{d}{dw}J(w,b)$ and `dJdb` = $\\frac{d}{db}J(w,b)$\n\n\n\n# Multiple features\nwhat if you have multiple features (variables)? \n\n- $x_j = j^{th}$ feature\n- $n$ = number of features\n- $\\vec{x}^{(i)}$= features of $i^{th}$ training example\n- $x_{j}^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n\nWe can then express the linear regression model as:\n\n$$f_{w,b}(x)=w_1x_1+w_2x_2+\\cdots+w_nx_n+b$$\ndefine $\\vec{w} = [w_1,\\ldots,w_n]$ and $\\vec{x}=[x_1,\\ldots x_n]^T$, $T$ here represents transpose. Then \n$$f_{\\vec{w},b}=\\vec{w}\\cdot \\vec{x}+b$$Where $(\\cdot)$ represents the dot product. \n\n\n\n# Feature Scaling\nWhen the range of values your features can take up differ greatly, i.e. \n- $x_1$ = square footage of house $\\in [500,5000]$ \n- $x_2$ = number of bedrooms $\\in [1,5]$\n\nthis may cause gradient descent to run slowly. ![[notes/images/feature_scaling.png]]\n\nSome examples of feature scaling\n## max scaling\n- divide each data point for a feature by the max value for that feature.\n\n## mean normalization\n- eg: if $300 \\leq x_1 \\leq 2000$ , we can scale it like such $$x_{1new} = \\frac{x_1-\\mu_1}{2000-300}$$\n- where $\\mu_1$ = mean\n\n\n## Z-score normalization \n- find standard deviation $\\sigma$ , mean $\\mu$ then $$x_1=\\frac{x_1-\\mu_1}{\\sigma_1}$$\n\n\n\n\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/back2basics/unsupervisedlearning":{"title":"Unsupervised Learning","content":"\n\n# What is Unsupervised Learning\n\n- no labelled data, algorithm finds something interesting in unlabeled data.\n\t- given only inputs $\\{x_0,\\ldots,x_n\\}$, but no output labels $\\{y_0,\\ldots,y_n\\}$\n- **Clustering:**  for example, groups data points together\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/config":{"title":"Configuration","content":"\n## Configuration\nQuartz is designed to be extremely configurable. You can find the bulk of the configuration scattered throughout the repository depending on how in-depth you'd like to get.\n\nThe majority of configuration can be found under `data/config.yaml`. An annotated example configuration is shown below.\n\n```yaml {title=\"data/config.yaml\"}\n# The name to display in the footer\nname: Jacky Zhao\n\n# whether to globally show the table of contents on each page\n# this can be turned off on a per-page basis by adding this to the\n# front-matter of that note\nenableToc: true\n\n# whether to by-default open or close the table of contents on each page\nopenToc: false\n\n# whether to display on-hover link preview cards\nenableLinkPreview: true\n\n# whether to render titles for code blocks\nenableCodeBlockTitle: true \n\n# whether to try to process Latex\nenableLatex: true\n\n# whether to enable single-page-app style rendering\n# this prevents flahses of unstyled content and overall improves\n# smoothness of quartz. More info in issue #109 on GitHub\nenableSPA: true\n\n# whether to render a footer\nenableFooter: true\n\n# whether backlinks of pages should show the context in which\n# they were mentioned\nenableContextualBacklinks: true\n\n# whether to show a section of recent notes on the home page\nenableRecentNotes: false\n\n# page description used for SEO\ndescription:\n  Host your second brain and digital garden for free. Quartz features extremely fast full-text search,\n  Wikilink support, backlinks, local graph, tags, and link previews.\n\n# title of the home page (also for SEO)\npage_title:\n  \"ü™¥ Quartz 3.2\"\n\n# links to show in the footer\nlinks:\n  - link_name: Twitter\n    link: https://twitter.com/_jzhao\n  - link_name: Github\n    link: https://github.com/jackyzha0\n```\n\n### Code Block Titles\n\nTo add code block titles with Quartz:\n\n1. Ensure that code block titles are enabled in Quartz's configuration:\n\n    ```yaml {title=\"data/config.yaml\", linenos=false}\n    enableCodeBlockTitle: true\n    ```\n\n2. Add the `title` attribute to the desired [code block\n   fence](https://gohugo.io/content-management/syntax-highlighting/#highlighting-in-code-fences):\n\n      ```markdown {linenos=false}\n       ```yaml {title=\"data/config.yaml\"}\n       enableCodeBlockTitle: true  # example from step 1\n       ```\n      ```\n\n**Note** that if `{title=\u003cmy-title\u003e}` is included, and code block titles are not\nenabled, no errors will occur and the title attribute will be ignored.\n\n### HTML Favicons\nIf you would like to customize the favicons of your quartz-based website, you \ncan add them to the `data/config.yaml` file. The **default** without any set \n`favicon` key is:\n\n```html {title=\"layouts/partials/head.html\", linenostart=15}\n\u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n```\n\nThe default can be overridden by defining a value to the `favicon` key in your \n`data/config.yaml` file. Here is a `List[Dictionary]` example format, which is\nequivalent to the default:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon:\n  - { rel: \"shortcut icon\", href: \"icon.png\", type: \"image/png\" }\n#  - { ... } # Repeat for each additional favicon you want to add\n```\n\nIn this format, the keys are identical to their HTML representations.\n\nIf you plan to add multiple favicons generated by a website (see list below), it\nmay be easier to define it as HTML. Here is an example which appends the \n**Apple touch icon** to quartz's default favicon:\n\n```yaml {title=\"data/config.yaml\", linenos=false}\nfavicon: |\n  \u003clink rel=\"shortcut icon\" href=\"icon.png\" type=\"image/png\"\u003e\n  \u003clink rel=\"apple-touch-icon\" sizes=\"180x180\" href=\"/apple-touch-icon.png\"\u003e\n```\n\nThis second favicon will now be used as a web page icon when someone adds your \nwebpage to the home screen of their Apple device. If you are interested in more \ninformation about the current, and past, standards of favicons, you can read \n[this article](https://www.emergeinteractive.com/insights/detail/the-essentials-of-favicons/).\n\n**Note** that all generated favicon paths, defined by the `href` \nattribute, are relative to the `static/` directory.\n\n### Graph View\nTo customize the Interactive Graph view, you can poke around `data/graphConfig.yaml`.\n\n\n```yaml {title=\"data/graphConfig.yaml\"}\n# if true, a Global Graph will be shown on home page with full width, no backlink.\n# A different set of Local Graphs will be shown on sub pages.\n# if false, Local Graph will be default on every page as usual\nenableGlobalGraph: false\n\n### Local Graph ###\nlocalGraph:\n\t# whether automatically generate a legend\n    enableLegend: false\n    \n    # whether to allow dragging nodes in the graph\n    enableDrag: true\n    \n    # whether to allow zooming and panning the graph\n    enableZoom: true\n    \n    # how many neighbours of the current node to show (-1 is all nodes)\n    depth: 1\n    \n    # initial zoom factor of the graph\n    scale: 1.2\n    \n    # how strongly nodes should repel each other\n    repelForce: 2\n\n    # how strongly should nodes be attracted to the center of gravity\n    centerForce: 1\n\n    # what the default link length should be\n    linkDistance: 1\n    \n    # how big the node labels should be\n    fontSize: 0.6\n    \n    # scale at which to start fading the labes on nodes\n    opacityScale: 3\n\n### Global Graph ###\nglobalGraph:\n\t# same settings as above\n\n### For all graphs ###\n# colour specific nodes path off of their path\npaths:\n  - /moc: \"#4388cc\"\n```\n\n\n## Styling\nWant to go even more in-depth? You can add custom CSS styling and change existing colours through editing `assets/styles/custom.scss`. If you'd like to target specific parts of the site, you can add ids and classes to the HTML partials in `/layouts/partials`. \n\n### Partials\nPartials are what dictate what actually gets rendered to the page. Want to change how pages are styled and structured? You can edit the appropriate layout in `/layouts`.\n\nFor example, the structure of the home page can be edited through `/layouts/index.html`. To customize the footer, you can edit `/layouts/partials/footer.html`\n\nMore info about partials on [Hugo's website.](https://gohugo.io/templates/partials/)\n\nStill having problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n\n## Language Support\n[CJK + Latex Support (ÊµãËØï)](notes/CJK%20+%20Latex%20Support%20(ÊµãËØï).md) comes out of the box with Quartz.\n\nWant to support languages that read from right-to-left (like Arabic)? Hugo (and by proxy, Quartz) supports this natively.\n\nFollow the steps [Hugo provides here](https://gohugo.io/content-management/multilingual/#configure-languages) and modify your `config.toml`\n\nFor example:\n\n```toml\ndefaultContentLanguage = 'ar'\n[languages]\n  [languages.ar]\n    languagedirection = 'rtl'\n    title = 'ŸÖÿØŸàŸÜÿ™Ÿä'\n    weight = 1\n```\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/editing":{"title":"Editing Content in Quartz","content":"\n## Editing \nQuartz runs on top of [Hugo](https://gohugo.io/) so all notes are written in [Markdown](https://www.markdownguide.org/getting-started/).\n\n### Folder Structure\nHere's a rough overview of what's what.\n\n**All content in your garden can found in the `/content` folder.** To make edits, you can open any of the files and make changes directly and save it. You can organize content into any folder you'd like.\n\n**To edit the main home page, open `/content/_index.md`.**\n\nTo create a link between notes in your garden, just create a normal link using Markdown pointing to the document in question. Please note that **all links should be relative to the root `/content` path**. \n\n```markdown\nFor example, I want to link this current document to `notes/config.md`.\n[A link to the config page](notes/config.md)\n```\n\nSimilarly, you can put local images anywhere in the `/content` folder.\n\n```markdown\nExample image (source is in content/notes/images/example.png)\n![Example Image](/content/notes/images/example.png)\n```\n\nYou can also use wikilinks if that is what you are more comfortable with!\n\n### Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so. You can also add tags here as well.\n\n```yaml\n---\ntitle: \"Example Title\"\ntags:\n- example-tag\n---\n\nRest of your content here...\n```\n\n### Obsidian\nI recommend using [Obsidian](http://obsidian.md/) as a way to edit and grow your digital garden. It comes with a really nice editor and graphical interface to preview all of your local files.\n\nThis step is **highly recommended**.\n\n\u003e üîó Step 3: [How to setup your Obsidian Vault to work with Quartz](notes/obsidian.md)\n\n## Previewing Changes\nThis step is purely optional and mostly for those who want to see the published version of their digital garden locally before opening it up to the internet. This is *highly recommended* but not required.\n\n\u003e üëÄ Step 4: [Preview Quartz Changes](notes/preview%20changes.md)\n\nFor those who like to live life more on the edge, viewing the garden through Obsidian gets you pretty close to the real thing.\n\n## Publishing Changes\nNow that you know the basics of managing your digital garden using Quartz, you can publish it to the internet!\n\n\u003e üåç Step 5: [Hosting Quartz online!](notes/hosting.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/hosting":{"title":"Deploying Quartz to the Web","content":"\n## Hosting on GitHub Pages\nQuartz is designed to be effortless to deploy. If you forked and cloned Quartz directly from the repository, everything should already be good to go! Follow the steps below.\n\n### Enable GitHub Actions\nBy default, GitHub disables workflows from running automatically on Forked Repostories. Head to the 'Actions' tab of your forked repository and Enable Workflows to setup deploying your Quartz site!\n\n![Enable GitHub Actions](notes/images/github-actions.png)*Enable GitHub Actions*\n\n### Enable GitHub Pages\n\nHead to the 'Settings' tab of your forked repository and go to the 'Pages' tab.\n\n1. (IMPORTANT) Set the source to deploy from `master` (and not `hugo`) using `/ (root)`\n2. Set a custom domain here if you have one!\n\n![Enable GitHub Pages](/notes/images/github-pages.png)*Enable GitHub Pages*\n\n### Pushing Changes\nTo see your changes on the internet, we need to push it them to GitHub. Quartz is a `git` repository so updating it is the same workflow as you would follow as if it were just a regular software project.\n\n```shell\n# Navigate to Quartz folder\ncd \u003cpath-to-quartz\u003e\n\n# Commit all changes\ngit add .\ngit commit -m \"message describing changes\"\n\n# Push to GitHub to update site\ngit push origin hugo\n```\n\nNote: we specifically push to the `hugo` branch here. Our GitHub action automatically runs everytime a push to is detected to that branch and then updates the `master` branch for redeployment.\n\n### Setting up the Site\nNow let's get this site up and running. Never hosted a site before? No problem. Have a fancy custom domain you already own or want to subdomain your Quartz? That's easy too.\n\nHere, we take advantage of GitHub's free page hosting to deploy our site. Change `baseURL` in `/config.toml`. \n\nMake sure that your `baseURL` has a trailing `/`!\n\n[Reference `config.toml` here](https://github.com/jackyzha0/quartz/blob/hugo/config.toml)\n\n```toml\nbaseURL = \"https://\u003cYOUR-DOMAIN\u003e/\"\n```\n\nIf you are using this under a subdomain (e.g. `\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz`), include the trailing `/`. **You need to do this especially if you are using GitHub!**\n\n```toml\nbaseURL = \"https://\u003cYOUR-GITHUB-USERNAME\u003e.github.io/quartz/\"\n```\n\nChange `cname` in `/.github/workflows/deploy.yaml`. Again, if you don't have a custom domain to use, you can use `\u003cYOUR-USERNAME\u003e.github.io`.\n\nPlease note that the `cname` field should *not* have any path `e.g. end with /quartz` or have a trailing `/`.\n\n[Reference `deploy.yaml` here](https://github.com/jackyzha0/quartz/blob/hugo/.github/workflows/deploy.yaml)\n\n```yaml {title=\".github/workflows/deploy.yaml\"}\n- name: Deploy  \n  uses: peaceiris/actions-gh-pages@v3  \n  with:  \n\tgithub_token: ${{ secrets.GITHUB_TOKEN }} # this can stay as is, GitHub fills this in for us!\n\tpublish_dir: ./public  \n\tpublish_branch: master\n\tcname: \u003cYOUR-DOMAIN\u003e\n```\n\nHave a custom domain? [Learn how to set it up with Quartz ](notes/custom%20Domain.md).\n\n### Ignoring Files\nOnly want to publish a subset of all of your notes? Don't worry, Quartz makes this a simple two-step process.\n\n‚ùå [Excluding pages from being published](notes/ignore%20notes.md)\n\n---\n\nNow that your Quartz is live, let's figure out how to make Quartz really *yours*!\n\n\u003e Step 6: üé® [Customizing Quartz](notes/config.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2022-07-03T05:01:05.757255894Z","tags":null},"/notes/interesting/Cross-Entropy":{"title":"Cross Entropy","content":"\nhttps://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/obsidian":{"title":"Obsidian Vault Integration","content":"\n## Setup\nObsidian is the preferred way to use Quartz. You can either create a new Obsidian Vault or link one that your already have.\n\n### New Vault\nIf you don't have an existing Vault, [download Obsidian](https://obsidian.md/) and create a new Vault in the `/content` folder that you created and cloned during the [setup](notes/setup.md) step.\n\n### Linking an existing Vault\nThe easiest way to use an existing Vault is to copy all of your files (directory and hierarchies intact) into the `/content` folder.\n\n## Settings\nGreat, now that you have your Obsidian linked to your Quartz, let's fix some settings so that they play well.\n\n1. Under Options \u003e Files and Links, set the New link format to always use Absolute Path in Vault.\n2. Go to Settings \u003e Files \u0026 Links \u003e Turn \"on\" automatically update internal links.\n\n![Obsidian Settings](/notes/images/obsidian-settings.png)*Obsidian Settings*\n\n## Templates\nInserting front matter everytime you want to create a new Note gets annoying really quickly. Luckily, Obsidian supports templates which makes inserting new content really easily.\n\n**If you decide to overwrite the `/content` folder completely, don't remove the `/content/templates` folder!**\n\nHead over to Options \u003e Core Plugins and enable the Templates plugin. Then go to Options \u003e Hotkeys and set a hotkey for 'Insert Template' (I recommend `[cmd]+T`). That way, when you create a new note, you can just press the hotkey for a new template and be ready to go!\n\n\u003e üëÄ Step 4: [Preview Quartz Changes](notes/preview%20changes.md)","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/openPCDet":{"title":"OpenPCDet","content":"# OpenPCDet \n\n## Code Architecture \nfile structure for a project should look something like this:\n\n### Directories\n```bash\nOpenPCDet (or proj name)\n‚îú‚îÄ‚îÄ data \n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kitti\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lyft\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ waymo\n‚îú‚îÄ‚îÄ docker\n‚îú‚îÄ‚îÄ docs\n‚îú‚îÄ‚îÄ pcdet\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ datasets\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ops\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ utils\n‚îî‚îÄ‚îÄ tools\n¬† ¬† ‚îú‚îÄ‚îÄ cfgs\n¬† ¬† ‚îú‚îÄ‚îÄ eval_utils\n¬† ¬† ‚îú‚îÄ‚îÄ scripts\n¬† ¬† ‚îú‚îÄ‚îÄ train_utils\n¬† ¬† ‚îî‚îÄ‚îÄ visual_utils\n```\n\n\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/3DETR":{"title":"An End-to-End Transformer Model for 3D Object Detection","content":"\n[link](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)\n\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/3DSSD":{"title":"3DSSD: Point-based 3D Single Stage Object Detector","content":"\n[arXiv link](https://arxiv.org/pdf/2002.10187.pdf) \ngithub #todo \n\n\n## Main Ideas:\n-  Introduced **Feature-Farthest Point Sampling (F-FPS)**\n- Introduced new sampling method called **Fusion Sampling** in [notes/papers/PointNet++#Set Abstraction SA Layer](Set Abstraction Layer)\n\n\n### Feature-Farthest Point Sampling (F-FPS)\n- Objective when downsample: \n\t1. Remove **negative points** (background points) \n\t2. Preserve only **positive points** (foreground points, i.e: points within any instance := ground truth box)\n- Therefore leverage semantic features of points as well when applying FPS \n- Given two points $A$ and $B$, the criterion used to compare them in FPS is:$$C(A,B)=\\lambda L_d(A,B)+L_f(A,B)$$\n\t\twhere \n\t\t- $L_d(A,B)$ is $L^2$ euclidean distance (xyz) \n\t\t- $L_f(A,B)$ is $L^2$ feature distance (distance between the two feature vectors)\n\t\t- $\\lambda$ is chosen parameter, paper seems to choose $\\lambda=1$\n\t\t- *reminder*: $L^2(A,B)= \\sqrt{(B_1-A_1)^2+(B_1-A_1)^2+\\cdots+(B_n-A_n)^2}$  if $A$ and $B$ are $n$ dimensional vectors\n- Result should be a subset of points that are less redundant and more diverse, as points are not only physically distant when sampling, but also in feature space.  \n\n### Fusion Sampling\n- Downsampling to $N_m$ points with **F-FPS** results in:\n\t\t- lots of positive points -\u003e good for regression \n\t\t- few negative points (due to limiting ) -\u003e bad for classification\n\t\t- **Why?** Negative points don't have enough neighbours #expand \n- Input: $N_i\\times C_i :=$ $N$ points each with feature vector of length $C$\n- want to output $N_{i+1}$ points, where $N_{i+1}$ points are subset of the $N_i$ points\n\t1. F-FPS$: N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t2. D-FPS:$N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t3. [notes/papers/PointNet++#Grouping Layer](grouping operation )\n\t4. MLP\n\t5. MaxPool\n\n![[notes/images/3dssdbackbone.png]]\n\n# Network walkthrough\nusing the following network config: ![[notes/images/3dssdcfg.png]]\n\n\n## Network Input:\n- Radar pts of dim 4: $[x,y,z,RCS]$\n- **input size** is determined by `sample_points` in `DATA_PROCESSOR`: e.g.: 512 \n- then the feature dimension is 1 (RCS)\n\n## syntax\n$B$: is batch size\n## SA_Layer 1 (D-FPS)\n**Input:**\n- xyz: (B,512,3) \u003c- `npoints=512` \n- feature:  (B,1,512) \u003c- 1 feature for 512 pts\n\n**Process**\n1. D-FPS to sample 512 points\n   \n3. Grouping: create `new_feature_list`\n\t1. ball query with r=0.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t\n\t3. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n\t5. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e) \n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e32+32+64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=128`, `out_channel=64`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\n\n**Output:**\n- new_xyz: (B,512,3)\n- new_feature: (B,64,512) \u003c- 64 features for 512 pts, etc... \n\n## SA_Layer 2 (FS)\n**Input**\n- xyz: (B,512,3) \u003c-`npoint=512`\n- feature: (B,64,512) \u003c- 64 features from layer 1 \n**Process**\n1. Sample 512 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n2. Grouping\n\t1. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t3. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, then MLP=\\[64,96,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e128+128+128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\t2. Conv1d with `in_channel=384`, `out_channel=128`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,1024,3)\n- new_feature: (B,128,1024)\n\n## SA_Layer 3 (F-FPS, D-FPS) \nLAYER\n**Input**\n- xyz: (B,1024,3)\n- feature: (B,128,1024)\n**Process**\n- \n**Output**\n- new_xyz: (B,512,3)\n- new_feature: (B,256,5112)\n\n## SA_Layer 4 (F-FPS, D-FPS)\n### THIS IS THE FIRST PART OF CANDIDATE GENERATION\n**Input:**\n- xyz: (B,,)\n- feature:\n**Output:**\n- new_xyz: \n- new_feature:\n\n# Vote_Layer (n/a)\n ### THIS IS THE SECOND PART OF CANDIDATE GENERATION \n \n**Input:**\n- xyz:\n- feature:\n**Output:**\n- new_xyz: \n- new_feature:\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/PCT":{"title":"Point Transformer","content":"\n[link](https://arxiv.org/pdf/2012.09164.pdf)","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/PVRCNN":{"title":"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection","content":"\n\n\n\n\n# Main Ideas\n- Two stage detector that uses the point **AND** voxel features (via sparse convolution) \n- Two novel operations: \n\t- **Voxel-to-keypoint scene encoding:** \n\t\t- which summarizes all the voxels of the overall scene feature volumes into a small number of feature keypoints\n\t- **point-to-grid RoI feature abstraction:** \n\t\t- effectively aggregates the scene keypoint features to RoI grids for proposal confidence prediction and location refinement\n\n# Network Diagram\n![[notes/images/pvrcnn.png]]\n# Feature Encoding \n- Input points **P** are split into voxels $L\\times W \\times H$  in $(x,y,z)$ respectively. \n- A $3\\times 3 \\times 3$  (3D) kernel is used in the sparse convulation.\n\t- result: **P** into feature volumes with $1\\times,2\\times,4\\times,8\\times$ downsampled sizes. \n\t\t- $2\\times$ means the voxel size (or $L/W/H$?) is $2\\times$ larger, so there are few voxels.   \n- One we have the the $8\\times$ downsampled feature volumes $\\left(\\frac{L}{8}\\times \\frac{W}{8}\\times \\frac{H}{8}\\right)$ \n\t- we stack the voxels along the $Z$ axis to get a $\\frac{L}{8}\\times \\frac{W}{8}$ bird-view feature maps.\n\t- \u003cspan style=\"color:red\"\u003ewhat is stacking here \u003c/span\u003e\n- Each class has $2\\times \\frac{L}{8} \\times \\frac{W}{8}$ 3D anchor boxes, and two anchors of $0^\\circ$ and $90^\\circ$ orientations. each anchor box is evaluated for each pixel of the BEV feature map.\n\n\n# Voxel Set Abstraction Module\n- from **P**, we first use [FPS](notes/papers/PointNet++#Sampling Layer ) to sample $n$ keypoints $\\mathcal{K}=\\{p_1,\\ldots,p_n\\}$ \n- Denote: $$\\mathcal{F}^{(l_k)}=\\{f_1^{(l_k)},\\ldots,f_{N_k}^{(l_k)}\\}: \\text{set of voxel-wise feature vectors at in }k\\text{th level}$$ $$\\mathcal{V}^{(l_k)}=\\{v_1^{(l_k)},\\ldots,v_{N_k}^{(l_k)}\\}: \\text{set o}$$ \n- Where $N_k$ is the number of non-empty voxels in the $k$th level\n\nFor each keypoint $p_i$ we find neighboarding non-empty voxels at the $k$th level within radius $r_k$. The resulting set of voxel-wise features vectors is:$$\nS_{i}^{(l_k)}=\n  \\left\\{ \n    [f_j^{(l_k)};\\underbrace{v_{j}^{(l_k)}-p_i}_{\\text{relative coords}}]^T\n  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert v_{j}^{(l_k)}-p_i\\rVert^2 \u003c r_{k}, \\\\[1ex]\n    \\forall v_{j}^{(l_k)}\\in \\mathcal{V}^{(l_k)}, \\\\[1ex]\n    \\forall f_{j}^{(l_k)}\\in\\mathcal{F}^{(l_k)}\n  \\end{array}\n  \\right\\}$$\n  where:\n  - $v_{j}^{(l_k)}-p_i$ : relative coordinates/location of $f_j^{(l_k)}$\n  - $f_j^{(l_k)}$: semantic voxel feature\n\n- The voxel-wise features within neighboring voxel set $S_{i}^{(l_k)}$ is then passed into a PointNet block:$$f_{i}^{(pv_k)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(S_{i}^{(l_k)}\\right)\\right)\\right\\}\\tag{2}$$\n- where:\n- $M(\\cdot)$: randomly sampling at most $T_k$ voxels from $S_{i}^{(l_k)}$. (for saving computation)\n- $G(\\cdot)$: MLP \n- $\\max(\\cdot)$: max-pooling along channel dimension.\n\t- The result of the block are the features of key point $p_i$ \n\n\nThen for each keypoint $p_i$ , we concatinate its features from different levels\n$$f_i^{(pv)}=\\left[f_{i}^{(pv_1)},f_{i}^{(pv_2)},f_{i}^{(pv_3)},f_{i}^{(pv_4)}\\right], \\text{for } i=1,\\ldots,n$$\nwhere:\n- $f_i^{(pv)}$: is 3D voxel CNN-based feature + pointnet features as explained above\n\n\n# Extended VSA Module\n- With the 8x downsampled 2D BEV feature map, and original point cloud **P**, put **P** through eq 2. We call these features $f_i^{(raw)}$ \n- Also project each $p_i$ onto the BEV map, and use bilinear interpolation to get $f_i^{(bev)}$ from BEV feature map\n\t- \u003cspan style=\"color:red\"\u003eHow exactly?\u003c/span\u003e\n - Then finally concatinate all these features together:$$f_i^{(p)}=\\left[f_{i}^{(pv)},f_i^{(raw)},f_{i}^{(bev)}\\right], \\text{ for }i=1,\\ldots,n$$\n\n\n# Predicted Keypoint Weighting\n![[notes/images/PKW.png]]\n- Because [FPS](notes/papers/PointNet++#Sampling Layer) might chose background points, we want to put more weight into the foreground points + their features. \n- Take the keypoint features, and pass it through a three layer mlp: $\\mathcal{A}(f_i^{(p)})$  with sigmoid as the final layer as illustrated above. \n\t- In training this is supervised via checking if the keypoint is in a GT box or not. i.e.: $p_i$ has label $1$ if $p_i$ in GT, else $0$. Focal loss is used here. \n- weights are then multiplied by the features: $$\\tilde{f_i}^{(p)}=\\mathcal{A}(f_i^{(p)})\\cdot f_{i}^{(p)}$$\n\n# RoI-grid Pooling via Set Abstraction\n- After all the previous steps, we have the set of keypoint features $$\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f_{i}}^{(p)},\\ldots,\\tilde{f_{n}}^{(p)}\\right\\}$$ \n- For each 3D proposal **(RoI)** we need to aggregate the keypoint features\n- We uniformly sample $6\\times6\\times6$ **grid points** within each 3D proposal **(RoI)** denoted as $\\mathcal{G}=\\{g_1,\\ldots,g_{216}\\}$\n\t-  (Grid points need not be points from raw pointcloud **P**) \n\t![[notes/images/roi_pooling.png]]\n- For each *grid point*, we gather all *key point* features within radius $\\tilde{r}$ $$\\tilde{\\Psi}=\\left\\{\\left[\\tilde{f}_{j}^{(p)};p_j-g_i\\right]^T  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert p_{j}-g_i\\rVert^2 \u003c \\tilde{r}, \\\\[1ex]\n    \\forall p_{j}\\in \\mathcal{K}, \\\\[1ex]\n    \\forall \\tilde{f}_{j}^{(p)}\\in\\tilde{\\mathcal{F}}\n  \\end{array}\\right\\}$$\n- $p_j-g_i$: local coordinates of features $\\tilde{f_j}^{(p)}$ relative to grid point $g_i$ \n- pointnet is used again to aggregate features: $$\\tilde{f}_i^{(g)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(\\tilde{\\Psi}\\right)\\right)\\right\\}$$\n- Two seperate MLP heads (256 dims) are then used for box refinement and confidence. \n\t- **Box Refinement:** for each of the RoI, it predicts the residuals compared to GT. $$L_{iou}=-y_k\\log(\\tilde{y}_k)-(1-y_k)\\log(1-\\tilde{y}_k)$$\n\t\t- where $\\tilde{y}_k$ is predicted score by network \n\t- **Confidence:** For the $k$th RoI, its confidence for a target $y_k$ is normalized to be between $[0,1]$ $$y_k=\\min(1,\\max(0,2\\text{IoU}_k-0.5))$$\n\t\t- Where $\\text{IoU}_k$ is the IoU of the $k$th RoI w.r.t to tis GT box.\n\t- Both are optimized via smooth-L1  \n\n\n# Training Loss\n- 3 Losses are used:\n\t- Region proposal loss: $L_{rpn}$ $$L_{rpn}=\\underbrace{L_{cls}}_{\\text{focal loss}}+\\beta \\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^a}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^a)$$\n\t\t- predicted residual: $\\widehat{\\Delta r^a}$ \n\t\t- regression target: $\\Delta r^a$\n\t- Keypoint segmentation loss: $L_{seg}$ \n\t\t- loss calculation is explained in the [Predicted Keypoint Weighting](notes/papers/PVRCNN#Predicted Keypoint Weighting)\n\t- Proposal refinement loss: $L_{rcnn}$ $$L_{rcnn}=L_{iou}+\\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^p}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^p)$$\n\t\t- predicted box residual: $\\widehat{\\Delta r^p}$\n\t\t- proposal regression target: $\\Delta r^p$\n- Overall loss is sum of these three with equal loss weights. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/PointNet++":{"title":"PointNet++","content":"\n\n## Set Abstraction (SA) Layer\n![[notes/images/pointnet2.png]]\n\n### Sampling Layer \nLets call the sampling layer $SL$.\n- Input: $N\\times 3$ \n\t- N points with $x,y,z$\n- Output: $M\\times 3$ \n\t- $M$ points with $x,y,z$ where points in $M$ are subset of $N$\n- Method: **Iterative Farthest Point Sampling** (FPS)\n\t1. Start by choosing 1 random point\n\t2. Calculate distance for all remaining points to selected points.\n\t\t1. so each point will have an array keeping track of the distances \n\t\t3. take the minimum of the arrray, i.e. for each remaining point, set its distance to the closest selected point.\n\t\t4. Finally, select the point with the greatest distance\n\t3. Repeat step 2 $M-1$ more times\n\t- **1D Example:** Consider an array of points $P=[1,6,7,8,20]$, \n\t\t1. suppose the list of selected points $S=[7,20]$ , $P$ is now $[1,6,8]$\n\t\t2. we calculate distances:\n\t\t   $$\\begin{align*}dist_1=[6,19]\\\\\n\t\t   dist_6=[1,14]\\\\\n\t\t   dist_8=[1,12]\\\\\n\t\t   \\end{align*}$$\n\t\t3. perform $min (dist_i)$$$\\begin{align*}dist_1=6\\\\\n\t\t   dist_6=1\\\\\n\t\t   dist_8=1\\\\\n\t\t   \\end{align*}$$\n\t\t4. now select the point with highest distance. i.e. $$\\underset{i}{\\operatorname{argmax}}(dist_i)=1$$\n\t\t5. So the next chosen point is 1. $S=[1,7,20]$ and $P=[6,8]$\n\n\n\n### Grouping Layer\n- Sampling Layer $SL$ yields $N'\\times 3$ points.\n\t- $N'$ centroids and $3:= x,y,z$ coordinates \n\t- $SL:(N\\times 3) \\to (N'\\times 3)$\n- Input: \n\t- $(N\\times 3+ C)$: points before sampling and their feature vectors of len $C$\n\t-  $(N'\\times d)$: predicted centroids \n- Output: \n\t- $(N'\\times K \\times (3+C))$: centroid + $K$ points within radius of $r$ along their features\n- Method: \n\t- Ball query\n\t\t\t- what if points within radius is $\u003cK$? \n\t\t\t- what if no points (only centroid point)?\n\t\t\t- with if more than $K$ points?  \n\n#### Multi-scale grouping (MSG)\n- Due to nonuniformity of point clouds, we needs some more tricks to make feature learning more robust. \n- One simple way is to applying grouping + pointnet multiple times then concat their features \n![[notes/images/msg.png|300]]\n\n\n\n#### Multi-resolution grouping (MRG)\n- MSG works, but its computationally expensive, so this is an alternative method \n- Consider a layer for input of points, we first apply set abstraction layer (sampling + grouping + pointnet), to yield a vector of features $L_1$. We then  apply pointnet of the raw pointcloud (before set abstraction) to yield feature vector $L_2$. Then we concat the results $(L_1,L_2)$ \n\n![[notes/images/mrg.png|300]]\n### PointNet Layer\nessentially just a FC layer on each \"ball\" of points\n- Input $(N'\\times K \\times (3+C))$.\n\t- $N'$ local regions (balls of points)\n\t- $K$ is num of balls in the ball\n\t- $3+C = x,y,z+C \\text{ features}$\n- Output: $(N'\\times (3+C'))$ \n- Method: \n\t- for each point $x_j$ in a ball, transform it into a local coordinate frame relative to its centroid $\\hat{x}$ $$x^{new}_j = x_j-\\hat{x}$$\n\t- Then we apply series of FC layers\n\n# Feature Propogation for Set Segmentation \n\n\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/TANet":{"title":"TANet: Robust 3D Object Detection from Point Clouds with Triple Attention","content":"\n[link](https://arxiv.org/pdf/1912.05163.pdf)\n\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/attentionalpointnet":{"title":"Attentional PointNet for 3D-Object Detection in Point Clouds","content":"\n[link](https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf)\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/iassd":{"title":"Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds","content":"\n# Main Ideas \n- Turns out [F-FPS](notes/papers/3DSSD) from 3DSSD is still not good enough in preserving foreground points. \n\t- Introduced Class-aware Sampling\n\n\n\n# Class-Aware Sampling\n- Superior sampling method compared to [F-FPS](notes/papers/3DSSD)  and [D-FPS](notes/papers/PointNet++)\n-  Using vanilla cross-entropy loss $$L_{cls-aware}=-\\sum^{C}_{c=1}(s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n- $C:$ Number of catergories \n- $s_i:$ One hot labels \n- $\\hat{s_i}:$ predicted logits  \n- ","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/papers/pointformer":{"title":"Pointformer","content":"\n[link](https://arxiv.org/pdf/2012.11409.pdf)","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/preview-changes":{"title":"Preview Changes","content":"\nIf you'd like to preview what your Quartz site looks like before deploying it to the internet, here's exactly how to do that!\n\nNote that both of these steps need to be completed.\n\n## Install `hugo-obsidian`\nThis step will generate the list of backlinks for Hugo to parse. Ensure you have [Go](https://golang.org/doc/install) (\u003e= 1.16) installed.\n\n```shell\n# Install and link `hugo-obsidian` locally\n$ go install github.com/jackyzha0/hugo-obsidian@latest\n```\n\nIf you are running into an error saying that `command not found: hugo-obsidian`, make sure you set your `GOPATH` correctly! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\nAfterwards, start the Hugo server as shown above and your local backlinks and interactive graph should be populated!\n\n##  Installing Hugo\nHugo is the static site generator that powers Quartz. [Install Hugo with \"extended\" Sass/SCSS version](https://gohugo.io/getting-started/installing/) first. Then,\n\n```\n# Navigate to your local Quartz folder\n$ cd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\n$ make serve\n\n# View your site in a browser at http://localhost:1313/\n```\n\n\u003e üåç Step 5: [Hosting Quartz online!](notes/hosting.md)","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/setup":{"title":"Setup","content":"\n## Making your own Quartz\nSetting up Quartz requires a basic understanding of `git`. If you are unfamiliar, [this resource](https://resources.nwplus.io/2-beginner/how-to-git-github.html) is a great place to start!\n\n### Forking\n\u003e A fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\n\nNavigate to the GitHub repository for the Quartz project:\n\nüìÅ [Quartz Repository](https://github.com/jackyzha0/quartz)\n\nThen, Fork the repository into your own GitHub account. If you don't have an account, you can make on for free [here](https://github.com/join). More details about forking a repo can be found on [GitHub's documentation](https://docs.github.com/en/get-started/quickstart/fork-a-repo).\n\n### Cloning\nAfter you've made a fork of the repository, you need to download the files locally onto your machine. Ensure you have `git`, then type the following command replacing `YOUR-USERNAME` with your GitHub username.\n\n```shell\n$ git clone https://github.com/YOUR-USERNAME/quartz\n```\n\n## Editing\nGreat! Now you have everything you need to start editing and growing your digital garden. If you're ready to start writing content already, check out the recommended flow for editing notes in Quartz.\n\n\u003e ‚úèÔ∏è Step 2: [Editing Notes in Quartz](notes/editing.md)\n\nHaving problems? Checkout our [FAQ and Troubleshooting guide](notes/troubleshooting.md).\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/notes/updating":{"title":"Updating","content":"\nHaven't updated Quartz in a while and want all the cool new optimizations? On Unix/Mac systems you can run the following command for a one-line update! This command will show you a log summary of all commits since you last updated, press `q` to acknowledge this. Then, it will show you each change in turn and press `y` to accept the patch or `n` to reject it. Usually you should press `y` for most of these unless it conflicts with existing changes you've made! \n\n```shell\nmake update\n```\n\nOr, if you don't want the interactive parts and just want to force update your local garden (this assumed that you are okay with some of your personalizations been overriden!)\n\n```shell\nmake update-force\n```\n\nOr, manually checkout the changes yourself.\n\n\u003e ‚ö†Ô∏è **WARNING** ‚ö†Ô∏è\n\u003e\n\u003e If you customized the files in `data/`, or anything inside `layouts/`, your customization may be overwritten!\n\u003e Make sure you have a copy of these changes if you don't want to lose them.\n\n\n```shell\n# add Quartz as a remote host\ngit remote add upstream git@github.com:jackyzha0/quartz.git\n\n# index and fetch changes\ngit fetch upstream\ngit checkout -p upstream/hugo -- layouts .github Makefile assets/js assets/styles/base.scss assets/styles/darkmode.scss config.toml data \n```\n","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null},"/tags/setup":{"title":"","content":"","lastmodified":"2022-07-03T05:01:05.77725585Z","tags":null}}