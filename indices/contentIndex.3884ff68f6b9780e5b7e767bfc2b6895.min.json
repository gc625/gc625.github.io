{"/":{"title":"{{Gabriel Chan's website}}","content":"\nHi, this is my website where I keep my notes for research, classes and personal projects. checkout my [github](https://github.com/gc625) too!\n\n\n# Research Notes\n## Point Cloud Networks\n- [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](notes/papers/PointNet++)\n### Single Stage Detectors \n- [3DSSD: Point-based 3D Single Stage Object Detector](notes/papers/3DSSD)\n- [(IASSD)Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds](notes/papers/iassd)\n\n### Two Stage Detectors\n- [RVRCNN](notes/papers/PVRCNN)\n  \n### Attention on point clouds\n- [3DETR: An End-to-End Transformer Model for 3D Object Detection](notes/papers/3DETR)\n- [Attentional PointNet for 3D-Object Detection in Point Clouds](notes/papers/attentionalpointnet)\n- [Point Transformer](notes/papers/PCT)\n- [TANet: Robust 3D Object Detection from Point Clouds with Triple Attention](notes/papers/TANet)\n- [3D Object Detection with Pointformer](notes/papers/pointformer)\n\n\n# Basic of Machine Learning\n- [Surpervised Learning](notes/back2basics/supervisedlearning)\n- [Unsupervised Learning](notes/back2basics/unsupervisedlearning)\n\n\n# Machine Learning frameworks\n- [OpenPCDet](notes/openPCDet)\n\n","lastmodified":"2022-07-03T06:35:58.73209634Z","tags":null},"/notes/HonorsSeminar":{"title":"Research Seminar Ideas","content":"\n\n# All Point Cloud Transformer papers\n\n- [3DETR](notes/papers/3DETR)\n- [Pointformer](notes/papers/pointformer)\n- [Point transformer](notes/papers/pointformer)\n- [Attentional Pointnet](notes/papers/attentionalpointnet)\n- [Triple Attention Net](notes/papers/TANet)\n\n\n\n","lastmodified":"2022-07-03T06:35:58.776096554Z","tags":null},"/notes/back2basics/supervisedlearning":{"title":"Supervised Learning","content":"\n\n# What is supervised learning?\n\n- Training an algorithm to output $y$ for a given $x$ using sufficient training samples $\\{(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\\ldots,(x^{(n)},y^{(n)})\\}$ for some input $x^{(i)}$ and **correct** output $y^{(i)}$ \n- **Regression:** predicting an a number (infinitely many outputs)\n- **Classification:** predicting categories (finite outputs)\n\n\n# Linear Regression \n- Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\\hat{y}$ given an input $x$ \n- for linear regression, $f$ is a straight line. With parameters $w,b$ we can then represent $f$ as: $$f_{w,b}(x)=wx+b$$\n## Cost Function\n- Since our objective to find $w,b$ such taht $\\hat{y}^{(i)}$ is close to $y^{(i)}$ for all $(x^{(i)},y^{(i)})$. \n- Squared error cost function:  $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$\n- where $m=$ number of training examples. So $\\frac{1}{m}$ is to average it so it doesnt blow up, factor of $2$ is for computational convience later. \n- Now we can also rewrite it as: $$J(w,b)=\\frac{1}{2m}\\sum^{m}_{i=1}f_{w,b}(x^{(i)})-y^{(i)})^2$$\n- This can be solved analytically for simple cost functions, but for complicated $J$, we can use gradiant descent to minimize $J$ instead: \n\n## Gradient Descent\n- initialize $w,b$, calcuated $J$ \n- adjust $w,b$ to decrease $J$ \n- repeat until hopefully $J$ settles near minimum \n\n- Step 1: ($=$ here is assignment, not equals)\n$$w=w -\\alpha \\frac{d}{dw}J(w,b)$$ where $\\alpha$ is the learning rate, a **hyperparameter** that controls the \"fast\" we change $w$ \n- Step 2: do the same for $b$ $$b=b -\\alpha \\frac{d}{db}J(w,b)$$\n- **Note:** $w$ and $b$ must be updated at the same time. \n### Learning rate\n- If $\\alpha$ is too small, then it will take many steps to reach minimum \n- If $\\alpha$ is too large, then it might never reach the minimum \n\n# Gradient Descent for linear regression\nCalculating derivatives   for $w$, $$\\frac{d}{dw}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$=\\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)}-y^{(i)})x^{(i)}$$\n\nand derivative for $b$, \n$$\\frac{d}{db}J(w,b)= \\frac{d}{dw}\\frac{1}{2m}\\sum^{m}_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2$$$$ =\\frac{d}{db}\\frac{1}{2m}\\sum^{m}_{i=1}(wx^{(i)}+b-y^{(i)})^2$$\nwhich is equal to \n$$\\frac{1}{m}\\sum^{m}_{i=1}(f_{w,b}(x^{(i)})-y^{(i)})$$\n\nPsuedocode for gradient descent:\n```python\nwhile J not converged:\n\tw = w - a * dJdW\n\tb = b- a * dJdb\n```\nwhere `dJdW` = $\\frac{d}{dw}J(w,b)$ and `dJdb` = $\\frac{d}{db}J(w,b)$\n\n\n\n# Multiple features\nwhat if you have multiple features (variables)? \n\n- $x_j = j^{th}$ feature\n- $n$ = number of features\n- $\\vec{x}^{(i)}$= features of $i^{th}$ training example\n- $x_{j}^{(i)}$ = value of feature $j$ in the $i^{th}$ training example\n\nWe can then express the linear regression model as:\n\n$$f_{w,b}(x)=w_1x_1+w_2x_2+\\cdots+w_nx_n+b$$\ndefine $\\vec{w} = [w_1,\\ldots,w_n]$ and $\\vec{x}=[x_1,\\ldots x_n]^T$, $T$ here represents transpose. Then \n$$f_{\\vec{w},b}=\\vec{w}\\cdot \\vec{x}+b$$Where $(\\cdot)$ represents the dot product. \n\n\n\n# Feature Scaling\nWhen the range of values your features can take up differ greatly, i.e. \n- $x_1$ = square footage of house $\\in [500,5000]$ \n- $x_2$ = number of bedrooms $\\in [1,5]$\n\nthis may cause gradient descent to run slowly. ![[notes/images/feature_scaling.png]]\n\nSome examples of feature scaling\n## max scaling\n- divide each data point for a feature by the max value for that feature.\n\n## mean normalization\n- eg: if $300 \\leq x_1 \\leq 2000$ , we can scale it like such $$x_{1new} = \\frac{x_1-\\mu_1}{2000-300}$$\n- where $\\mu_1$ = mean\n\n\n## Z-score normalization \n- find standard deviation $\\sigma$ , mean $\\mu$ then $$x_1=\\frac{x_1-\\mu_1}{\\sigma_1}$$\n\n\n\n\n","lastmodified":"2022-07-03T06:35:58.776096554Z","tags":null},"/notes/back2basics/unsupervisedlearning":{"title":"Unsupervised Learning","content":"\n\n# What is Unsupervised Learning\n\n- no labelled data, algorithm finds something interesting in unlabeled data.\n\t- given only inputs $\\{x_0,\\ldots,x_n\\}$, but no output labels $\\{y_0,\\ldots,y_n\\}$\n- **Clustering:**  for example, groups data points together\n","lastmodified":"2022-07-03T06:35:58.776096554Z","tags":null},"/notes/interesting/Cross-Entropy":{"title":"Cross Entropy","content":"\nhttps://chris-said.io/2020/12/26/two-things-that-confused-me-about-cross-entropy/\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/openPCDet":{"title":"OpenPCDet","content":"# OpenPCDet \n\n## Code Architecture \nfile structure for a project should look something like this:\n\n### Directories\n```bash\nOpenPCDet (or proj name)\n├── data \n│   ├── kitti\n│   ├── lyft\n│   └── waymo\n├── docker\n├── docs\n├── pcdet\n│   ├── datasets\n│   ├── models\n│   ├── ops\n│   └── utils\n└── tools\n    ├── cfgs\n    ├── eval_utils\n    ├── scripts\n    ├── train_utils\n    └── visual_utils\n```\n\n\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/3DETR":{"title":"An End-to-End Transformer Model for 3D Object Detection","content":"\n[link](https://openaccess.thecvf.com/content/ICCV2021/papers/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.pdf)\n\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/3DSSD":{"title":"3DSSD: Point-based 3D Single Stage Object Detector","content":"\n[arXiv link](https://arxiv.org/pdf/2002.10187.pdf) \ngithub #todo \n\n\n## Main Ideas:\n-  Introduced **Feature-Farthest Point Sampling (F-FPS)**\n- Introduced new sampling method called **Fusion Sampling** in [notes/papers/PointNet++#Set Abstraction SA Layer](Set Abstraction Layer)\n\n\n### Feature-Farthest Point Sampling (F-FPS)\n- Objective when downsample: \n\t1. Remove **negative points** (background points) \n\t2. Preserve only **positive points** (foreground points, i.e: points within any instance := ground truth box)\n- Therefore leverage semantic features of points as well when applying FPS \n- Given two points $A$ and $B$, the criterion used to compare them in FPS is:$$C(A,B)=\\lambda L_d(A,B)+L_f(A,B)$$\n\t\twhere \n\t\t- $L_d(A,B)$ is $L^2$ euclidean distance (xyz) \n\t\t- $L_f(A,B)$ is $L^2$ feature distance (distance between the two feature vectors)\n\t\t- $\\lambda$ is chosen parameter, paper seems to choose $\\lambda=1$\n\t\t- *reminder*: $L^2(A,B)= \\sqrt{(B_1-A_1)^2+(B_1-A_1)^2+\\cdots+(B_n-A_n)^2}$  if $A$ and $B$ are $n$ dimensional vectors\n- Result should be a subset of points that are less redundant and more diverse, as points are not only physically distant when sampling, but also in feature space.  \n\n### Fusion Sampling\n- Downsampling to $N_m$ points with **F-FPS** results in:\n\t\t- lots of positive points -\u003e good for regression \n\t\t- few negative points (due to limiting ) -\u003e bad for classification\n\t\t- **Why?** Negative points don't have enough neighbours #expand \n- Input: $N_i\\times C_i :=$ $N$ points each with feature vector of length $C$\n- want to output $N_{i+1}$ points, where $N_{i+1}$ points are subset of the $N_i$ points\n\t1. F-FPS$: N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t2. D-FPS:$N_i \\to \\frac{N_{i+1}}{2}$ #Q\n\t3. [grouping operation](notes/papers/PointNet++#Grouping Layer)\n\t4. MLP\n\t5. MaxPool\n\n![[notes/images/3dssdbackbone.png]]\n\n\n\n\n# Network walkthrough\nusing the following network config: ![[notes/images/3dssdcfg.png]]\n\n\n## Network Input:\n- Radar pts of dim 4: $[x,y,z,RCS]$\n- **input size** is determined by `sample_points` in `DATA_PROCESSOR`: e.g.: 512 \n- then the feature dimension is 1 (RCS)\n\n## syntax\n$B$: is batch size\n## SA_Layer 1 (D-FPS)\n**Input:**\n- xyz: (B,512,3) \u003c- `npoints=512` \n- feature:  (B,1,512) \u003c- 1 feature for 512 pts\n\n**Process**\n1. D-FPS to sample 512 points\n   \n3. Grouping: create `new_feature_list`\n\t1. ball query with r=0.2, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e), append to `new_feature_list`\n\t\t\n\t3. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e) \n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n\t5. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, MLP=\\[16,16,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e) \n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e32\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e) , append to `new_feature_list`\n\n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e32+32+64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\t2. Conv1d with `in_channel=128`, `out_channel=64`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e64\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e512\u003c/span\u003e)\n\n**Output:**\n- new_xyz: (B,512,3)\n- new_feature: (B,64,512) \u003c- 64 features for 512 pts, etc... \n\n## SA_Layer 2 (FS)\n**Input**\n- xyz: (B,512,3) \u003c-`npoint=512`\n- feature: (B,64,512) \u003c- 64 features from layer 1 \n  \n**Process**\n1. Sample 512 points via D-FPS and F-FPS, then concat them together (total pts=\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n2. Grouping\n\t1. ball query with r=0.4, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then  MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t2. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t3. ball query with r=0.8, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e, then MLP=\\[64,64,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e32\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t4. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n\t\t  \n\t5. ball query with r=1.6, nsample=\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e, then MLP=\\[64,96,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e\\] \n\t\t- new_feauture: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e+3,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e,\u003cspan style=\"color: RoyalBlue\"\u003e64\u003c/span\u003e)  \u003c- the +3 here is xyz of each sampled point\n\t6. Maxpool and squeeze last channel `[-1]`\n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e), append to `new_feature_list`\n \n3. Aggregation Channel:\n\t1. `torch.cat` all features along `dim=1`\n\t\t-  new_feature: (B,\u003cspan style=\"color:red\"\u003e128+128+128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\t2. Conv1d with `in_channel=384`, `out_channel=128`, `kernel_size = 1`, batchnorm1d and ReLU \n\t\t- new_feature: (B,\u003cspan style=\"color: red\"\u003e128\u003c/span\u003e,\u003cspan style=\"color: green\"\u003e1024\u003c/span\u003e)\n\n**Output**\n- new_xyz: (B,1024,3)\n- new_feature: (B,128,1024)\n\n## SA_Layer 3 (F-FPS, D-FPS) \n- Not sure why we use F-FPS and D-FPS instead of just FS, I think this is to make sure the set of points sampled $F\\text{-}FPS \\cap D\\text{-}FPS =\\emptyset$ ![[notes/images/samplingrange.png]]\n\t- so in this layer, F-FPS samples `[0:512]` pts, then D-FPS samples from `[512:1024]`\n\t- but then \n**Input**\n- xyz: (B,1024,3)\n- feature: (B,128,1024)\n**Process**\n- \n**Output**\n- new_xyz: (B,512,3)\n- new_feature: (B,256,5112)\n\n## SA_Layer 4 (F-FPS, D-FPS)\n### THIS IS THE FIRST PART OF CANDIDATE GENERATION\n**Input:**\n- xyz: (B,,)\n- feature:\n**Output:**\n- new_xyz: \n- new_feature:\n\n# Vote_Layer (n/a)\n ### THIS IS THE SECOND PART OF CANDIDATE GENERATION \n \n**Input:**\n- xyz:\n- feature:\n**Output:**\n- new_xyz: \n- new_feature:\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/PCT":{"title":"Point Transformer","content":"\n[link](https://arxiv.org/pdf/2012.09164.pdf)","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/PVRCNN":{"title":"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection","content":"\n\n\n\n\n# Main Ideas\n- Two stage detector that uses the point **AND** voxel features (via sparse convolution) \n- Two novel operations: \n\t- **Voxel-to-keypoint scene encoding:** \n\t\t- which summarizes all the voxels of the overall scene feature volumes into a small number of feature keypoints\n\t- **point-to-grid RoI feature abstraction:** \n\t\t- effectively aggregates the scene keypoint features to RoI grids for proposal confidence prediction and location refinement\n\n# Network Diagram\n![[notes/images/pvrcnn.png]]\n# Feature Encoding \n- Input points **P** are split into voxels $L\\times W \\times H$  in $(x,y,z)$ respectively. \n- A $3\\times 3 \\times 3$  (3D) kernel is used in the sparse convulation.\n\t- result: **P** into feature volumes with $1\\times,2\\times,4\\times,8\\times$ downsampled sizes. \n\t\t- $2\\times$ means the voxel size (or $L/W/H$?) is $2\\times$ larger, so there are few voxels.   \n- One we have the the $8\\times$ downsampled feature volumes $\\left(\\frac{L}{8}\\times \\frac{W}{8}\\times \\frac{H}{8}\\right)$ \n\t- we stack the voxels along the $Z$ axis to get a $\\frac{L}{8}\\times \\frac{W}{8}$ bird-view feature maps.\n\t- \u003cspan style=\"color:red\"\u003ewhat is stacking here \u003c/span\u003e\n- Each class has $2\\times \\frac{L}{8} \\times \\frac{W}{8}$ 3D anchor boxes, and two anchors of $0^\\circ$ and $90^\\circ$ orientations. each anchor box is evaluated for each pixel of the BEV feature map.\n\n\n# Voxel Set Abstraction Module\n- from **P**, we first use [FPS](notes/papers/PointNet++#Sampling Layer ) to sample $n$ keypoints $\\mathcal{K}=\\{p_1,\\ldots,p_n\\}$ \n- Denote: $$\\mathcal{F}^{(l_k)}=\\{f_1^{(l_k)},\\ldots,f_{N_k}^{(l_k)}\\}: \\text{set of voxel-wise feature vectors at in }k\\text{th level}$$ $$\\mathcal{V}^{(l_k)}=\\{v_1^{(l_k)},\\ldots,v_{N_k}^{(l_k)}\\}: \\text{set o}$$ \n- Where $N_k$ is the number of non-empty voxels in the $k$th level\n\nFor each keypoint $p_i$ we find neighboarding non-empty voxels at the $k$th level within radius $r_k$. The resulting set of voxel-wise features vectors is:$$\nS_{i}^{(l_k)}=\n  \\left\\{ \n    [f_j^{(l_k)};\\underbrace{v_{j}^{(l_k)}-p_i}_{\\text{relative coords}}]^T\n  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert v_{j}^{(l_k)}-p_i\\rVert^2 \u003c r_{k}, \\\\[1ex]\n    \\forall v_{j}^{(l_k)}\\in \\mathcal{V}^{(l_k)}, \\\\[1ex]\n    \\forall f_{j}^{(l_k)}\\in\\mathcal{F}^{(l_k)}\n  \\end{array}\n  \\right\\}$$\n  where:\n  - $v_{j}^{(l_k)}-p_i$ : relative coordinates/location of $f_j^{(l_k)}$\n  - $f_j^{(l_k)}$: semantic voxel feature\n\n- The voxel-wise features within neighboring voxel set $S_{i}^{(l_k)}$ is then passed into a PointNet block:$$f_{i}^{(pv_k)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(S_{i}^{(l_k)}\\right)\\right)\\right\\}\\tag{2}$$\n- where:\n- $M(\\cdot)$: randomly sampling at most $T_k$ voxels from $S_{i}^{(l_k)}$. (for saving computation)\n- $G(\\cdot)$: MLP \n- $\\max(\\cdot)$: max-pooling along channel dimension.\n\t- The result of the block are the features of key point $p_i$ \n\n\nThen for each keypoint $p_i$ , we concatinate its features from different levels\n$$f_i^{(pv)}=\\left[f_{i}^{(pv_1)},f_{i}^{(pv_2)},f_{i}^{(pv_3)},f_{i}^{(pv_4)}\\right], \\text{for } i=1,\\ldots,n$$\nwhere:\n- $f_i^{(pv)}$: is 3D voxel CNN-based feature + pointnet features as explained above\n\n\n# Extended VSA Module\n- With the 8x downsampled 2D BEV feature map, and original point cloud **P**, put **P** through eq 2. We call these features $f_i^{(raw)}$ \n- Also project each $p_i$ onto the BEV map, and use bilinear interpolation to get $f_i^{(bev)}$ from BEV feature map\n\t- \u003cspan style=\"color:red\"\u003eHow exactly?\u003c/span\u003e\n - Then finally concatinate all these features together:$$f_i^{(p)}=\\left[f_{i}^{(pv)},f_i^{(raw)},f_{i}^{(bev)}\\right], \\text{ for }i=1,\\ldots,n$$\n\n\n# Predicted Keypoint Weighting\n![[notes/images/PKW.png]]\n- Because [FPS](notes/papers/PointNet++#Sampling Layer) might chose background points, we want to put more weight into the foreground points + their features. \n- Take the keypoint features, and pass it through a three layer mlp: $\\mathcal{A}(f_i^{(p)})$  with sigmoid as the final layer as illustrated above. \n\t- In training this is supervised via checking if the keypoint is in a GT box or not. i.e.: $p_i$ has label $1$ if $p_i$ in GT, else $0$. Focal loss is used here. \n- weights are then multiplied by the features: $$\\tilde{f_i}^{(p)}=\\mathcal{A}(f_i^{(p)})\\cdot f_{i}^{(p)}$$\n\n# RoI-grid Pooling via Set Abstraction\n- After all the previous steps, we have the set of keypoint features $$\\tilde{\\mathcal{F}}=\\left\\{\\tilde{f_{i}}^{(p)},\\ldots,\\tilde{f_{n}}^{(p)}\\right\\}$$ \n- For each 3D proposal **(RoI)** we need to aggregate the keypoint features\n- We uniformly sample $6\\times6\\times6$ **grid points** within each 3D proposal **(RoI)** denoted as $\\mathcal{G}=\\{g_1,\\ldots,g_{216}\\}$\n\t-  (Grid points need not be points from raw pointcloud **P**) \n\t![[notes/images/roi_pooling.png]]\n- For each *grid point*, we gather all *key point* features within radius $\\tilde{r}$ $$\\tilde{\\Psi}=\\left\\{\\left[\\tilde{f}_{j}^{(p)};p_j-g_i\\right]^T  \\;\\middle|\\;\n  \\begin{array}{@{}l@{}}\n    \\lVert p_{j}-g_i\\rVert^2 \u003c \\tilde{r}, \\\\[1ex]\n    \\forall p_{j}\\in \\mathcal{K}, \\\\[1ex]\n    \\forall \\tilde{f}_{j}^{(p)}\\in\\tilde{\\mathcal{F}}\n  \\end{array}\\right\\}$$\n- $p_j-g_i$: local coordinates of features $\\tilde{f_j}^{(p)}$ relative to grid point $g_i$ \n- pointnet is used again to aggregate features: $$\\tilde{f}_i^{(g)}=\\max\\left\\{G\\left(\\mathcal{M}\\left(\\tilde{\\Psi}\\right)\\right)\\right\\}$$\n- Two seperate MLP heads (256 dims) are then used for box refinement and confidence. \n\t- **Box Refinement:** for each of the RoI, it predicts the residuals compared to GT. $$L_{iou}=-y_k\\log(\\tilde{y}_k)-(1-y_k)\\log(1-\\tilde{y}_k)$$\n\t\t- where $\\tilde{y}_k$ is predicted score by network \n\t- **Confidence:** For the $k$th RoI, its confidence for a target $y_k$ is normalized to be between $[0,1]$ $$y_k=\\min(1,\\max(0,2\\text{IoU}_k-0.5))$$\n\t\t- Where $\\text{IoU}_k$ is the IoU of the $k$th RoI w.r.t to tis GT box.\n\t- Both are optimized via smooth-L1  \n\n\n# Training Loss\n- 3 Losses are used:\n\t- Region proposal loss: $L_{rpn}$ $$L_{rpn}=\\underbrace{L_{cls}}_{\\text{focal loss}}+\\beta \\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^a}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^a)$$\n\t\t- predicted residual: $\\widehat{\\Delta r^a}$ \n\t\t- regression target: $\\Delta r^a$\n\t- Keypoint segmentation loss: $L_{seg}$ \n\t\t- loss calculation is explained in the [Predicted Keypoint Weighting](notes/papers/PVRCNN#Predicted Keypoint Weighting)\n\t- Proposal refinement loss: $L_{rcnn}$ $$L_{rcnn}=L_{iou}+\\sum_{r\\in\\{x,y,z,l,w,h,\\theta\\}}\\mathcal{L}_{smooth-L1}(\\underbrace{\\widehat{\\Delta r^p}}_{\\substack{\\text{predicted} \\\\ \\text{residual}}},\\Delta r^p)$$\n\t\t- predicted box residual: $\\widehat{\\Delta r^p}$\n\t\t- proposal regression target: $\\Delta r^p$\n- Overall loss is sum of these three with equal loss weights. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/PointNet++":{"title":"PointNet++","content":"\n\n## Set Abstraction (SA) Layer\n![[notes/images/pointnet2.png]]\n\n### Sampling Layer \nLets call the sampling layer $SL$.\n- Input: $N\\times 3$ \n\t- N points with $x,y,z$\n- Output: $M\\times 3$ \n\t- $M$ points with $x,y,z$ where points in $M$ are subset of $N$\n- Method: **Iterative Farthest Point Sampling** (FPS)\n\t1. Start by choosing 1 random point\n\t2. Calculate distance for all remaining points to selected points.\n\t\t1. so each point will have an array keeping track of the distances \n\t\t3. take the minimum of the arrray, i.e. for each remaining point, set its distance to the closest selected point.\n\t\t4. Finally, select the point with the greatest distance\n\t3. Repeat step 2 $M-1$ more times\n\t- **1D Example:** Consider an array of points $P=[1,6,7,8,20]$, \n\t\t1. suppose the list of selected points $S=[7,20]$ , $P$ is now $[1,6,8]$\n\t\t2. we calculate distances:\n\t\t   $$\\begin{align*}dist_1=[6,19]\\\\\n\t\t   dist_6=[1,14]\\\\\n\t\t   dist_8=[1,12]\\\\\n\t\t   \\end{align*}$$\n\t\t3. perform $min (dist_i)$$$\\begin{align*}dist_1=6\\\\\n\t\t   dist_6=1\\\\\n\t\t   dist_8=1\\\\\n\t\t   \\end{align*}$$\n\t\t4. now select the point with highest distance. i.e. $$\\underset{i}{\\operatorname{argmax}}(dist_i)=1$$\n\t\t5. So the next chosen point is 1. $S=[1,7,20]$ and $P=[6,8]$\n\n\n\n### Grouping Layer\n- Sampling Layer $SL$ yields $N'\\times 3$ points.\n\t- $N'$ centroids and $3:= x,y,z$ coordinates \n\t- $SL:(N\\times 3) \\to (N'\\times 3)$\n- Input: \n\t- $(N\\times 3+ C)$: points before sampling and their feature vectors of len $C$\n\t-  $(N'\\times d)$: predicted centroids \n- Output: \n\t- $(N'\\times K \\times (3+C))$: centroid + $K$ points within radius of $r$ along their features\n- Method: \n\t- Ball query\n\t\t\t- what if points within radius is $\u003cK$? \n\t\t\t- what if no points (only centroid point)?\n\t\t\t- with if more than $K$ points?  \n\n#### Multi-scale grouping (MSG)\n- Due to nonuniformity of point clouds, we needs some more tricks to make feature learning more robust. \n- One simple way is to applying grouping + pointnet multiple times then concat their features \n- **Dilated Group:** implementation setting where you set a min radius.\n\t- eg: $r=[0.2,0.4,0.8]$, then \n\t- for $r=0.2$, sample within radius $[0,0.2]$\n\t- for $r=0.4$, sample within radius $[0.2,0.4]$\n\t- for $r=0.8$, sample within radius $[0.4,0.8]$\n![[notes/images/msg.png|300]]\n\n\n\n#### Multi-resolution grouping (MRG)\n- MSG works, but its computationally expensive, so this is an alternative method \n- Consider a layer for input of points, we first apply set abstraction layer (sampling + grouping + pointnet), to yield a vector of features $L_1$. We then  apply pointnet of the raw pointcloud (before set abstraction) to yield feature vector $L_2$. Then we concat the results $(L_1,L_2)$ \n\n![[notes/images/mrg.png|300]]\n### PointNet Layer\nessentially just a FC layer on each \"ball\" of points\n- Input $(N'\\times K \\times (3+C))$.\n\t- $N'$ local regions (balls of points)\n\t- $K$ is num of balls in the ball\n\t- $3+C = x,y,z+C \\text{ features}$\n- Output: $(N'\\times (3+C'))$ \n- Method: \n\t- for each point $x_j$ in a ball, transform it into a local coordinate frame relative to its centroid $\\hat{x}$ $$x^{new}_j = x_j-\\hat{x}$$\n\t- Then we apply series of FC layers\n\n# Feature Propogation for Set Segmentation \n\n\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/TANet":{"title":"TANet: Robust 3D Object Detection from Point Clouds with Triple Attention","content":"\n[link](https://arxiv.org/pdf/1912.05163.pdf)\n\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/attentionalpointnet":{"title":"Attentional PointNet for 3D-Object Detection in Point Clouds","content":"\n[link](https://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Paigwar_Attentional_PointNet_for_3D-Object_Detection_in_Point_Clouds_CVPRW_2019_paper.pdf)\n","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/iassd":{"title":"Not All Points Are Equal: Learning Highly Efficient Point-based Detectors for 3D LiDAR Point Clouds","content":"\n# Main Ideas \n- Turns out [F-FPS](notes/papers/3DSSD) from 3DSSD is still not good enough in preserving foreground points. \n\t- Introduced Class-aware Sampling\n\n\n\n# Class-Aware Sampling\n- Superior sampling method compared to [F-FPS](notes/papers/3DSSD)  and [D-FPS](notes/papers/PointNet++)\n-  Using vanilla cross-entropy loss $$L_{cls-aware}=-\\sum^{C}_{c=1}(s_i\\log(\\hat{s_i})+(1-s_i)\\log(1-\\hat{s_i}))$$\n- $C:$ Number of catergories \n- $s_i:$ One hot labels \n- $\\hat{s_i}:$ predicted logits  \n- ","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/notes/papers/pointformer":{"title":"Pointformer","content":"\n[link](https://arxiv.org/pdf/2012.11409.pdf)","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null},"/tags/setup":{"title":"","content":"","lastmodified":"2022-07-03T06:35:58.792096631Z","tags":null}}