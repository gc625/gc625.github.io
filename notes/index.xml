<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on</title><link>https://gc625.github.io/notes/</link><description>Recent content in Notes on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://gc625.github.io/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>3DSSD: Point-based 3D Single Stage Object Detector</title><link>https://gc625.github.io/notes/papers/3DSSD/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/3DSSD/</guid><description>arXiv link github #todo
Main Ideas: Introduced Feature-Farthest Point Sampling (F-FPS) Introduced new sampling method called Fusion Sampling in [[notes/papers/PointNet++#Set Abstraction SA Layer]] Feature-Farthest Point Sampling (F-FPS) Objective when downsample: Remove negative points (background points) Preserve only positive points (foreground points, i.</description></item><item><title>OpenPCDet</title><link>https://gc625.github.io/notes/openPCDet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/openPCDet/</guid><description>OpenPCDet Code Architecture file structure for a project should look something like this:
Directories 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 OpenPCDet (or proj name) ├── data │ ├── kitti │ ├── lyft │ └── waymo ├── docker ├── docs ├── pcdet │ ├── datasets │ ├── models │ ├── ops │ └── utils └── tools ├── cfgs ├── eval_utils ├── scripts ├── train_utils └── visual_utils</description></item><item><title>PointNet++</title><link>https://gc625.github.io/notes/papers/PointNet++/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/papers/PointNet++/</guid><description>Set Abstraction (SA) Layer ![[notes/images/pointnet2.png]]
Sampling Layer Lets call the sampling layer $SL$.
Input: $N\times 3$ N points with $x,y,z$ Output: $M\times 3$ $M$ points with $x,y,z$ where points in $M$ are subset of $N$ Method: Iterative Farthest Point Sampling (FPS) Start by choosing 1 random point Calculate distance for all remaining points to selected points.</description></item><item><title>Unsupervised Learning</title><link>https://gc625.github.io/notes/back2basics/unsupervisedlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/back2basics/unsupervisedlearning/</guid><description>What is Unsupervised Learning no labelled data, algorithm finds something interesting in unlabeled data. given only inputs ${x_0,\ldots,x_n}$, but no output labels ${y_0,\ldots,y_n}$ Clustering: for example, groups data points together</description></item><item><title>{{Supervised Learning}}</title><link>https://gc625.github.io/notes/back2basics/supervisedlearning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gc625.github.io/notes/back2basics/supervisedlearning/</guid><description>What is supervised learning? Training an algorithm to output $y$ for a given $x$ using sufficient training samples ${(x^{(0)},y^{(0)}),(x^{(1)},y^{(1)}),\ldots,(x^{(n)},y^{(n)})}$ for some input $x^{(i)}$ and correct output $y^{(i)}$ Regression: predicting an a number (infinitely many outputs) Classification: predicting categories (finite outputs) Linear Regression Given a training set we can use a learning algorithm to learnign a function $f$ that predicts an output $\hat{y}$ given an input $x$ for linear regression, $f$ is a straight line.</description></item></channel></rss>